<!DOCTYPE html><html lang="en"><head><meta name="viewport" content="width=device-width"/><meta name="description" content="Starter kit for NextJS with Nx"/><meta charSet="utf8"/><title>machine learning posts - idmontie&#x27;s Portfolio</title><meta name="keywords" content="machine learning"/><meta name="next-head-count" content="5"/><script>
                        try {
                            const storage = window && window.localStorage;
                            if (
                                storage.getItem("color-theme") === "dark" ||
                                (!("color-theme" in storage) &&
                                    window.matchMedia("(prefers-color-scheme: dark)").matches)
                            ) {
                                document.documentElement.classList.add("dark");
                            } else {
                                document.documentElement.classList.remove("dark");
                            }
                        } catch (e) {
                            console.error(e);
                        }
                        </script><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin /><link rel="preload" href="/_next/static/css/e3a1fdb20d955547.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e3a1fdb20d955547.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-f5a48affa2e5582e.js" defer=""></script><script src="/_next/static/chunks/framework-dfd14d7ce6600b03.js" defer=""></script><script src="/_next/static/chunks/main-3412f4325dfb32ac.js" defer=""></script><script src="/_next/static/chunks/pages/_app-16c6de052764dd01.js" defer=""></script><script src="/_next/static/chunks/2e6ef0e3-e0509ad90a52e50d.js" defer=""></script><script src="/_next/static/chunks/956-14a634a4f6c953dc.js" defer=""></script><script src="/_next/static/chunks/pages/blog/tag/%5Btag%5D-d58d4eca18d427b0.js" defer=""></script><script src="/_next/static/ErleKivQfo9lIee9PDR27/_buildManifest.js" defer=""></script><script src="/_next/static/ErleKivQfo9lIee9PDR27/_ssgManifest.js" defer=""></script><style data-href="https://fonts.googleapis.com/css2?family=Merriweather:wght@300;400;700&family=Open+Sans:wght@300;400;700&display=swap">@font-face{font-family:'Merriweather';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-4n0qyriQwlOrhSvowK_l521wRpXA.woff) format('woff')}@font-face{font-family:'Merriweather';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-440qyriQwlOrhSvowK_l5OeA.woff) format('woff')}@font-face{font-family:'Merriweather';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-4n0qyriQwlOrhSvowK_l52xwNpXA.woff) format('woff')}@font-face{font-family:'Open Sans';font-style:normal;font-weight:300;font-stretch:normal;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memSYaGs126MiZpBA-UvWbX2vVnXBbObj2OVZyOOSr4dVJWUgsiH0C4k.woff) format('woff')}@font-face{font-family:'Open Sans';font-style:normal;font-weight:400;font-stretch:normal;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memSYaGs126MiZpBA-UvWbX2vVnXBbObj2OVZyOOSr4dVJWUgsjZ0C4k.woff) format('woff')}@font-face{font-family:'Open Sans';font-style:normal;font-weight:700;font-stretch:normal;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memSYaGs126MiZpBA-UvWbX2vVnXBbObj2OVZyOOSr4dVJWUgsg-1y4k.woff) format('woff')}@font-face{font-family:'Merriweather';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-4n0qyriQwlOrhSvowK_l521wRZVcf6hPvhPUWH.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Merriweather';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-4n0qyriQwlOrhSvowK_l521wRZXMf6hPvhPUWH.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Merriweather';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-4n0qyriQwlOrhSvowK_l521wRZV8f6hPvhPUWH.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Merriweather';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-4n0qyriQwlOrhSvowK_l521wRZVsf6hPvhPUWH.woff2) format('woff2');unicode-range:U+0100-02AF,U+0304,U+0308,U+0329,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Merriweather';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-4n0qyriQwlOrhSvowK_l521wRZWMf6hPvhPQ.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Merriweather';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-440qyriQwlOrhSvowK_l5-cSZMdeX3rsHo.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Merriweather';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-440qyriQwlOrhSvowK_l5-eCZMdeX3rsHo.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Merriweather';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-440qyriQwlOrhSvowK_l5-cyZMdeX3rsHo.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Merriweather';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-440qyriQwlOrhSvowK_l5-ciZMdeX3rsHo.woff2) format('woff2');unicode-range:U+0100-02AF,U+0304,U+0308,U+0329,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Merriweather';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-440qyriQwlOrhSvowK_l5-fCZMdeX3rg.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Merriweather';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-4n0qyriQwlOrhSvowK_l52xwNZVcf6hPvhPUWH.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Merriweather';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-4n0qyriQwlOrhSvowK_l52xwNZXMf6hPvhPUWH.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Merriweather';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-4n0qyriQwlOrhSvowK_l52xwNZV8f6hPvhPUWH.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Merriweather';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-4n0qyriQwlOrhSvowK_l52xwNZVsf6hPvhPUWH.woff2) format('woff2');unicode-range:U+0100-02AF,U+0304,U+0308,U+0329,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Merriweather';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-4n0qyriQwlOrhSvowK_l52xwNZWMf6hPvhPQ.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Open Sans';font-style:normal;font-weight:300;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSKmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Open Sans';font-style:normal;font-weight:300;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSumu0SC55K5gw.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Open Sans';font-style:normal;font-weight:300;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSOmu0SC55K5gw.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:300;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSymu0SC55K5gw.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:300;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTS2mu0SC55K5gw.woff2) format('woff2');unicode-range:U+0590-05FF,U+200C-2010,U+20AA,U+25CC,U+FB1D-FB4F}@font-face{font-family:'Open Sans';font-style:normal;font-weight:300;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSCmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Open Sans';font-style:normal;font-weight:300;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSGmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0100-02AF,U+0304,U+0308,U+0329,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:300;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTS-mu0SC55I.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Open Sans';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSKmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Open Sans';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSumu0SC55K5gw.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Open Sans';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSOmu0SC55K5gw.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSymu0SC55K5gw.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTS2mu0SC55K5gw.woff2) format('woff2');unicode-range:U+0590-05FF,U+200C-2010,U+20AA,U+25CC,U+FB1D-FB4F}@font-face{font-family:'Open Sans';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSCmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Open Sans';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSGmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0100-02AF,U+0304,U+0308,U+0329,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTS-mu0SC55I.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Open Sans';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSKmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Open Sans';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSumu0SC55K5gw.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Open Sans';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSOmu0SC55K5gw.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSymu0SC55K5gw.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTS2mu0SC55K5gw.woff2) format('woff2');unicode-range:U+0590-05FF,U+200C-2010,U+20AA,U+25CC,U+FB1D-FB4F}@font-face{font-family:'Open Sans';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSCmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Open Sans';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSGmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0100-02AF,U+0304,U+0308,U+0329,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTS-mu0SC55I.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style></head><body class="leading-base bg-white text-lg antialiased dark:bg-gray-900 dark:text-white"><div id="__next"><nav class="flex items-center p-3"><div class="flex-1 space-x-4"><a class="" href="/">Home</a><a class="" href="/blog">Blog</a><a class="" href="/blog/tag">Tags</a><a class="" href="/portfolio">Portfolio</a></div><div class="flex flex-1 justify-end"><label for="toggle" class="relative flex w-fit cursor-pointer items-center">🌑<input type="checkbox" id="toggle" class="sr-only"/><div class="
                        relative
                        h-6
                        w-11
                        rounded-full
                        border-2
                    
                        after:absolute
                        after:h-5
                        after:w-5
                        after:rounded-full
                        after:border
                        after:border-gray-300
                        after:bg-white
                        after:shadow-sm
                        after:transition
                     
                            border-gray-200
                            bg-gray-200
                            dark:border-gray-700
                            dark:bg-gray-700
                        "></div>☀️</label></div></nav><div class="relative overflow-hidden pb-4"><div class="mx-auto flex w-full max-w-screen-md flex-col items-center"><main class="app max-w-full"><div><div class="px-4"><header><h1 class="my-8 text-center text-3xl font-bold dark:text-white md:text-5xl"><span class="capitalize">machine learning</span> posts</h1></header><div class="space-y-6"><div class="overflow-visible rounded-lg bg-white shadow-lg dark:bg-gray-800"><div class="p-4"><a href="/blog/post/2023-07-01-fast-embedding-lookingup"><h2 class="text-2xl font-bold">Fast Similar Embedding Lookup</h2></a><div class="py-2 text-sm"></div><div class="prose dark:prose-dark py-4"><div><div><p>While working on the Clarity Hub NLP API, we had a common use-case where we would create embeddings from text, and use those embedding to determine cosine similarity with other embeddings. Doing this required loading all of the embeddings in-memory and then computing cosine similarity with the entire dataset. As the dataset grew, this operation would get incredibly slow.</p></div></div></div><div class="pt-2"><a href="/blog/post/2023-07-01-fast-embedding-lookingup">Continue reading<span class="ml-2">→</span></a></div></div></div><div class="overflow-visible rounded-lg bg-white shadow-lg dark:bg-gray-800"><div class="p-4"><a href="/blog/post/2023-01-07-clarity-hub-infer"><h2 class="text-2xl font-bold">Clarity Hub Infer API</h2></a><div class="py-2 text-sm"></div><div class="prose dark:prose-dark py-4"><div><div><p><img alt="Screen Shot 2023-01-07 at 3.57.30 PM.png" src="/media/2023-01-07-clarity-hub-infer/Screen_Shot_2023-01-07_at_3.57.30_PM.png" style="max-height:500px;margin:auto;text-align:center"/></p></div></div></div><div class="pt-2"><a href="/blog/post/2023-01-07-clarity-hub-infer">Continue reading<span class="ml-2">→</span></a></div></div></div></div></div></div></main></div></div><footer class="width-full border-top bg-gray-light my-4 p-4"><div class="flex-justify-between flex px-3 text-gray-700 dark:text-gray-300"><div class="flex flex-1 flex-row gap-6"><div>Like this blog and portfolio? Check it out on<!-- --> <a href="https://github.com/idmontie/idmontie.github.io" target="_blank" rel="noopener noreferrer">Github</a></div><div>|</div><div><a href="/rss.xml" target="_blank" rel="noopener noreferrer">RSS Feed</a></div></div><div class="flex self-end"><button>Back to top</button></div></div></footer></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"headTitle":"machine learning posts - idmontie's Portfolio","headKeywords":"machine learning","tag":"machine learning","posts":[{"slug":"2023-07-01-fast-embedding-lookingup","date":"2023-07-01","title":"Fast Similar Embedding Lookup","frontmatter":{"title":"Fast Similar Embedding Lookup","tags":["nlp","machine learning"]},"contentRaw":"\nWhile working on the Clarity Hub NLP API, we had a common use-case where we would create embeddings from text, and use those embedding to determine cosine similarity with other embeddings. Doing this required loading all of the embeddings in-memory and then computing cosine similarity with the entire dataset. As the dataset grew, this operation would get incredibly slow.\n\nWe worked on a fast way to do these lookups using ranges that can be performed in any database. This approach was never implemented, but we worked on multiple proof-of-concepts to test out our ideas. The goal was to take an input text, compute an embedding, load the entire embedding datasets loaded into an AWS lambda, find the most similar set of vectors, and return the top N similar vectors in one use-case. To tackle that, we came up with the following idea.\n\nGiven a vector A, compute is similar to a unit vector U of the same dimension as A. So:\n\n```cpp\ndim(U) = dim(A)\n```\n\nAnd\n\n```cpp\nS_u = cos(θ) = A · U / ||A|| x ||U||\n```\n\nWhere S_u is the similarity with the unit vector. The unit vector just needs to be the same across all samples.\n\nFor each embedding, store the calculated S_u.\n\nIf we want to find similar vectors for a new vector B, then we compute is similarity to the unit vector.\n\nThen, we can query the database for vectors within an interval of `[S_u - ε, S_u + ε]` . This will give us a subset of the dataset that have similar similarities with the unit vector.\n\nWe can re-query increasing or decreasing ε until the top N results are found.\n\nTo further improve accuracy, we can also re-compute the similarity score using cosine similarity with the subset of vectors, which is still much faster then computing the similarity against the entire dataset.\n\nThis approach begins to break down as the cosine similarity to the unit vector chosen gets very large (`\u003e 0.4`).  We end up with the possibility of matching against vectors that are of opposite directions – the least similar vectors to the original input vector.\n\nOne solution to workaround this could be to pre-compute the similarity of a vector against unit vectors for each dimension of the input vector. But this could be 512 or more cosine similarity calculations for modern embeddings just to precompute the data. Once all unit vector similarities are calculated and stored, the range query against the database would be made against the column for which the input vector’s similarity is closest to 0.\n\nThere are a lot of real solutions to this problem, but this was a fun exercise to think about and work on.\n\n## Further reading\n\nVector similarity search is becoming increasingly popular and integrated into databases. Here are some resources to learn more: [Vector Similarity Search](https://zilliz.com/blog/vector-similarity-search).\n","contentHTML":"\u003cp\u003eWhile working on the Clarity Hub NLP API, we had a common use-case where we would create embeddings from text, and use those embedding to determine cosine similarity with other embeddings. Doing this required loading all of the embeddings in-memory and then computing cosine similarity with the entire dataset. As the dataset grew, this operation would get incredibly slow.\u003c/p\u003e\n\u003cp\u003eWe worked on a fast way to do these lookups using ranges that can be performed in any database. This approach was never implemented, but we worked on multiple proof-of-concepts to test out our ideas. The goal was to take an input text, compute an embedding, load the entire embedding datasets loaded into an AWS lambda, find the most similar set of vectors, and return the top N similar vectors in one use-case. To tackle that, we came up with the following idea.\u003c/p\u003e\n\u003cp\u003eGiven a vector A, compute is similar to a unit vector U of the same dimension as A. So:\u003c/p\u003e\n\u003cdiv class=\"overflow-auto rounded bg-gray-200 p-4 font-mono text-sm dark:bg-gray-800 dark:text-gray-100\"\u003e\u003cpre\u003e\u003ccode class=\"language-cpp\"\u003edim(U) = dim(A)\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eAnd\u003c/p\u003e\n\u003cdiv class=\"overflow-auto rounded bg-gray-200 p-4 font-mono text-sm dark:bg-gray-800 dark:text-gray-100\"\u003e\u003cpre\u003e\u003ccode class=\"language-cpp\"\u003eS_u = cos(θ) = A · U / ||A|| x ||U||\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eWhere S_u is the similarity with the unit vector. The unit vector just needs to be the same across all samples.\u003c/p\u003e\n\u003cp\u003eFor each embedding, store the calculated S_u.\u003c/p\u003e\n\u003cp\u003eIf we want to find similar vectors for a new vector B, then we compute is similarity to the unit vector.\u003c/p\u003e\n\u003cp\u003eThen, we can query the database for vectors within an interval of \u003ccode\u003e[S_u - ε, S_u + ε]\u003c/code\u003e . This will give us a subset of the dataset that have similar similarities with the unit vector.\u003c/p\u003e\n\u003cp\u003eWe can re-query increasing or decreasing ε until the top N results are found.\u003c/p\u003e\n\u003cp\u003eTo further improve accuracy, we can also re-compute the similarity score using cosine similarity with the subset of vectors, which is still much faster then computing the similarity against the entire dataset.\u003c/p\u003e\n\u003cp\u003eThis approach begins to break down as the cosine similarity to the unit vector chosen gets very large (\u003ccode\u003e\u0026gt; 0.4\u003c/code\u003e).  We end up with the possibility of matching against vectors that are of opposite directions – the least similar vectors to the original input vector.\u003c/p\u003e\n\u003cp\u003eOne solution to workaround this could be to pre-compute the similarity of a vector against unit vectors for each dimension of the input vector. But this could be 512 or more cosine similarity calculations for modern embeddings just to precompute the data. Once all unit vector similarities are calculated and stored, the range query against the database would be made against the column for which the input vector’s similarity is closest to 0.\u003c/p\u003e\n\u003cp\u003eThere are a lot of real solutions to this problem, but this was a fun exercise to think about and work on.\u003c/p\u003e\n\u003ch2\u003eFurther reading\u003c/h2\u003e\n\u003cp\u003eVector similarity search is becoming increasingly popular and integrated into databases. Here are some resources to learn more: \u003ca href=\"https://zilliz.com/blog/vector-similarity-search\"\u003eVector Similarity Search\u003c/a\u003e.\u003c/p\u003e","contentCode":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    pre: \"pre\",\n    code: \"code\",\n    h2: \"h2\",\n    a: \"a\"\n  }, props.components);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"While working on the Clarity Hub NLP API, we had a common use-case where we would create embeddings from text, and use those embedding to determine cosine similarity with other embeddings. Doing this required loading all of the embeddings in-memory and then computing cosine similarity with the entire dataset. As the dataset grew, this operation would get incredibly slow.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"We worked on a fast way to do these lookups using ranges that can be performed in any database. This approach was never implemented, but we worked on multiple proof-of-concepts to test out our ideas. The goal was to take an input text, compute an embedding, load the entire embedding datasets loaded into an AWS lambda, find the most similar set of vectors, and return the top N similar vectors in one use-case. To tackle that, we came up with the following idea.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Given a vector A, compute is similar to a unit vector U of the same dimension as A. So:\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-cpp\",\n        children: \"dim(U) = dim(A)\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"And\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-cpp\",\n        children: \"S_u = cos(θ) = A · U / ||A|| x ||U||\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Where S_u is the similarity with the unit vector. The unit vector just needs to be the same across all samples.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"For each embedding, store the calculated S_u.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"If we want to find similar vectors for a new vector B, then we compute is similarity to the unit vector.\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Then, we can query the database for vectors within an interval of \", _jsx(_components.code, {\n        children: \"[S_u - ε, S_u + ε]\"\n      }), \" . This will give us a subset of the dataset that have similar similarities with the unit vector.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"We can re-query increasing or decreasing ε until the top N results are found.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"To further improve accuracy, we can also re-compute the similarity score using cosine similarity with the subset of vectors, which is still much faster then computing the similarity against the entire dataset.\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"This approach begins to break down as the cosine similarity to the unit vector chosen gets very large (\", _jsx(_components.code, {\n        children: \"\u003e 0.4\"\n      }), \").  We end up with the possibility of matching against vectors that are of opposite directions – the least similar vectors to the original input vector.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"One solution to workaround this could be to pre-compute the similarity of a vector against unit vectors for each dimension of the input vector. But this could be 512 or more cosine similarity calculations for modern embeddings just to precompute the data. Once all unit vector similarities are calculated and stored, the range query against the database would be made against the column for which the input vector’s similarity is closest to 0.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"There are a lot of real solutions to this problem, but this was a fun exercise to think about and work on.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Further reading\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Vector similarity search is becoming increasingly popular and integrated into databases. Here are some resources to learn more: \", _jsx(_components.a, {\n        href: \"https://zilliz.com/blog/vector-similarity-search\",\n        children: \"Vector Similarity Search\"\n      }), \".\"]\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","excerptRaw":"\nWhile working on the Clarity Hub NLP API, we had a common use-case where we would create embeddings from text, and use those embedding to determine cosine similarity with other embeddings. Doing this required loading all of the embeddings in-memory and then computing cosine similarity with the entire dataset. As the dataset grew, this operation would get incredibly slow.","excerptHTML":"\u003cp\u003eWhile working on the Clarity Hub NLP API, we had a common use-case where we would create embeddings from text, and use those embedding to determine cosine similarity with other embeddings. Doing this required loading all of the embeddings in-memory and then computing cosine similarity with the entire dataset. As the dataset grew, this operation would get incredibly slow.\u003c/p\u003e","excerptCode":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {jsx: _jsx} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\"\n  }, props.components);\n  return _jsx(_components.p, {\n    children: \"While working on the Clarity Hub NLP API, we had a common use-case where we would create embeddings from text, and use those embedding to determine cosine similarity with other embeddings. Doing this required loading all of the embeddings in-memory and then computing cosine similarity with the entire dataset. As the dataset grew, this operation would get incredibly slow.\"\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","tags":["nlp","machine learning"]},{"slug":"2023-01-07-clarity-hub-infer","date":"2023-01-07","title":"Clarity Hub Infer API","frontmatter":{"title":"Clarity Hub Infer API","tags":["nlp","machine learning"]},"contentRaw":"\n![Screen Shot 2023-01-07 at 3.57.30 PM.png](/media/2023-01-07-clarity-hub-infer/Screen_Shot_2023-01-07_at_3.57.30_PM.png)\n\nWhile working on Clarity Hub, we created a Clarity Hub Infer API along with a developer portal that would let anyone create infer models.\n\nThe Clarity Hub Infer API provides a fast and intuitive way to create, manage, and deploy NLP models based on labelling utterances.\n\nAt the most basic level, the Infer API would let users send utterances via an API and get toxicity analysis, sentiment scores, and simple NLP data like nouns and topics from the utterances.\n\nThe power of the Infer API is that consumers can supply a set of pre-labelled utterances to the API, and the API will create a model from this, even if there are only a few utterances used for training. Then the consumer can send a new utterance get a label using that model.\n\nThe NLP APIs at Clarity Hub were a set of APIs:\n\n```mermaid\ngraph RL\n  NLP(Clarity Hub NLP API) --\u003e API(Clarity Hub Infer API) --\u003e Consumer\n```\n\nThe Consumer would user the Infer API which provided APIs for training and labeling datasets and getting toxicity and sentiment analyses. the Clarity Hub NLP API contained trained Tensorflow datasets for creating embeddings via the Universal Sentence Encoder (USE).\n\nAn **embedding** a vector that represents an utterance - a sentence, sentence fragment, or paragraph of text.\n\nTraining would involve a consumer sending a payload of utterances with labels to the Infer API, which would call the NLP API internally to create embeddings. We then clustered these embeddings to and re-labelled the clusters using the given labels. If no label was found for an utterance cluster, we attempted to pull a topic out of the utterances to re-label it.\n\nThe clusters with labels were then stored into S3.\n\n```mermaid\ngraph TD\n  Train --\u003e|Utterances with labels| USE\n  USE --\u003e|Embeddings with labels| Clustering\n  Clustering --\u003e|Embedding Clusters| Labeller\n  Labeller --\u003e|Clusters with Labels| S3\n```\n\nTo classify a new utterance, we created an embedding from it, loaded the existing dataset in, then ran cosine similarity to find the most probabilistic matches:\n\n```mermaid\ngraph TD\n  Classify --\u003e |Utterance| USE\n  USE --\u003e |Embedding| Classifier(Classifier)\n  Classifier --\u003e |Embedding + Clusters from S3| Similarity(Cosine Similarity)\n  Similarity --\u003e |Labels with Probability| Response\n```\n\n### What it looked like\n\n![Screen Shot 2023-01-07 at 3.57.56 PM.png](/media/2023-01-07-clarity-hub-infer/Screen_Shot_2023-01-07_at_3.57.56_PM.png)\n\n![Screen Shot 2023-01-07 at 3.58.08 PM.png](/media/2023-01-07-clarity-hub-infer/Screen_Shot_2023-01-07_at_3.58.08_PM.png)\n\n![Screen Shot 2023-01-07 at 3.58.28 PM.png](/media/2023-01-07-clarity-hub-infer/Screen_Shot_2023-01-07_at_3.58.28_PM.png)\n\n### Conclusion\n\nWith ChatGPT and other NLP models coming out lately, this seems fairly basic, but the following processes are still very useful to understand:\n\n- Convert language to a representation that is easier to work with, like a vector.\n- Clustering vectors is a great way to find representative vectors, reducing the size of the number of vectors you need to work with.\n- Cosine Similarity can be used to find how similar vectors are. If a vector is labelled with metadata, it also tells you how similar the metadata between the vectors are as well.\n\nYou can see [my project page](/projects/2020-05-18-clarity-hub-infer) for more details and links to the Github repos.\n","contentHTML":"\u003cp\u003e\u003cimg alt=\"Screen Shot 2023-01-07 at 3.57.30 PM.png\" src=\"/media/2023-01-07-clarity-hub-infer/Screen_Shot_2023-01-07_at_3.57.30_PM.png\" style=\"max-height:500px;margin:auto;text-align:center\"/\u003e\u003c/p\u003e\n\u003cp\u003eWhile working on Clarity Hub, we created a Clarity Hub Infer API along with a developer portal that would let anyone create infer models.\u003c/p\u003e\n\u003cp\u003eThe Clarity Hub Infer API provides a fast and intuitive way to create, manage, and deploy NLP models based on labelling utterances.\u003c/p\u003e\n\u003cp\u003eAt the most basic level, the Infer API would let users send utterances via an API and get toxicity analysis, sentiment scores, and simple NLP data like nouns and topics from the utterances.\u003c/p\u003e\n\u003cp\u003eThe power of the Infer API is that consumers can supply a set of pre-labelled utterances to the API, and the API will create a model from this, even if there are only a few utterances used for training. Then the consumer can send a new utterance get a label using that model.\u003c/p\u003e\n\u003cp\u003eThe NLP APIs at Clarity Hub were a set of APIs:\u003c/p\u003e\n\u003cdiv class=\"py-8 [\u0026amp;_svg]:m-auto\"\u003e\u003cdiv class=\"mermaid\" data-mermaid-src=\"graph RL\n  NLP(Clarity Hub NLP API) --\u0026gt; API(Clarity Hub Infer API) --\u0026gt; Consumer\"\u003egraph RL\n  NLP(Clarity Hub NLP API) --\u0026gt; API(Clarity Hub Infer API) --\u0026gt; Consumer\u003c/div\u003e\u003c/div\u003e\n\u003cp\u003eThe Consumer would user the Infer API which provided APIs for training and labeling datasets and getting toxicity and sentiment analyses. the Clarity Hub NLP API contained trained Tensorflow datasets for creating embeddings via the Universal Sentence Encoder (USE).\u003c/p\u003e\n\u003cp\u003eAn \u003cstrong\u003eembedding\u003c/strong\u003e a vector that represents an utterance - a sentence, sentence fragment, or paragraph of text.\u003c/p\u003e\n\u003cp\u003eTraining would involve a consumer sending a payload of utterances with labels to the Infer API, which would call the NLP API internally to create embeddings. We then clustered these embeddings to and re-labelled the clusters using the given labels. If no label was found for an utterance cluster, we attempted to pull a topic out of the utterances to re-label it.\u003c/p\u003e\n\u003cp\u003eThe clusters with labels were then stored into S3.\u003c/p\u003e\n\u003cdiv class=\"py-8 [\u0026amp;_svg]:m-auto\"\u003e\u003cdiv class=\"mermaid\" data-mermaid-src=\"graph TD\n  Train --\u0026gt;|Utterances with labels| USE\n  USE --\u0026gt;|Embeddings with labels| Clustering\n  Clustering --\u0026gt;|Embedding Clusters| Labeller\n  Labeller --\u0026gt;|Clusters with Labels| S3\"\u003egraph TD\n  Train --\u0026gt;|Utterances with labels| USE\n  USE --\u0026gt;|Embeddings with labels| Clustering\n  Clustering --\u0026gt;|Embedding Clusters| Labeller\n  Labeller --\u0026gt;|Clusters with Labels| S3\u003c/div\u003e\u003c/div\u003e\n\u003cp\u003eTo classify a new utterance, we created an embedding from it, loaded the existing dataset in, then ran cosine similarity to find the most probabilistic matches:\u003c/p\u003e\n\u003cdiv class=\"py-8 [\u0026amp;_svg]:m-auto\"\u003e\u003cdiv class=\"mermaid\" data-mermaid-src=\"graph TD\n  Classify --\u0026gt; |Utterance| USE\n  USE --\u0026gt; |Embedding| Classifier(Classifier)\n  Classifier --\u0026gt; |Embedding + Clusters from S3| Similarity(Cosine Similarity)\n  Similarity --\u0026gt; |Labels with Probability| Response\"\u003egraph TD\n  Classify --\u0026gt; |Utterance| USE\n  USE --\u0026gt; |Embedding| Classifier(Classifier)\n  Classifier --\u0026gt; |Embedding + Clusters from S3| Similarity(Cosine Similarity)\n  Similarity --\u0026gt; |Labels with Probability| Response\u003c/div\u003e\u003c/div\u003e\n\u003ch3\u003eWhat it looked like\u003c/h3\u003e\n\u003cp\u003e\u003cimg alt=\"Screen Shot 2023-01-07 at 3.57.56 PM.png\" src=\"/media/2023-01-07-clarity-hub-infer/Screen_Shot_2023-01-07_at_3.57.56_PM.png\" style=\"max-height:500px;margin:auto;text-align:center\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Screen Shot 2023-01-07 at 3.58.08 PM.png\" src=\"/media/2023-01-07-clarity-hub-infer/Screen_Shot_2023-01-07_at_3.58.08_PM.png\" style=\"max-height:500px;margin:auto;text-align:center\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Screen Shot 2023-01-07 at 3.58.28 PM.png\" src=\"/media/2023-01-07-clarity-hub-infer/Screen_Shot_2023-01-07_at_3.58.28_PM.png\" style=\"max-height:500px;margin:auto;text-align:center\"/\u003e\u003c/p\u003e\n\u003ch3\u003eConclusion\u003c/h3\u003e\n\u003cp\u003eWith ChatGPT and other NLP models coming out lately, this seems fairly basic, but the following processes are still very useful to understand:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConvert language to a representation that is easier to work with, like a vector.\u003c/li\u003e\n\u003cli\u003eClustering vectors is a great way to find representative vectors, reducing the size of the number of vectors you need to work with.\u003c/li\u003e\n\u003cli\u003eCosine Similarity can be used to find how similar vectors are. If a vector is labelled with metadata, it also tells you how similar the metadata between the vectors are as well.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eYou can see \u003ca href=\"/projects/2020-05-18-clarity-hub-infer\"\u003emy project page\u003c/a\u003e for more details and links to the Github repos.\u003c/p\u003e","contentCode":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    img: \"img\",\n    strong: \"strong\",\n    h3: \"h3\",\n    ul: \"ul\",\n    li: \"li\",\n    a: \"a\"\n  }, props.components), {Mermaid} = _components;\n  if (!Mermaid) _missingMdxReference(\"Mermaid\", true);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/media/2023-01-07-clarity-hub-infer/Screen_Shot_2023-01-07_at_3.57.30_PM.png\",\n        alt: \"Screen Shot 2023-01-07 at 3.57.30 PM.png\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"While working on Clarity Hub, we created a Clarity Hub Infer API along with a developer portal that would let anyone create infer models.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The Clarity Hub Infer API provides a fast and intuitive way to create, manage, and deploy NLP models based on labelling utterances.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"At the most basic level, the Infer API would let users send utterances via an API and get toxicity analysis, sentiment scores, and simple NLP data like nouns and topics from the utterances.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The power of the Infer API is that consumers can supply a set of pre-labelled utterances to the API, and the API will create a model from this, even if there are only a few utterances used for training. Then the consumer can send a new utterance get a label using that model.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The NLP APIs at Clarity Hub were a set of APIs:\"\n    }), \"\\n\", _jsx(Mermaid, {\n      chart: \"graph RL\\n  NLP(Clarity Hub NLP API) --\u003e API(Clarity Hub Infer API) --\u003e Consumer\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The Consumer would user the Infer API which provided APIs for training and labeling datasets and getting toxicity and sentiment analyses. the Clarity Hub NLP API contained trained Tensorflow datasets for creating embeddings via the Universal Sentence Encoder (USE).\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"An \", _jsx(_components.strong, {\n        children: \"embedding\"\n      }), \" a vector that represents an utterance - a sentence, sentence fragment, or paragraph of text.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Training would involve a consumer sending a payload of utterances with labels to the Infer API, which would call the NLP API internally to create embeddings. We then clustered these embeddings to and re-labelled the clusters using the given labels. If no label was found for an utterance cluster, we attempted to pull a topic out of the utterances to re-label it.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The clusters with labels were then stored into S3.\"\n    }), \"\\n\", _jsx(Mermaid, {\n      chart: \"graph TD\\n  Train --\u003e|Utterances with labels| USE\\n  USE --\u003e|Embeddings with labels| Clustering\\n  Clustering --\u003e|Embedding Clusters| Labeller\\n  Labeller --\u003e|Clusters with Labels| S3\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"To classify a new utterance, we created an embedding from it, loaded the existing dataset in, then ran cosine similarity to find the most probabilistic matches:\"\n    }), \"\\n\", _jsx(Mermaid, {\n      chart: \"graph TD\\n  Classify --\u003e |Utterance| USE\\n  USE --\u003e |Embedding| Classifier(Classifier)\\n  Classifier --\u003e |Embedding + Clusters from S3| Similarity(Cosine Similarity)\\n  Similarity --\u003e |Labels with Probability| Response\"\n    }), \"\\n\", _jsx(_components.h3, {\n      children: \"What it looked like\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/media/2023-01-07-clarity-hub-infer/Screen_Shot_2023-01-07_at_3.57.56_PM.png\",\n        alt: \"Screen Shot 2023-01-07 at 3.57.56 PM.png\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/media/2023-01-07-clarity-hub-infer/Screen_Shot_2023-01-07_at_3.58.08_PM.png\",\n        alt: \"Screen Shot 2023-01-07 at 3.58.08 PM.png\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/media/2023-01-07-clarity-hub-infer/Screen_Shot_2023-01-07_at_3.58.28_PM.png\",\n        alt: \"Screen Shot 2023-01-07 at 3.58.28 PM.png\"\n      })\n    }), \"\\n\", _jsx(_components.h3, {\n      children: \"Conclusion\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"With ChatGPT and other NLP models coming out lately, this seems fairly basic, but the following processes are still very useful to understand:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"Convert language to a representation that is easier to work with, like a vector.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Clustering vectors is a great way to find representative vectors, reducing the size of the number of vectors you need to work with.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Cosine Similarity can be used to find how similar vectors are. If a vector is labelled with metadata, it also tells you how similar the metadata between the vectors are as well.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"You can see \", _jsx(_components.a, {\n        href: \"/projects/2020-05-18-clarity-hub-infer\",\n        children: \"my project page\"\n      }), \" for more details and links to the Github repos.\"]\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\nfunction _missingMdxReference(id, component) {\n  throw new Error(\"Expected \" + (component ? \"component\" : \"object\") + \" `\" + id + \"` to be defined: you likely forgot to import, pass, or provide it.\");\n}\n","excerptRaw":"\n![Screen Shot 2023-01-07 at 3.57.30 PM.png](/media/2023-01-07-clarity-hub-infer/Screen_Shot_2023-01-07_at_3.57.30_PM.png)","excerptHTML":"\u003cp\u003e\u003cimg alt=\"Screen Shot 2023-01-07 at 3.57.30 PM.png\" src=\"/media/2023-01-07-clarity-hub-infer/Screen_Shot_2023-01-07_at_3.57.30_PM.png\" style=\"max-height:500px;margin:auto;text-align:center\"/\u003e\u003c/p\u003e","excerptCode":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {jsx: _jsx} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    img: \"img\"\n  }, props.components);\n  return _jsx(_components.p, {\n    children: _jsx(_components.img, {\n      src: \"/media/2023-01-07-clarity-hub-infer/Screen_Shot_2023-01-07_at_3.57.30_PM.png\",\n      alt: \"Screen Shot 2023-01-07 at 3.57.30 PM.png\"\n    })\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","tags":["nlp","machine learning"]}]},"__N_SSG":true},"page":"/blog/tag/[tag]","query":{"tag":"machine learning"},"buildId":"ErleKivQfo9lIee9PDR27","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>