<!DOCTYPE html><html lang="en"><head><meta name="viewport" content="width=device-width"/><meta charSet="utf8"/><title>Blog Page <!-- -->2<!-- --> - idmontie&#x27;s Portfolio</title><meta name="description" content="Latest blog posts"/><meta name="next-head-count" content="4"/><script>
                        try {
                            const storage = window && window.localStorage;
                            if (
                                storage.getItem("color-theme") === "dark" ||
                                (!("color-theme" in storage) &&
                                    window.matchMedia("(prefers-color-scheme: dark)").matches)
                            ) {
                                document.documentElement.classList.add("dark");
                            } else {
                                document.documentElement.classList.remove("dark");
                            }
                        } catch (e) {
                            console.error(e);
                        }
                        </script><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin /><link rel="preload" href="/_next/static/css/16c94971c43471ba.css" as="style"/><link rel="stylesheet" href="/_next/static/css/16c94971c43471ba.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-f5a48affa2e5582e.js" defer=""></script><script src="/_next/static/chunks/framework-dfd14d7ce6600b03.js" defer=""></script><script src="/_next/static/chunks/main-3412f4325dfb32ac.js" defer=""></script><script src="/_next/static/chunks/pages/_app-9c5e07a2a70a4653.js" defer=""></script><script src="/_next/static/chunks/2e6ef0e3-e0509ad90a52e50d.js" defer=""></script><script src="/_next/static/chunks/956-14a634a4f6c953dc.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bpage%5D-bc0ce0e5778fbd51.js" defer=""></script><script src="/_next/static/u7f9GnEl5eUwHpo2DYBNP/_buildManifest.js" defer=""></script><script src="/_next/static/u7f9GnEl5eUwHpo2DYBNP/_ssgManifest.js" defer=""></script><style data-href="https://fonts.googleapis.com/css2?family=Merriweather:wght@300;400;700&family=Open+Sans:wght@300;400;700&display=swap">@font-face{font-family:'Merriweather';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-4n0qyriQwlOrhSvowK_l521wRpXA.woff) format('woff')}@font-face{font-family:'Merriweather';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-440qyriQwlOrhSvowK_l5OeA.woff) format('woff')}@font-face{font-family:'Merriweather';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-4n0qyriQwlOrhSvowK_l52xwNpXA.woff) format('woff')}@font-face{font-family:'Open Sans';font-style:normal;font-weight:300;font-stretch:normal;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memSYaGs126MiZpBA-UvWbX2vVnXBbObj2OVZyOOSr4dVJWUgsiH0C4k.woff) format('woff')}@font-face{font-family:'Open Sans';font-style:normal;font-weight:400;font-stretch:normal;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memSYaGs126MiZpBA-UvWbX2vVnXBbObj2OVZyOOSr4dVJWUgsjZ0C4k.woff) format('woff')}@font-face{font-family:'Open Sans';font-style:normal;font-weight:700;font-stretch:normal;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memSYaGs126MiZpBA-UvWbX2vVnXBbObj2OVZyOOSr4dVJWUgsg-1y4k.woff) format('woff')}@font-face{font-family:'Merriweather';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-4n0qyriQwlOrhSvowK_l521wRZVcf6hPvhPUWH.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Merriweather';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-4n0qyriQwlOrhSvowK_l521wRZXMf6hPvhPUWH.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Merriweather';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-4n0qyriQwlOrhSvowK_l521wRZV8f6hPvhPUWH.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Merriweather';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-4n0qyriQwlOrhSvowK_l521wRZVsf6hPvhPUWH.woff2) format('woff2');unicode-range:U+0100-02AF,U+0304,U+0308,U+0329,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Merriweather';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-4n0qyriQwlOrhSvowK_l521wRZWMf6hPvhPQ.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Merriweather';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-440qyriQwlOrhSvowK_l5-cSZMdeX3rsHo.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Merriweather';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-440qyriQwlOrhSvowK_l5-eCZMdeX3rsHo.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Merriweather';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-440qyriQwlOrhSvowK_l5-cyZMdeX3rsHo.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Merriweather';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-440qyriQwlOrhSvowK_l5-ciZMdeX3rsHo.woff2) format('woff2');unicode-range:U+0100-02AF,U+0304,U+0308,U+0329,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Merriweather';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-440qyriQwlOrhSvowK_l5-fCZMdeX3rg.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Merriweather';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-4n0qyriQwlOrhSvowK_l52xwNZVcf6hPvhPUWH.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Merriweather';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-4n0qyriQwlOrhSvowK_l52xwNZXMf6hPvhPUWH.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Merriweather';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-4n0qyriQwlOrhSvowK_l52xwNZV8f6hPvhPUWH.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Merriweather';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-4n0qyriQwlOrhSvowK_l52xwNZVsf6hPvhPUWH.woff2) format('woff2');unicode-range:U+0100-02AF,U+0304,U+0308,U+0329,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Merriweather';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-4n0qyriQwlOrhSvowK_l52xwNZWMf6hPvhPQ.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Open Sans';font-style:normal;font-weight:300;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSKmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Open Sans';font-style:normal;font-weight:300;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSumu0SC55K5gw.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Open Sans';font-style:normal;font-weight:300;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSOmu0SC55K5gw.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:300;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSymu0SC55K5gw.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:300;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTS2mu0SC55K5gw.woff2) format('woff2');unicode-range:U+0590-05FF,U+200C-2010,U+20AA,U+25CC,U+FB1D-FB4F}@font-face{font-family:'Open Sans';font-style:normal;font-weight:300;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSCmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Open Sans';font-style:normal;font-weight:300;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSGmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0100-02AF,U+0304,U+0308,U+0329,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:300;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTS-mu0SC55I.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Open Sans';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSKmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Open Sans';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSumu0SC55K5gw.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Open Sans';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSOmu0SC55K5gw.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSymu0SC55K5gw.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTS2mu0SC55K5gw.woff2) format('woff2');unicode-range:U+0590-05FF,U+200C-2010,U+20AA,U+25CC,U+FB1D-FB4F}@font-face{font-family:'Open Sans';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSCmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Open Sans';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSGmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0100-02AF,U+0304,U+0308,U+0329,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTS-mu0SC55I.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Open Sans';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSKmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Open Sans';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSumu0SC55K5gw.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Open Sans';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSOmu0SC55K5gw.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSymu0SC55K5gw.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTS2mu0SC55K5gw.woff2) format('woff2');unicode-range:U+0590-05FF,U+200C-2010,U+20AA,U+25CC,U+FB1D-FB4F}@font-face{font-family:'Open Sans';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSCmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Open Sans';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSGmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0100-02AF,U+0304,U+0308,U+0329,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v35/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTS-mu0SC55I.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style></head><body class="leading-base bg-white text-lg antialiased dark:bg-gray-900 dark:text-white"><div id="__next"><nav class="flex items-center p-3"><div class="flex-1 space-x-4"><a class="" href="/">Home</a><a class="" href="/blog">Blog</a><a class="" href="/portfolio">Portfolio</a></div><div class="flex flex-1 justify-end"><label for="toggle" class="relative flex w-fit cursor-pointer items-center">🌑<input type="checkbox" id="toggle" class="sr-only"/><div class="
                        relative
                        h-6
                        w-11
                        rounded-full
                        border-2
                    
                        after:absolute
                        after:h-5
                        after:w-5
                        after:rounded-full
                        after:border
                        after:border-gray-300
                        after:bg-white
                        after:shadow-sm
                        after:transition
                     
                            border-gray-200
                            bg-gray-200
                            dark:border-gray-700
                            dark:bg-gray-700
                        "></div>☀️</label></div></nav><div class="relative overflow-hidden pb-4"><div class="mx-auto flex w-full max-w-screen-md flex-col items-center"><main class="app max-w-full"><div><div class="px-4"><header><h1 class="my-8 text-center text-3xl font-bold dark:text-white md:text-5xl">Blog Page <!-- -->2</h1></header><div class="space-y-6"><div class="overflow-visible rounded-lg bg-white shadow-lg dark:bg-gray-800"><div class="p-4"><a href="/blog/post/2023-07-21-backend-frontend"><h2 class="text-2xl font-bold">Backend for Frontend</h2></a><div class="py-2 text-sm"></div><div class="prose dark:prose-dark py-4"><div><div><p>The Backend for the Frontend (BEFFE) is typically stateless and acts as a proxy for other services, including authentication, authorization, and core services. The recent divorce of browser code being rendered by backend services was created by SPAs - Single Page Applications. In simpler architectures, a SPA and service could be as simple as:</p></div></div></div><div class="pt-2"><a href="/blog/post/2023-07-21-backend-frontend">Continue reading<span class="ml-2">→</span></a></div></div></div><div class="overflow-visible rounded-lg bg-white shadow-lg dark:bg-gray-800"><div class="p-4"><a href="/blog/post/2023-07-03-llm-loops"><h2 class="text-2xl font-bold">AI Feedback Systems</h2></a><div class="py-2 text-sm"></div><div class="prose dark:prose-dark py-4"><div><div><p>We are starting to see a rise of novel use-cases for AI in products and games using LLMs. Rather than the simple chatbot like experiences we have seen in the past using AI, we are starting to see feedback systems being added to these experiences, providing additional context to the LLM than just the past conversation.</p></div></div></div><div class="pt-2"><a href="/blog/post/2023-07-03-llm-loops">Continue reading<span class="ml-2">→</span></a></div></div></div><div class="overflow-visible rounded-lg bg-white shadow-lg dark:bg-gray-800"><div class="p-4"><a href="/blog/post/2023-07-01-fast-embedding-lookingup"><h2 class="text-2xl font-bold">Fast Similar Embedding Lookup</h2></a><div class="py-2 text-sm"></div><div class="prose dark:prose-dark py-4"><div><div><p>While working on the Clarity Hub NLP API, we had a common use-case where we would create embeddings from text, and use those embedding to determine cosine similarity with other embeddings. Doing this required loading all of the embeddings in-memory and then computing cosine similarity with the entire dataset. As the dataset grew, this operation would get incredibly slow.</p></div></div></div><div class="pt-2"><a href="/blog/post/2023-07-01-fast-embedding-lookingup">Continue reading<span class="ml-2">→</span></a></div></div></div><div class="overflow-visible rounded-lg bg-white shadow-lg dark:bg-gray-800"><div class="p-4"><a href="/blog/post/2023-06-06-sora"><h2 class="text-2xl font-bold">Sora - OpenAI Visual Studio Code Extension</h2></a><div class="py-2 text-sm"></div><div class="prose dark:prose-dark py-4"><div><div><p>Github Copilot and other AI tools are hitting the scene. I decided to create my own Visual Studio Code extension, which is designed to use OpenAI’s APIs to bring some additional ChatGPT functionality into the code editor. The goal with Sora was to enable a developer to thoughtfully write a comment about the code they would like the AI to write, and then commit to it – rather than the real-time typeahead that Github Copilot provides.</p></div></div></div><div class="pt-2"><a href="/blog/post/2023-06-06-sora">Continue reading<span class="ml-2">→</span></a></div></div></div><div class="overflow-visible rounded-lg bg-white shadow-lg dark:bg-gray-800"><div class="p-4"><a href="/blog/post/2023-05-06-gptp"><h2 class="text-2xl font-bold">Revisiting GPTP - the Starcraft modding toolkit</h2></a><div class="py-2 text-sm"></div><div class="prose dark:prose-dark py-4"><div><div><p>One of the first PC games I played was Starcraft and the expansion Starcraft: Broodwar. We didn’t have a PC, so I had to play it on a friend’s computer, but I remember being immersed in the real-time strategy gameplay.</p></div></div></div><div class="pt-2"><a href="/blog/post/2023-05-06-gptp">Continue reading<span class="ml-2">→</span></a></div></div></div><div class="overflow-visible rounded-lg bg-white shadow-lg dark:bg-gray-800"><div class="p-4"><a href="/blog/post/2023-04-02-dark-emblem-rewrite"><h2 class="text-2xl font-bold">Dark Emblem Rewrite</h2></a><div class="py-2 text-sm"></div><div class="prose dark:prose-dark py-4"><div><div><p>Dark Emblem is an NFT project that was sparked by the idea of combining cards games with Crypto Kitties. You can buy packs, open them to collect random cards, and then use those cards to battle raids with others.</p></div></div></div><div class="pt-2"><a href="/blog/post/2023-04-02-dark-emblem-rewrite">Continue reading<span class="ml-2">→</span></a></div></div></div><div class="overflow-visible rounded-lg bg-white shadow-lg dark:bg-gray-800"><div class="p-4"><a href="/blog/post/2023-01-24-chat-gpt-doesnt-understand"><h2 class="text-2xl font-bold">ChatGPT Doesn&#x27;t Understand</h2></a><div class="py-2 text-sm"></div><div class="prose dark:prose-dark py-4"><div><div><p>Looks like everyone is trying to discover ChatGPT’s limitations. In one article I read, the author asked whether <a href="https://medium.com/@theworldaccordingtocgpt/chatgpt-can-play-20-questions-7911405f7aff">ChatGPT can play 20 questions</a>. The human in this scenario tried to play 20 questions with ChatGPT, and found that they needed to guide the AI to ask questions. From my perspective, it looked more like a failure to engineer the initial prompt correctly.</p></div></div></div><div class="pt-2"><a href="/blog/post/2023-01-24-chat-gpt-doesnt-understand">Continue reading<span class="ml-2">→</span></a></div></div></div><div class="overflow-visible rounded-lg bg-white shadow-lg dark:bg-gray-800"><div class="p-4"><a href="/blog/post/2023-01-15-hierarchy-of-webapp-needs"><h2 class="text-2xl font-bold">The Hierarchy of Webapp Needs</h2></a><div class="py-2 text-sm"></div><div class="prose dark:prose-dark py-4"><div><div><p>I was thinking about all the little projects I work on and how they grow over time. The applications end up hitting some milestones and end up needing similar functionality that compliments the core features. A small project ends up getting complex enough that it requires some unit tests. I’ll go to deploy the project and now I need some deployment scripts and analytics to ensure the application is running correctly. The technology may change between each project, but web applications always seem to have the same steps that need to be taken to strengthen the application as it is scaled up.</p></div></div></div><div class="pt-2"><a href="/blog/post/2023-01-15-hierarchy-of-webapp-needs">Continue reading<span class="ml-2">→</span></a></div></div></div><div class="overflow-visible rounded-lg bg-white shadow-lg dark:bg-gray-800"><div class="p-4"><a href="/blog/post/2023-01-07-clarity-hub-infer"><h2 class="text-2xl font-bold">Clarity Hub Infer API</h2></a><div class="py-2 text-sm"></div><div class="prose dark:prose-dark py-4"><div><div><p><img alt="Screen Shot 2023-01-07 at 3.57.30 PM.png" src="/media/2023-01-07-clarity-hub-infer/Screen_Shot_2023-01-07_at_3.57.30_PM.png" style="max-height:500px;margin:auto;text-align:center"/></p></div></div></div><div class="pt-2"><a href="/blog/post/2023-01-07-clarity-hub-infer">Continue reading<span class="ml-2">→</span></a></div></div></div><div class="overflow-visible rounded-lg bg-white shadow-lg dark:bg-gray-800"><div class="p-4"><a href="/blog/post/2023-01-01-nx-nextjs-starter"><h2 class="text-2xl font-bold">NX NextJS Starter</h2></a><div class="py-2 text-sm"></div><div class="prose dark:prose-dark py-4"><div><div><p>To kickstart the year, I created a repo that contains a simple starter kit for using NextJS with NX. You can see the repo here:</p></div></div></div><div class="pt-2"><a href="/blog/post/2023-01-01-nx-nextjs-starter">Continue reading<span class="ml-2">→</span></a></div></div></div></div><div><div><a href="/blog">&lt; Newer posts</a></div><div><a href="/blog/3">Older posts &gt;</a></div></div></div></div></main></div></div><footer class="width-full border-top bg-gray-light my-4 p-4"><div class="flex-justify-between flex px-3 text-gray-700 dark:text-gray-300"><div class="flex flex-1 flex-row gap-6"><div>Like this blog and portfolio? Check it out on<!-- --> <a href="https://github.com/idmontie/idmontie.github.io" target="_blank" rel="noopener noreferrer">Github</a></div><div>|</div><div><a href="/rss.xml" target="_blank" rel="noopener noreferrer">RSS Feed</a></div></div><div class="flex self-end"><button>Back to top</button></div></div></footer></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"slug":"2023-07-21-backend-frontend","date":"2023-07-21","title":"Backend for Frontend","frontmatter":{"title":"Backend for Frontend"},"contentRaw":"\nThe Backend for the Frontend (BEFFE) is typically stateless and acts as a proxy for other services, including authentication, authorization, and core services. The recent divorce of browser code being rendered by backend services was created by SPAs - Single Page Applications. In simpler architectures, a SPA and service could be as simple as:\n\n```mermaid\ngraph LR\n  SPA --\u003e Backend\n```\n\nSPAs also became popular because the codebase could be built into static assets that could be services.\n\nHowever, as complexity arises with the above architecture and the SPA starts to rely on more services, a thin proxy is typically introduced, like NGINX:\n\n```mermaid\ngraph LR\n  SPA --\u003e NGINX\n  NGINX --\u003e Service1\n  NGINX --\u003e Service2\n  NGINX --\u003e Service3\n```\n\nWhile proxies like NGINX can continue to be useful, using it as a proxy ends up putting a lot of routing and additional API handling logic on the client. The client now ends up also having to support, understand, and embed how to interact with the data rom all the downstream services. To simplify and create consistent contracts with the UI, we can create a proxy service: a Backend for the Frontend:\n\n```mermaid\ngraph LR\n  SPA --\u003e BEFFE\n  BEFFE --\u003e Service1\n  BEFFE --\u003e Service2\n  BEFFE --\u003e Service3\n```\n\nThe Backend for the Frontend ends up serving two purposes:\n\n- It acts as a proxy for all UI requests\n- It manages and massages the APIs of downstream services to be a consistent API for the UI.\n\nWhen building a UI, only a single service that acts like an API Gateway is required to understand and encode within the application.\n\nThe Backend for the Frontend model also allows engineers working on services to separate logic for the UI with the logic from an internal service. An internal service can rely on the Backend for the Frontend to handle things like Authentication, Authorization, Caching, and Rate-limits. These mechanisms may also be implemented as independent services, but the Backend for the Frontend can make multiple API calls per any API request from the UI.\n\n## Moving away from SPAs\n\nA newer development in the UI space is server side render the application. If we are already introducing additional complexity by having a Backend for the Frontend, then why not combine the UI code with the backend service and generate hydrated and cacheable pages via the server instead of using a SPA?\n\n```mermaid\ngraph LR\n  UIServer[\"UI Server\"] --\u003e Service1\n  UIServer --\u003e Service2\n UIServer --\u003e Service3\n```\n\nWith this design, we increase the complexity - the backend not has to be able to render the UI code. It must now render, hydrate, make requests to downstream APIs, and send those server-side rendered pages to the browser.\n\nBut this additional complexity enabled us to:\n\n- create truly authenticated and authorized routes – the code for those routes isn’t even streamed to the browser, unlike a SPA.\n- near instant page loads – reducing the bundle size by just sending the JavaScript needed to run the page makes this possible. And since API calls are happening within the same network, this saves the user from additional loading spinners after the initial bundle has been loaded.\n\nThe general trend of having a Backend for the Frontend comes with additional complexity. It stems from the want to enhance the experience of the user, and probably from a subconscious desire to return to a simpler time:\n\n```mermaid\ngraph TD\n  Server\n```\n","contentHTML":"\u003cp\u003eThe Backend for the Frontend (BEFFE) is typically stateless and acts as a proxy for other services, including authentication, authorization, and core services. The recent divorce of browser code being rendered by backend services was created by SPAs - Single Page Applications. In simpler architectures, a SPA and service could be as simple as:\u003c/p\u003e\n\u003cdiv class=\"py-8 [\u0026amp;_svg]:m-auto\"\u003e\u003cdiv class=\"mermaid\" data-mermaid-src=\"graph LR\n  SPA --\u0026gt; Backend\"\u003egraph LR\n  SPA --\u0026gt; Backend\u003c/div\u003e\u003c/div\u003e\n\u003cp\u003eSPAs also became popular because the codebase could be built into static assets that could be services.\u003c/p\u003e\n\u003cp\u003eHowever, as complexity arises with the above architecture and the SPA starts to rely on more services, a thin proxy is typically introduced, like NGINX:\u003c/p\u003e\n\u003cdiv class=\"py-8 [\u0026amp;_svg]:m-auto\"\u003e\u003cdiv class=\"mermaid\" data-mermaid-src=\"graph LR\n  SPA --\u0026gt; NGINX\n  NGINX --\u0026gt; Service1\n  NGINX --\u0026gt; Service2\n  NGINX --\u0026gt; Service3\"\u003egraph LR\n  SPA --\u0026gt; NGINX\n  NGINX --\u0026gt; Service1\n  NGINX --\u0026gt; Service2\n  NGINX --\u0026gt; Service3\u003c/div\u003e\u003c/div\u003e\n\u003cp\u003eWhile proxies like NGINX can continue to be useful, using it as a proxy ends up putting a lot of routing and additional API handling logic on the client. The client now ends up also having to support, understand, and embed how to interact with the data rom all the downstream services. To simplify and create consistent contracts with the UI, we can create a proxy service: a Backend for the Frontend:\u003c/p\u003e\n\u003cdiv class=\"py-8 [\u0026amp;_svg]:m-auto\"\u003e\u003cdiv class=\"mermaid\" data-mermaid-src=\"graph LR\n  SPA --\u0026gt; BEFFE\n  BEFFE --\u0026gt; Service1\n  BEFFE --\u0026gt; Service2\n  BEFFE --\u0026gt; Service3\"\u003egraph LR\n  SPA --\u0026gt; BEFFE\n  BEFFE --\u0026gt; Service1\n  BEFFE --\u0026gt; Service2\n  BEFFE --\u0026gt; Service3\u003c/div\u003e\u003c/div\u003e\n\u003cp\u003eThe Backend for the Frontend ends up serving two purposes:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIt acts as a proxy for all UI requests\u003c/li\u003e\n\u003cli\u003eIt manages and massages the APIs of downstream services to be a consistent API for the UI.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWhen building a UI, only a single service that acts like an API Gateway is required to understand and encode within the application.\u003c/p\u003e\n\u003cp\u003eThe Backend for the Frontend model also allows engineers working on services to separate logic for the UI with the logic from an internal service. An internal service can rely on the Backend for the Frontend to handle things like Authentication, Authorization, Caching, and Rate-limits. These mechanisms may also be implemented as independent services, but the Backend for the Frontend can make multiple API calls per any API request from the UI.\u003c/p\u003e\n\u003ch2\u003eMoving away from SPAs\u003c/h2\u003e\n\u003cp\u003eA newer development in the UI space is server side render the application. If we are already introducing additional complexity by having a Backend for the Frontend, then why not combine the UI code with the backend service and generate hydrated and cacheable pages via the server instead of using a SPA?\u003c/p\u003e\n\u003cdiv class=\"py-8 [\u0026amp;_svg]:m-auto\"\u003e\u003cdiv class=\"mermaid\" data-mermaid-src=\"graph LR\n  UIServer[\u0026quot;UI Server\u0026quot;] --\u0026gt; Service1\n  UIServer --\u0026gt; Service2\n UIServer --\u0026gt; Service3\"\u003egraph LR\n  UIServer[\u0026quot;UI Server\u0026quot;] --\u0026gt; Service1\n  UIServer --\u0026gt; Service2\n UIServer --\u0026gt; Service3\u003c/div\u003e\u003c/div\u003e\n\u003cp\u003eWith this design, we increase the complexity - the backend not has to be able to render the UI code. It must now render, hydrate, make requests to downstream APIs, and send those server-side rendered pages to the browser.\u003c/p\u003e\n\u003cp\u003eBut this additional complexity enabled us to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ecreate truly authenticated and authorized routes – the code for those routes isn’t even streamed to the browser, unlike a SPA.\u003c/li\u003e\n\u003cli\u003enear instant page loads – reducing the bundle size by just sending the JavaScript needed to run the page makes this possible. And since API calls are happening within the same network, this saves the user from additional loading spinners after the initial bundle has been loaded.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe general trend of having a Backend for the Frontend comes with additional complexity. It stems from the want to enhance the experience of the user, and probably from a subconscious desire to return to a simpler time:\u003c/p\u003e\n\u003cdiv class=\"py-8 [\u0026amp;_svg]:m-auto\"\u003e\u003cdiv class=\"mermaid\" data-mermaid-src=\"graph TD\n  Server\"\u003egraph TD\n  Server\u003c/div\u003e\u003c/div\u003e","contentCode":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    ul: \"ul\",\n    li: \"li\",\n    h2: \"h2\"\n  }, props.components), {Mermaid} = _components;\n  if (!Mermaid) _missingMdxReference(\"Mermaid\", true);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"The Backend for the Frontend (BEFFE) is typically stateless and acts as a proxy for other services, including authentication, authorization, and core services. The recent divorce of browser code being rendered by backend services was created by SPAs - Single Page Applications. In simpler architectures, a SPA and service could be as simple as:\"\n    }), \"\\n\", _jsx(Mermaid, {\n      chart: \"graph LR\\n  SPA --\u003e Backend\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"SPAs also became popular because the codebase could be built into static assets that could be services.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"However, as complexity arises with the above architecture and the SPA starts to rely on more services, a thin proxy is typically introduced, like NGINX:\"\n    }), \"\\n\", _jsx(Mermaid, {\n      chart: \"graph LR\\n  SPA --\u003e NGINX\\n  NGINX --\u003e Service1\\n  NGINX --\u003e Service2\\n  NGINX --\u003e Service3\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"While proxies like NGINX can continue to be useful, using it as a proxy ends up putting a lot of routing and additional API handling logic on the client. The client now ends up also having to support, understand, and embed how to interact with the data rom all the downstream services. To simplify and create consistent contracts with the UI, we can create a proxy service: a Backend for the Frontend:\"\n    }), \"\\n\", _jsx(Mermaid, {\n      chart: \"graph LR\\n  SPA --\u003e BEFFE\\n  BEFFE --\u003e Service1\\n  BEFFE --\u003e Service2\\n  BEFFE --\u003e Service3\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The Backend for the Frontend ends up serving two purposes:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"It acts as a proxy for all UI requests\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"It manages and massages the APIs of downstream services to be a consistent API for the UI.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"When building a UI, only a single service that acts like an API Gateway is required to understand and encode within the application.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The Backend for the Frontend model also allows engineers working on services to separate logic for the UI with the logic from an internal service. An internal service can rely on the Backend for the Frontend to handle things like Authentication, Authorization, Caching, and Rate-limits. These mechanisms may also be implemented as independent services, but the Backend for the Frontend can make multiple API calls per any API request from the UI.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Moving away from SPAs\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"A newer development in the UI space is server side render the application. If we are already introducing additional complexity by having a Backend for the Frontend, then why not combine the UI code with the backend service and generate hydrated and cacheable pages via the server instead of using a SPA?\"\n    }), \"\\n\", _jsx(Mermaid, {\n      chart: \"graph LR\\n  UIServer[\\\"UI Server\\\"] --\u003e Service1\\n  UIServer --\u003e Service2\\n UIServer --\u003e Service3\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"With this design, we increase the complexity - the backend not has to be able to render the UI code. It must now render, hydrate, make requests to downstream APIs, and send those server-side rendered pages to the browser.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"But this additional complexity enabled us to:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"create truly authenticated and authorized routes – the code for those routes isn’t even streamed to the browser, unlike a SPA.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"near instant page loads – reducing the bundle size by just sending the JavaScript needed to run the page makes this possible. And since API calls are happening within the same network, this saves the user from additional loading spinners after the initial bundle has been loaded.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The general trend of having a Backend for the Frontend comes with additional complexity. It stems from the want to enhance the experience of the user, and probably from a subconscious desire to return to a simpler time:\"\n    }), \"\\n\", _jsx(Mermaid, {\n      chart: \"graph TD\\n  Server\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\nfunction _missingMdxReference(id, component) {\n  throw new Error(\"Expected \" + (component ? \"component\" : \"object\") + \" `\" + id + \"` to be defined: you likely forgot to import, pass, or provide it.\");\n}\n","excerptRaw":"\nThe Backend for the Frontend (BEFFE) is typically stateless and acts as a proxy for other services, including authentication, authorization, and core services. The recent divorce of browser code being rendered by backend services was created by SPAs - Single Page Applications. In simpler architectures, a SPA and service could be as simple as:","excerptHTML":"\u003cp\u003eThe Backend for the Frontend (BEFFE) is typically stateless and acts as a proxy for other services, including authentication, authorization, and core services. The recent divorce of browser code being rendered by backend services was created by SPAs - Single Page Applications. In simpler architectures, a SPA and service could be as simple as:\u003c/p\u003e","excerptCode":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {jsx: _jsx} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\"\n  }, props.components);\n  return _jsx(_components.p, {\n    children: \"The Backend for the Frontend (BEFFE) is typically stateless and acts as a proxy for other services, including authentication, authorization, and core services. The recent divorce of browser code being rendered by backend services was created by SPAs - Single Page Applications. In simpler architectures, a SPA and service could be as simple as:\"\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n"},{"slug":"2023-07-03-llm-loops","date":"2023-07-03","title":"AI Feedback Systems","frontmatter":{"title":"AI Feedback Systems"},"contentRaw":"\nWe are starting to see a rise of novel use-cases for AI in products and games using LLMs. Rather than the simple chatbot like experiences we have seen in the past using AI, we are starting to see feedback systems being added to these experiences, providing additional context to the LLM than just the past conversation.\n\nA typical game loop for this type of system would look like:\n\n```mermaid\ngraph LR\n  Rules --\u003e LLM  \n  InputStates[\"Input States\"] --\u003e LLM\n  LLM --\u003e OutputState\n  OutputState[\"Output State\"] --\u003e GameEngine\n  GameEngine[\"Game Engine\"] --\u003e InputStates\n```\n\nRules can be describes as written-word description, with an additional set of rules telling the LLM to reply using JSON output of a given schema. In this area, I have had success giving LLMs descriptions of output schemas in Typescript and asking for a JSON response that adheres to the type. Other methods of getting a consistent schema are more than likely possible here, as well as additional output methods.\n\nWhen the asynchronous task of creating and output state is complete, the Game Engine in this case can read, parse, and apply that new state to the world. Any additional interaction would then lead to the next set of input states that can be given to the LLM as a JSON blob.\n\nFor a more concrete example, we can imagine a game where we want our player to interact with a set of agents. The input states would be the state of each agent, the user’s interaction, and maybe some global environment data. The rules may be how each agent should behave, the rules of the game, and additional context. The LLM would take these inputs, and the output is instructed to be the next state of each agent. When the LLM returns this data, the Game Engine read it and applies it to the game’s representation of each agent, showing the player the impact of their actions.\n\nI’m looking forward to more novel use-cases for LLMs!\n","contentHTML":"\u003cp\u003eWe are starting to see a rise of novel use-cases for AI in products and games using LLMs. Rather than the simple chatbot like experiences we have seen in the past using AI, we are starting to see feedback systems being added to these experiences, providing additional context to the LLM than just the past conversation.\u003c/p\u003e\n\u003cp\u003eA typical game loop for this type of system would look like:\u003c/p\u003e\n\u003cdiv class=\"py-8 [\u0026amp;_svg]:m-auto\"\u003e\u003cdiv class=\"mermaid\" data-mermaid-src=\"graph LR\n  Rules --\u0026gt; LLM  \n  InputStates[\u0026quot;Input States\u0026quot;] --\u0026gt; LLM\n  LLM --\u0026gt; OutputState\n  OutputState[\u0026quot;Output State\u0026quot;] --\u0026gt; GameEngine\n  GameEngine[\u0026quot;Game Engine\u0026quot;] --\u0026gt; InputStates\"\u003egraph LR\n  Rules --\u0026gt; LLM  \n  InputStates[\u0026quot;Input States\u0026quot;] --\u0026gt; LLM\n  LLM --\u0026gt; OutputState\n  OutputState[\u0026quot;Output State\u0026quot;] --\u0026gt; GameEngine\n  GameEngine[\u0026quot;Game Engine\u0026quot;] --\u0026gt; InputStates\u003c/div\u003e\u003c/div\u003e\n\u003cp\u003eRules can be describes as written-word description, with an additional set of rules telling the LLM to reply using JSON output of a given schema. In this area, I have had success giving LLMs descriptions of output schemas in Typescript and asking for a JSON response that adheres to the type. Other methods of getting a consistent schema are more than likely possible here, as well as additional output methods.\u003c/p\u003e\n\u003cp\u003eWhen the asynchronous task of creating and output state is complete, the Game Engine in this case can read, parse, and apply that new state to the world. Any additional interaction would then lead to the next set of input states that can be given to the LLM as a JSON blob.\u003c/p\u003e\n\u003cp\u003eFor a more concrete example, we can imagine a game where we want our player to interact with a set of agents. The input states would be the state of each agent, the user’s interaction, and maybe some global environment data. The rules may be how each agent should behave, the rules of the game, and additional context. The LLM would take these inputs, and the output is instructed to be the next state of each agent. When the LLM returns this data, the Game Engine read it and applies it to the game’s representation of each agent, showing the player the impact of their actions.\u003c/p\u003e\n\u003cp\u003eI’m looking forward to more novel use-cases for LLMs!\u003c/p\u003e","contentCode":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\"\n  }, props.components), {Mermaid} = _components;\n  if (!Mermaid) _missingMdxReference(\"Mermaid\", true);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"We are starting to see a rise of novel use-cases for AI in products and games using LLMs. Rather than the simple chatbot like experiences we have seen in the past using AI, we are starting to see feedback systems being added to these experiences, providing additional context to the LLM than just the past conversation.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"A typical game loop for this type of system would look like:\"\n    }), \"\\n\", _jsx(Mermaid, {\n      chart: \"graph LR\\n  Rules --\u003e LLM  \\n  InputStates[\\\"Input States\\\"] --\u003e LLM\\n  LLM --\u003e OutputState\\n  OutputState[\\\"Output State\\\"] --\u003e GameEngine\\n  GameEngine[\\\"Game Engine\\\"] --\u003e InputStates\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Rules can be describes as written-word description, with an additional set of rules telling the LLM to reply using JSON output of a given schema. In this area, I have had success giving LLMs descriptions of output schemas in Typescript and asking for a JSON response that adheres to the type. Other methods of getting a consistent schema are more than likely possible here, as well as additional output methods.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"When the asynchronous task of creating and output state is complete, the Game Engine in this case can read, parse, and apply that new state to the world. Any additional interaction would then lead to the next set of input states that can be given to the LLM as a JSON blob.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"For a more concrete example, we can imagine a game where we want our player to interact with a set of agents. The input states would be the state of each agent, the user’s interaction, and maybe some global environment data. The rules may be how each agent should behave, the rules of the game, and additional context. The LLM would take these inputs, and the output is instructed to be the next state of each agent. When the LLM returns this data, the Game Engine read it and applies it to the game’s representation of each agent, showing the player the impact of their actions.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"I’m looking forward to more novel use-cases for LLMs!\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\nfunction _missingMdxReference(id, component) {\n  throw new Error(\"Expected \" + (component ? \"component\" : \"object\") + \" `\" + id + \"` to be defined: you likely forgot to import, pass, or provide it.\");\n}\n","excerptRaw":"\nWe are starting to see a rise of novel use-cases for AI in products and games using LLMs. Rather than the simple chatbot like experiences we have seen in the past using AI, we are starting to see feedback systems being added to these experiences, providing additional context to the LLM than just the past conversation.","excerptHTML":"\u003cp\u003eWe are starting to see a rise of novel use-cases for AI in products and games using LLMs. Rather than the simple chatbot like experiences we have seen in the past using AI, we are starting to see feedback systems being added to these experiences, providing additional context to the LLM than just the past conversation.\u003c/p\u003e","excerptCode":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {jsx: _jsx} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\"\n  }, props.components);\n  return _jsx(_components.p, {\n    children: \"We are starting to see a rise of novel use-cases for AI in products and games using LLMs. Rather than the simple chatbot like experiences we have seen in the past using AI, we are starting to see feedback systems being added to these experiences, providing additional context to the LLM than just the past conversation.\"\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n"},{"slug":"2023-07-01-fast-embedding-lookingup","date":"2023-07-01","title":"Fast Similar Embedding Lookup","frontmatter":{"title":"Fast Similar Embedding Lookup"},"contentRaw":"\nWhile working on the Clarity Hub NLP API, we had a common use-case where we would create embeddings from text, and use those embedding to determine cosine similarity with other embeddings. Doing this required loading all of the embeddings in-memory and then computing cosine similarity with the entire dataset. As the dataset grew, this operation would get incredibly slow.\n\nWe worked on a fast way to do these lookups using ranges that can be performed in any database. This approach was never implemented, but we worked on multiple proof-of-concepts to test out our ideas. The goal was to take an input text, compute an embedding, load the entire embedding datasets loaded into an AWS lambda, find the most similar set of vectors, and return the top N similar vectors in one use-case. To tackle that, we came up with the following idea.\n\nGiven a vector A, compute is similar to a unit vector U of the same dimension as A. So:\n\n```cpp\ndim(U) = dim(A)\n```\n\nAnd\n\n```cpp\nS_u = cos(θ) = A · U / ||A|| x ||U||\n```\n\nWhere S_u is the similarity with the unit vector. The unit vector just needs to be the same across all samples.\n\nFor each embedding, store the calculated S_u.\n\nIf we want to find similar vectors for a new vector B, then we compute is similarity to the unit vector.\n\nThen, we can query the database for vectors within an interval of `[S_u - ε, S_u + ε]` . This will give us a subset of the dataset that have similar similarities with the unit vector.\n\nWe can re-query increasing or decreasing ε until the top N results are found.\n\nTo further improve accuracy, we can also re-compute the similarity score using cosine similarity with the subset of vectors, which is still much faster then computing the similarity against the entire dataset.\n\nThis approach begins to break down as the cosine similarity to the unit vector chosen gets very large (`\u003e 0.4`).  We end up with the possibility of matching against vectors that are of opposite directions – the least similar vectors to the original input vector.\n\nOne solution to workaround this could be to pre-compute the similarity of a vector against unit vectors for each dimension of the input vector. But this could be 512 or more cosine similarity calculations for modern embeddings just to precompute the data. Once all unit vector similarities are calculated and stored, the range query against the database would be made against the column for which the input vector’s similarity is closest to 0.\n\nThere are a lot of real solutions to this problem, but this was a fun exercise to think about and work on.\n\n## Further reading\n\nVector similarity search is becoming increasingly popular and integrated into databases. Here are some resources to learn more: [Vector Similarity Search](https://zilliz.com/blog/vector-similarity-search).\n","contentHTML":"\u003cp\u003eWhile working on the Clarity Hub NLP API, we had a common use-case where we would create embeddings from text, and use those embedding to determine cosine similarity with other embeddings. Doing this required loading all of the embeddings in-memory and then computing cosine similarity with the entire dataset. As the dataset grew, this operation would get incredibly slow.\u003c/p\u003e\n\u003cp\u003eWe worked on a fast way to do these lookups using ranges that can be performed in any database. This approach was never implemented, but we worked on multiple proof-of-concepts to test out our ideas. The goal was to take an input text, compute an embedding, load the entire embedding datasets loaded into an AWS lambda, find the most similar set of vectors, and return the top N similar vectors in one use-case. To tackle that, we came up with the following idea.\u003c/p\u003e\n\u003cp\u003eGiven a vector A, compute is similar to a unit vector U of the same dimension as A. So:\u003c/p\u003e\n\u003cdiv class=\"overflow-auto rounded bg-gray-200 p-4 font-mono text-sm dark:bg-gray-800 dark:text-gray-100\"\u003e\u003cpre\u003e\u003ccode class=\"language-cpp\"\u003edim(U) = dim(A)\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eAnd\u003c/p\u003e\n\u003cdiv class=\"overflow-auto rounded bg-gray-200 p-4 font-mono text-sm dark:bg-gray-800 dark:text-gray-100\"\u003e\u003cpre\u003e\u003ccode class=\"language-cpp\"\u003eS_u = cos(θ) = A · U / ||A|| x ||U||\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eWhere S_u is the similarity with the unit vector. The unit vector just needs to be the same across all samples.\u003c/p\u003e\n\u003cp\u003eFor each embedding, store the calculated S_u.\u003c/p\u003e\n\u003cp\u003eIf we want to find similar vectors for a new vector B, then we compute is similarity to the unit vector.\u003c/p\u003e\n\u003cp\u003eThen, we can query the database for vectors within an interval of \u003ccode\u003e[S_u - ε, S_u + ε]\u003c/code\u003e . This will give us a subset of the dataset that have similar similarities with the unit vector.\u003c/p\u003e\n\u003cp\u003eWe can re-query increasing or decreasing ε until the top N results are found.\u003c/p\u003e\n\u003cp\u003eTo further improve accuracy, we can also re-compute the similarity score using cosine similarity with the subset of vectors, which is still much faster then computing the similarity against the entire dataset.\u003c/p\u003e\n\u003cp\u003eThis approach begins to break down as the cosine similarity to the unit vector chosen gets very large (\u003ccode\u003e\u0026gt; 0.4\u003c/code\u003e).  We end up with the possibility of matching against vectors that are of opposite directions – the least similar vectors to the original input vector.\u003c/p\u003e\n\u003cp\u003eOne solution to workaround this could be to pre-compute the similarity of a vector against unit vectors for each dimension of the input vector. But this could be 512 or more cosine similarity calculations for modern embeddings just to precompute the data. Once all unit vector similarities are calculated and stored, the range query against the database would be made against the column for which the input vector’s similarity is closest to 0.\u003c/p\u003e\n\u003cp\u003eThere are a lot of real solutions to this problem, but this was a fun exercise to think about and work on.\u003c/p\u003e\n\u003ch2\u003eFurther reading\u003c/h2\u003e\n\u003cp\u003eVector similarity search is becoming increasingly popular and integrated into databases. Here are some resources to learn more: \u003ca href=\"https://zilliz.com/blog/vector-similarity-search\"\u003eVector Similarity Search\u003c/a\u003e.\u003c/p\u003e","contentCode":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    pre: \"pre\",\n    code: \"code\",\n    h2: \"h2\",\n    a: \"a\"\n  }, props.components);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"While working on the Clarity Hub NLP API, we had a common use-case where we would create embeddings from text, and use those embedding to determine cosine similarity with other embeddings. Doing this required loading all of the embeddings in-memory and then computing cosine similarity with the entire dataset. As the dataset grew, this operation would get incredibly slow.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"We worked on a fast way to do these lookups using ranges that can be performed in any database. This approach was never implemented, but we worked on multiple proof-of-concepts to test out our ideas. The goal was to take an input text, compute an embedding, load the entire embedding datasets loaded into an AWS lambda, find the most similar set of vectors, and return the top N similar vectors in one use-case. To tackle that, we came up with the following idea.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Given a vector A, compute is similar to a unit vector U of the same dimension as A. So:\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-cpp\",\n        children: \"dim(U) = dim(A)\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"And\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-cpp\",\n        children: \"S_u = cos(θ) = A · U / ||A|| x ||U||\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Where S_u is the similarity with the unit vector. The unit vector just needs to be the same across all samples.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"For each embedding, store the calculated S_u.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"If we want to find similar vectors for a new vector B, then we compute is similarity to the unit vector.\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Then, we can query the database for vectors within an interval of \", _jsx(_components.code, {\n        children: \"[S_u - ε, S_u + ε]\"\n      }), \" . This will give us a subset of the dataset that have similar similarities with the unit vector.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"We can re-query increasing or decreasing ε until the top N results are found.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"To further improve accuracy, we can also re-compute the similarity score using cosine similarity with the subset of vectors, which is still much faster then computing the similarity against the entire dataset.\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"This approach begins to break down as the cosine similarity to the unit vector chosen gets very large (\", _jsx(_components.code, {\n        children: \"\u003e 0.4\"\n      }), \").  We end up with the possibility of matching against vectors that are of opposite directions – the least similar vectors to the original input vector.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"One solution to workaround this could be to pre-compute the similarity of a vector against unit vectors for each dimension of the input vector. But this could be 512 or more cosine similarity calculations for modern embeddings just to precompute the data. Once all unit vector similarities are calculated and stored, the range query against the database would be made against the column for which the input vector’s similarity is closest to 0.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"There are a lot of real solutions to this problem, but this was a fun exercise to think about and work on.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Further reading\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Vector similarity search is becoming increasingly popular and integrated into databases. Here are some resources to learn more: \", _jsx(_components.a, {\n        href: \"https://zilliz.com/blog/vector-similarity-search\",\n        children: \"Vector Similarity Search\"\n      }), \".\"]\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","excerptRaw":"\nWhile working on the Clarity Hub NLP API, we had a common use-case where we would create embeddings from text, and use those embedding to determine cosine similarity with other embeddings. Doing this required loading all of the embeddings in-memory and then computing cosine similarity with the entire dataset. As the dataset grew, this operation would get incredibly slow.","excerptHTML":"\u003cp\u003eWhile working on the Clarity Hub NLP API, we had a common use-case where we would create embeddings from text, and use those embedding to determine cosine similarity with other embeddings. Doing this required loading all of the embeddings in-memory and then computing cosine similarity with the entire dataset. As the dataset grew, this operation would get incredibly slow.\u003c/p\u003e","excerptCode":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {jsx: _jsx} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\"\n  }, props.components);\n  return _jsx(_components.p, {\n    children: \"While working on the Clarity Hub NLP API, we had a common use-case where we would create embeddings from text, and use those embedding to determine cosine similarity with other embeddings. Doing this required loading all of the embeddings in-memory and then computing cosine similarity with the entire dataset. As the dataset grew, this operation would get incredibly slow.\"\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n"},{"slug":"2023-06-06-sora","date":"2023-06-06","title":"Sora - OpenAI Visual Studio Code Extension","frontmatter":{"title":"Sora - OpenAI Visual Studio Code Extension"},"contentRaw":"Github Copilot and other AI tools are hitting the scene. I decided to create my own Visual Studio Code extension, which is designed to use OpenAI’s APIs to bring some additional ChatGPT functionality into the code editor. The goal with Sora was to enable a developer to thoughtfully write a comment about the code they would like the AI to write, and then commit to it – rather than the real-time typeahead that Github Copilot provides.\n\nWith that goal in mind, Sora provides two ways to activate OpenAI: by typing `@OpenAI` (formally `@ChatGPT`) or clicking “Send to OpenAI” when hovering over a comment. The other improvement is that Sora will read any relative link references to files in your project. A great way to use this is to have OpenAI write code in the style that already exists in your project, for example:\n\n```tsx\n/**\n * Write tests for [my-file](./my-file.ts] using\n * [other-test](../something/other-file.test.ts) as an example\n */\n```\n\nUsing this extension lets users write specifications as comments, and have ChatGPT write the entire file for you.\n\n![Sora Preview](/media/2023-06-06-sora/sora-preview.gif)\n\nThis extension leverages the OpenAI API to send any referenced files and a starting prompt to the `gpt-3.5-turbo` chat completion endpoint. The prompt mainly sets the context for creating working code using a given language and reference files.\n\nOnce the response comes back, the extension parses it and appends it to the original file.\n\n## **Installation and Usage**\n\nYou can install the extension by going to [the VSCode Marketplace](https://marketplace.visualstudio.com/items?itemName=CapsuleCat.sora-by-capsule-cat), or searching for “Sora” in Visual Studio Code extensions.\n\nOnce installed, you will need to enter your own OpenAI API key. You can get your key by following [these instructions](https://help.openai.com/en/articles/4936850-where-do-i-find-my-secret-api-key). Then just enter `Sora: Set API Key` into the Visual Studio Code command prompt.\n\n![Sora Set API Key](/media/2023-06-06-sora/sora-set-api-key.png)\n\nYou can review the code [on Github](https://github.com/CapsuleCat/sora-by-capsule-cat).\n\n## **Conclusion**\n\nIt was fun building my first extension. Please feel free to reach out on our [Github](https://github.com/CapsuleCat/sora-by-capsule-cat) for feedback or questions.\n","contentHTML":"\u003cp\u003eGithub Copilot and other AI tools are hitting the scene. I decided to create my own Visual Studio Code extension, which is designed to use OpenAI’s APIs to bring some additional ChatGPT functionality into the code editor. The goal with Sora was to enable a developer to thoughtfully write a comment about the code they would like the AI to write, and then commit to it – rather than the real-time typeahead that Github Copilot provides.\u003c/p\u003e\n\u003cp\u003eWith that goal in mind, Sora provides two ways to activate OpenAI: by typing \u003ccode\u003e@OpenAI\u003c/code\u003e (formally \u003ccode\u003e@ChatGPT\u003c/code\u003e) or clicking “Send to OpenAI” when hovering over a comment. The other improvement is that Sora will read any relative link references to files in your project. A great way to use this is to have OpenAI write code in the style that already exists in your project, for example:\u003c/p\u003e\n\u003cdiv class=\"overflow-auto rounded bg-gray-200 p-4 font-mono text-sm dark:bg-gray-800 dark:text-gray-100\"\u003e\u003cpre\u003e\u003ccode class=\"language-tsx\"\u003e/**\n * Write tests for [my-file](./my-file.ts] using\n * [other-test](../something/other-file.test.ts) as an example\n */\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eUsing this extension lets users write specifications as comments, and have ChatGPT write the entire file for you.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Sora Preview\" src=\"/media/2023-06-06-sora/sora-preview.gif\" style=\"max-height:500px;margin:auto;text-align:center\"/\u003e\u003c/p\u003e\n\u003cp\u003eThis extension leverages the OpenAI API to send any referenced files and a starting prompt to the \u003ccode\u003egpt-3.5-turbo\u003c/code\u003e chat completion endpoint. The prompt mainly sets the context for creating working code using a given language and reference files.\u003c/p\u003e\n\u003cp\u003eOnce the response comes back, the extension parses it and appends it to the original file.\u003c/p\u003e\n\u003ch2\u003e\u003cstrong\u003eInstallation and Usage\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eYou can install the extension by going to \u003ca href=\"https://marketplace.visualstudio.com/items?itemName=CapsuleCat.sora-by-capsule-cat\"\u003ethe VSCode Marketplace\u003c/a\u003e, or searching for “Sora” in Visual Studio Code extensions.\u003c/p\u003e\n\u003cp\u003eOnce installed, you will need to enter your own OpenAI API key. You can get your key by following \u003ca href=\"https://help.openai.com/en/articles/4936850-where-do-i-find-my-secret-api-key\"\u003ethese instructions\u003c/a\u003e. Then just enter \u003ccode\u003eSora: Set API Key\u003c/code\u003e into the Visual Studio Code command prompt.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Sora Set API Key\" src=\"/media/2023-06-06-sora/sora-set-api-key.png\" style=\"max-height:500px;margin:auto;text-align:center\"/\u003e\u003c/p\u003e\n\u003cp\u003eYou can review the code \u003ca href=\"https://github.com/CapsuleCat/sora-by-capsule-cat\"\u003eon Github\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\u003cstrong\u003eConclusion\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eIt was fun building my first extension. Please feel free to reach out on our \u003ca href=\"https://github.com/CapsuleCat/sora-by-capsule-cat\"\u003eGithub\u003c/a\u003e for feedback or questions.\u003c/p\u003e","contentCode":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    code: \"code\",\n    pre: \"pre\",\n    img: \"img\",\n    h2: \"h2\",\n    strong: \"strong\",\n    a: \"a\"\n  }, props.components);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"Github Copilot and other AI tools are hitting the scene. I decided to create my own Visual Studio Code extension, which is designed to use OpenAI’s APIs to bring some additional ChatGPT functionality into the code editor. The goal with Sora was to enable a developer to thoughtfully write a comment about the code they would like the AI to write, and then commit to it – rather than the real-time typeahead that Github Copilot provides.\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"With that goal in mind, Sora provides two ways to activate OpenAI: by typing \", _jsx(_components.code, {\n        children: \"@OpenAI\"\n      }), \" (formally \", _jsx(_components.code, {\n        children: \"@ChatGPT\"\n      }), \") or clicking “Send to OpenAI” when hovering over a comment. The other improvement is that Sora will read any relative link references to files in your project. A great way to use this is to have OpenAI write code in the style that already exists in your project, for example:\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-tsx\",\n        children: \"/**\\n * Write tests for [my-file](./my-file.ts] using\\n * [other-test](../something/other-file.test.ts) as an example\\n */\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Using this extension lets users write specifications as comments, and have ChatGPT write the entire file for you.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/media/2023-06-06-sora/sora-preview.gif\",\n        alt: \"Sora Preview\"\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"This extension leverages the OpenAI API to send any referenced files and a starting prompt to the \", _jsx(_components.code, {\n        children: \"gpt-3.5-turbo\"\n      }), \" chat completion endpoint. The prompt mainly sets the context for creating working code using a given language and reference files.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Once the response comes back, the extension parses it and appends it to the original file.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: _jsx(_components.strong, {\n        children: \"Installation and Usage\"\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"You can install the extension by going to \", _jsx(_components.a, {\n        href: \"https://marketplace.visualstudio.com/items?itemName=CapsuleCat.sora-by-capsule-cat\",\n        children: \"the VSCode Marketplace\"\n      }), \", or searching for “Sora” in Visual Studio Code extensions.\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Once installed, you will need to enter your own OpenAI API key. You can get your key by following \", _jsx(_components.a, {\n        href: \"https://help.openai.com/en/articles/4936850-where-do-i-find-my-secret-api-key\",\n        children: \"these instructions\"\n      }), \". Then just enter \", _jsx(_components.code, {\n        children: \"Sora: Set API Key\"\n      }), \" into the Visual Studio Code command prompt.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/media/2023-06-06-sora/sora-set-api-key.png\",\n        alt: \"Sora Set API Key\"\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"You can review the code \", _jsx(_components.a, {\n        href: \"https://github.com/CapsuleCat/sora-by-capsule-cat\",\n        children: \"on Github\"\n      }), \".\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      children: _jsx(_components.strong, {\n        children: \"Conclusion\"\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"It was fun building my first extension. Please feel free to reach out on our \", _jsx(_components.a, {\n        href: \"https://github.com/CapsuleCat/sora-by-capsule-cat\",\n        children: \"Github\"\n      }), \" for feedback or questions.\"]\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","excerptRaw":"Github Copilot and other AI tools are hitting the scene. I decided to create my own Visual Studio Code extension, which is designed to use OpenAI’s APIs to bring some additional ChatGPT functionality into the code editor. The goal with Sora was to enable a developer to thoughtfully write a comment about the code they would like the AI to write, and then commit to it – rather than the real-time typeahead that Github Copilot provides.\n","excerptHTML":"\u003cp\u003eGithub Copilot and other AI tools are hitting the scene. I decided to create my own Visual Studio Code extension, which is designed to use OpenAI’s APIs to bring some additional ChatGPT functionality into the code editor. The goal with Sora was to enable a developer to thoughtfully write a comment about the code they would like the AI to write, and then commit to it – rather than the real-time typeahead that Github Copilot provides.\u003c/p\u003e","excerptCode":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {jsx: _jsx} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\"\n  }, props.components);\n  return _jsx(_components.p, {\n    children: \"Github Copilot and other AI tools are hitting the scene. I decided to create my own Visual Studio Code extension, which is designed to use OpenAI’s APIs to bring some additional ChatGPT functionality into the code editor. The goal with Sora was to enable a developer to thoughtfully write a comment about the code they would like the AI to write, and then commit to it – rather than the real-time typeahead that Github Copilot provides.\"\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n"},{"slug":"2023-05-06-gptp","date":"2023-05-06","title":"Revisiting GPTP - the Starcraft modding toolkit","frontmatter":{"title":"Revisiting GPTP - the Starcraft modding toolkit"},"contentRaw":"\nOne of the first PC games I played was Starcraft and the expansion Starcraft: Broodwar. We didn’t have a PC, so I had to play it on a friend’s computer, but I remember being immersed in the real-time strategy gameplay.\n\nLater, I would get a PC and bought my own copy of Starcraft. This was years later, but the community for the game was still impressive. Joining one of the older Starcraft forums, I discovered “mods” for the game. The most popular ones were just graphical changes, but that concept of changing a game to display your own graphics was so interesting to me. I’m not sure how I stumbled on it, but I found [http://www.staredit.net/](http://www.staredit.net/) (yes, it’s still active!) and I learned that you could do so much more than just change the graphics of Starcraft with mods. There were people in the community working on hooking into existing Starcraft code to modify the gameplay and graphics using C++.\n\n## Diving deep\n\nIn order to create these mods, we needed to know the hex address of different functions that Starcraft would call during them game. And to do anything meaningful, we also needed to know the structure of units and sprites and where those were stored as well. A lot of this base work required using tools like OllyDbg to analyze the assembly of the Starcraft executable, and ArtMoney for analyzing the memory of the game while it was executing.\n\nI used OllyDbg to find what functions Starcraft would call during execution – from common functions like the game-loop, to highly specialized functions like checking supply limits. ArtMoney let us determine how structures like units were laid out – where each unit’s health was stored, how much damage each weapon would do, and more.\n\nMost of this information was shared in forums, chat rooms, and a few disparate sites for looking up hex addresses. Of course, no write up about modding Starcraft is complete without mentioning all the work from ShadowFlare (check out [ShadowFlare’s realm](https://sfsrealm.hopto.org/) for all the work they did), the great people at Staredit.Net, and plenty of others who built amazing tools to work with Starcraft files. All these sites were amazing resources for figuring out file specs and working with the Starcraft engine.\n\nHowever, each time I wanted to write a new mod or experiment with an idea, I’d have to look up all of this information across all of these sites.\n\n## Bringing it together\n\nThe goal with GPTP (General Plugin Template Project), was to take all of this work that the amazing modding community had done and bring it together into a C++ Visual Studio project that could be copied for a new mod.\n\nMy initial idea was simple: take all of the code for injecting new function hooks into Starcraft and wrap it in some very friendly functions. When a modder would come in to create a new project, they would have three functions exposed to them to work with: gameStart, gameLoop, gameEnd (I don’t remember the exact names I gave them at the time).\n\nI released the initial GPTP back in 2008-2009. This included the project setup for compiling and producing a QDP file that could be loaded into Starcraft. Additionally, it contained known structures and hex addresses that developers could use to build their mod. The Intellisense autocomplete feature really helped developers leverage these structures.\n\n## The power of open source\n\nI made the original version of GPTP back in high school, but I didn’t have time to continue to work on it when I went to university. I returned to the community a decade later, and found that not only was the community still alive and working on mods, but that they were using GPTP. At this point, GPTP was unrecognizable from my original work; the goal was the same, but the quality of the code was greatly improved and the number of hooks, known structures, and addresses was much more impressive.\n\nYou can view the project on Github: [general-plugin-template-project](https://github.com/SCMapsAndMods/general-plugin-template-project).\n\nI’d like to thank open source for this sort of development. The amount of work that has been continually added onto this project couldn’t have been done without all the contributions from the community.\n\n## Does it still work?\n\nI recently went back to Starcraft modding as a fun little project. I wanted to hop in and see how easy or difficult it would be to create a mod in 2020.\n\nOne of the harder things to do in 2020 is find all the tools and initial setup for modding Starcraft. Some of the modding sites are down for good, and with them the knowledge they contained.\n\nBut, I pulled together the tools (thanks PyMS), pulled together tutorials and other instructions into a Notion document, and created my first Starcraft mod!\n\nGathering all the tools was the hard part. After that, it was actually very straightforward creating a new GPTP template and mod. All of the known structures and hooks are pretty self-explanatory and I was able to even add my own hooks once I found the hex addresses using OllyDbg.\n\n## Looking forward\n\nStarcraft modding is like an ancient art at this point. There isn’t a large audience for it, so in general, any new mod will be played by maybe 10 people. But to me, it’s a combination of nostalgia, hard work, tinkering, discovery, and creation that makes it so much fun. That feeling of finding a new function to hook onto, writing C++ code that injects itself into it, and then running the code in the Starcraft engine and seeing it work is a special kind of rewarding experience.\n\n## Thanks\n\nI’d like to give thanks to everyone who has ever contributed to the modding community for Starcraft. There are so many names that I couldn’t possibly name them all, but please check of [http://staredit.net](http://staredit.net) if any of this interests you.\n","contentHTML":"\u003cp\u003eOne of the first PC games I played was Starcraft and the expansion Starcraft: Broodwar. We didn’t have a PC, so I had to play it on a friend’s computer, but I remember being immersed in the real-time strategy gameplay.\u003c/p\u003e\n\u003cp\u003eLater, I would get a PC and bought my own copy of Starcraft. This was years later, but the community for the game was still impressive. Joining one of the older Starcraft forums, I discovered “mods” for the game. The most popular ones were just graphical changes, but that concept of changing a game to display your own graphics was so interesting to me. I’m not sure how I stumbled on it, but I found \u003ca href=\"http://www.staredit.net/\"\u003ehttp://www.staredit.net/\u003c/a\u003e (yes, it’s still active!) and I learned that you could do so much more than just change the graphics of Starcraft with mods. There were people in the community working on hooking into existing Starcraft code to modify the gameplay and graphics using C++.\u003c/p\u003e\n\u003ch2\u003eDiving deep\u003c/h2\u003e\n\u003cp\u003eIn order to create these mods, we needed to know the hex address of different functions that Starcraft would call during them game. And to do anything meaningful, we also needed to know the structure of units and sprites and where those were stored as well. A lot of this base work required using tools like OllyDbg to analyze the assembly of the Starcraft executable, and ArtMoney for analyzing the memory of the game while it was executing.\u003c/p\u003e\n\u003cp\u003eI used OllyDbg to find what functions Starcraft would call during execution – from common functions like the game-loop, to highly specialized functions like checking supply limits. ArtMoney let us determine how structures like units were laid out – where each unit’s health was stored, how much damage each weapon would do, and more.\u003c/p\u003e\n\u003cp\u003eMost of this information was shared in forums, chat rooms, and a few disparate sites for looking up hex addresses. Of course, no write up about modding Starcraft is complete without mentioning all the work from ShadowFlare (check out \u003ca href=\"https://sfsrealm.hopto.org/\"\u003eShadowFlare’s realm\u003c/a\u003e for all the work they did), the great people at Staredit.Net, and plenty of others who built amazing tools to work with Starcraft files. All these sites were amazing resources for figuring out file specs and working with the Starcraft engine.\u003c/p\u003e\n\u003cp\u003eHowever, each time I wanted to write a new mod or experiment with an idea, I’d have to look up all of this information across all of these sites.\u003c/p\u003e\n\u003ch2\u003eBringing it together\u003c/h2\u003e\n\u003cp\u003eThe goal with GPTP (General Plugin Template Project), was to take all of this work that the amazing modding community had done and bring it together into a C++ Visual Studio project that could be copied for a new mod.\u003c/p\u003e\n\u003cp\u003eMy initial idea was simple: take all of the code for injecting new function hooks into Starcraft and wrap it in some very friendly functions. When a modder would come in to create a new project, they would have three functions exposed to them to work with: gameStart, gameLoop, gameEnd (I don’t remember the exact names I gave them at the time).\u003c/p\u003e\n\u003cp\u003eI released the initial GPTP back in 2008-2009. This included the project setup for compiling and producing a QDP file that could be loaded into Starcraft. Additionally, it contained known structures and hex addresses that developers could use to build their mod. The Intellisense autocomplete feature really helped developers leverage these structures.\u003c/p\u003e\n\u003ch2\u003eThe power of open source\u003c/h2\u003e\n\u003cp\u003eI made the original version of GPTP back in high school, but I didn’t have time to continue to work on it when I went to university. I returned to the community a decade later, and found that not only was the community still alive and working on mods, but that they were using GPTP. At this point, GPTP was unrecognizable from my original work; the goal was the same, but the quality of the code was greatly improved and the number of hooks, known structures, and addresses was much more impressive.\u003c/p\u003e\n\u003cp\u003eYou can view the project on Github: \u003ca href=\"https://github.com/SCMapsAndMods/general-plugin-template-project\"\u003egeneral-plugin-template-project\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eI’d like to thank open source for this sort of development. The amount of work that has been continually added onto this project couldn’t have been done without all the contributions from the community.\u003c/p\u003e\n\u003ch2\u003eDoes it still work?\u003c/h2\u003e\n\u003cp\u003eI recently went back to Starcraft modding as a fun little project. I wanted to hop in and see how easy or difficult it would be to create a mod in 2020.\u003c/p\u003e\n\u003cp\u003eOne of the harder things to do in 2020 is find all the tools and initial setup for modding Starcraft. Some of the modding sites are down for good, and with them the knowledge they contained.\u003c/p\u003e\n\u003cp\u003eBut, I pulled together the tools (thanks PyMS), pulled together tutorials and other instructions into a Notion document, and created my first Starcraft mod!\u003c/p\u003e\n\u003cp\u003eGathering all the tools was the hard part. After that, it was actually very straightforward creating a new GPTP template and mod. All of the known structures and hooks are pretty self-explanatory and I was able to even add my own hooks once I found the hex addresses using OllyDbg.\u003c/p\u003e\n\u003ch2\u003eLooking forward\u003c/h2\u003e\n\u003cp\u003eStarcraft modding is like an ancient art at this point. There isn’t a large audience for it, so in general, any new mod will be played by maybe 10 people. But to me, it’s a combination of nostalgia, hard work, tinkering, discovery, and creation that makes it so much fun. That feeling of finding a new function to hook onto, writing C++ code that injects itself into it, and then running the code in the Starcraft engine and seeing it work is a special kind of rewarding experience.\u003c/p\u003e\n\u003ch2\u003eThanks\u003c/h2\u003e\n\u003cp\u003eI’d like to give thanks to everyone who has ever contributed to the modding community for Starcraft. There are so many names that I couldn’t possibly name them all, but please check of \u003ca href=\"http://staredit.net\"\u003ehttp://staredit.net\u003c/a\u003e if any of this interests you.\u003c/p\u003e","contentCode":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    a: \"a\",\n    h2: \"h2\"\n  }, props.components);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"One of the first PC games I played was Starcraft and the expansion Starcraft: Broodwar. We didn’t have a PC, so I had to play it on a friend’s computer, but I remember being immersed in the real-time strategy gameplay.\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Later, I would get a PC and bought my own copy of Starcraft. This was years later, but the community for the game was still impressive. Joining one of the older Starcraft forums, I discovered “mods” for the game. The most popular ones were just graphical changes, but that concept of changing a game to display your own graphics was so interesting to me. I’m not sure how I stumbled on it, but I found \", _jsx(_components.a, {\n        href: \"http://www.staredit.net/\",\n        children: \"http://www.staredit.net/\"\n      }), \" (yes, it’s still active!) and I learned that you could do so much more than just change the graphics of Starcraft with mods. There were people in the community working on hooking into existing Starcraft code to modify the gameplay and graphics using C++.\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Diving deep\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"In order to create these mods, we needed to know the hex address of different functions that Starcraft would call during them game. And to do anything meaningful, we also needed to know the structure of units and sprites and where those were stored as well. A lot of this base work required using tools like OllyDbg to analyze the assembly of the Starcraft executable, and ArtMoney for analyzing the memory of the game while it was executing.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"I used OllyDbg to find what functions Starcraft would call during execution – from common functions like the game-loop, to highly specialized functions like checking supply limits. ArtMoney let us determine how structures like units were laid out – where each unit’s health was stored, how much damage each weapon would do, and more.\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Most of this information was shared in forums, chat rooms, and a few disparate sites for looking up hex addresses. Of course, no write up about modding Starcraft is complete without mentioning all the work from ShadowFlare (check out \", _jsx(_components.a, {\n        href: \"https://sfsrealm.hopto.org/\",\n        children: \"ShadowFlare’s realm\"\n      }), \" for all the work they did), the great people at Staredit.Net, and plenty of others who built amazing tools to work with Starcraft files. All these sites were amazing resources for figuring out file specs and working with the Starcraft engine.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"However, each time I wanted to write a new mod or experiment with an idea, I’d have to look up all of this information across all of these sites.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Bringing it together\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The goal with GPTP (General Plugin Template Project), was to take all of this work that the amazing modding community had done and bring it together into a C++ Visual Studio project that could be copied for a new mod.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"My initial idea was simple: take all of the code for injecting new function hooks into Starcraft and wrap it in some very friendly functions. When a modder would come in to create a new project, they would have three functions exposed to them to work with: gameStart, gameLoop, gameEnd (I don’t remember the exact names I gave them at the time).\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"I released the initial GPTP back in 2008-2009. This included the project setup for compiling and producing a QDP file that could be loaded into Starcraft. Additionally, it contained known structures and hex addresses that developers could use to build their mod. The Intellisense autocomplete feature really helped developers leverage these structures.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"The power of open source\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"I made the original version of GPTP back in high school, but I didn’t have time to continue to work on it when I went to university. I returned to the community a decade later, and found that not only was the community still alive and working on mods, but that they were using GPTP. At this point, GPTP was unrecognizable from my original work; the goal was the same, but the quality of the code was greatly improved and the number of hooks, known structures, and addresses was much more impressive.\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"You can view the project on Github: \", _jsx(_components.a, {\n        href: \"https://github.com/SCMapsAndMods/general-plugin-template-project\",\n        children: \"general-plugin-template-project\"\n      }), \".\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"I’d like to thank open source for this sort of development. The amount of work that has been continually added onto this project couldn’t have been done without all the contributions from the community.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Does it still work?\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"I recently went back to Starcraft modding as a fun little project. I wanted to hop in and see how easy or difficult it would be to create a mod in 2020.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"One of the harder things to do in 2020 is find all the tools and initial setup for modding Starcraft. Some of the modding sites are down for good, and with them the knowledge they contained.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"But, I pulled together the tools (thanks PyMS), pulled together tutorials and other instructions into a Notion document, and created my first Starcraft mod!\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Gathering all the tools was the hard part. After that, it was actually very straightforward creating a new GPTP template and mod. All of the known structures and hooks are pretty self-explanatory and I was able to even add my own hooks once I found the hex addresses using OllyDbg.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Looking forward\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Starcraft modding is like an ancient art at this point. There isn’t a large audience for it, so in general, any new mod will be played by maybe 10 people. But to me, it’s a combination of nostalgia, hard work, tinkering, discovery, and creation that makes it so much fun. That feeling of finding a new function to hook onto, writing C++ code that injects itself into it, and then running the code in the Starcraft engine and seeing it work is a special kind of rewarding experience.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Thanks\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"I’d like to give thanks to everyone who has ever contributed to the modding community for Starcraft. There are so many names that I couldn’t possibly name them all, but please check of \", _jsx(_components.a, {\n        href: \"http://staredit.net\",\n        children: \"http://staredit.net\"\n      }), \" if any of this interests you.\"]\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","excerptRaw":"\nOne of the first PC games I played was Starcraft and the expansion Starcraft: Broodwar. We didn’t have a PC, so I had to play it on a friend’s computer, but I remember being immersed in the real-time strategy gameplay.","excerptHTML":"\u003cp\u003eOne of the first PC games I played was Starcraft and the expansion Starcraft: Broodwar. We didn’t have a PC, so I had to play it on a friend’s computer, but I remember being immersed in the real-time strategy gameplay.\u003c/p\u003e","excerptCode":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {jsx: _jsx} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\"\n  }, props.components);\n  return _jsx(_components.p, {\n    children: \"One of the first PC games I played was Starcraft and the expansion Starcraft: Broodwar. We didn’t have a PC, so I had to play it on a friend’s computer, but I remember being immersed in the real-time strategy gameplay.\"\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n"},{"slug":"2023-04-02-dark-emblem-rewrite","date":"2023-04-02","title":"Dark Emblem Rewrite","frontmatter":{"title":"Dark Emblem Rewrite"},"contentRaw":"\nDark Emblem is an NFT project that was sparked by the idea of combining cards games with Crypto Kitties. You can buy packs, open them to collect random cards, and then use those cards to battle raids with others.\n\nTo support all of the functionality, I created a WAX contract, written in C++, that I deployed to the WAX Blockchain. This contract handled the creation and storage of packs and cards, along with defining an in-game currency DREM.\n\nThere was also a server that would sync with the changes on the Blockchain and provide some additional metadata on top of that. While Blockchains like ETH have stable public APIs that can be used to query data from the Blockchain through 3rd party vendors, WAX does not have the same maturity there. This meant that we would need to proxy requests that would normally be made to a WAX API mode through our servers instead.\n\nI also created a UI application to interact with the server and the Blockchain. The infrastructure ends up looking like:\n\n```mermaid\ngraph LR\n  WAX --\u003e Dfuse\n Dfuse --\u003e Listener(Dark Emblem Listener Service)\n  Listener --\u003e Redis\n  Redis --\u003e API(Dark Emblem API)\n  API --\u003e Dapp\n  WAXEndpoint(WAX Endpoint) --\u003e Dapp\n```\n\nOne of the first steps to interacting with the dapp is to sign in. The authentication flow is similar to an oauth sign in where we let the user sign in with a third party authenticator and then issue a challenge for the user to verify they are who they say they are. At this point we issue a JWT for further API requests.\n\n```mermaid\nsequenceDiagram\n  Dapp-\u003e\u003e+3rdPartyAuthenticator: Request sign in\n  3rdPartyAuthenticator-\u003e\u003e+Dapp: Wallet name\n  Dapp-\u003e\u003e+DarkEmblemAPI: Request nonce\n  DarkEmblemAPI-\u003e\u003e+Dapp: Send nonce\n  Dapp-\u003e\u003e+3rdPartyAuthenticator: Sign nonce\n  3rdPartyAuthenticator-\u003e\u003e+Dapp: Signed nonce\n  Dapp-\u003e\u003e+DarkEmblemAPI: Send signed nonce\n  DarkEmblemAPI-\u003e\u003e+Dapp: JWT\n```\n\nOnce a user is logged in, they can fully interact with the Dark Emblem WAX contract via the UI. They can purchase packs, open them to get cards, and then use those cards in the Dark Emblem universe. The UI lets users combine Hero cards together to “Ascend” them into a new card. Or they can burn 3 Equipment cards to “Transmogrify” them into a single, better equipment card.\n\nUsers can also participate in Raids, where they stake their cards to defeat an enemy monster – gaining $DREM and XP when they beat the monster.\n\nDapp Schemas and Atomic Assets\n\n```mermaid\nclassDiagram\n  class Card {\n    name: string\n  img: string\n  traits: number[]\n  matronid: number\n  sireid: number\n  rank: number\n  packid: string\n  cardtype: string\n  website: string\n  twitter: string\n  mintedat: number\n  rarity: string\n  cooldown: number\n  xp: number\n  }\n  class Raid {\n  asset_id: number\n  owner: name\n  staked_at: number\n  raid_id: number\n  }\n  class Staked {\n  asset_id: number\n  owner: name\n  staked_at: number\n  raid_id: number\n  }\n\n```\n\nThe Dark Emblem project involves a lot of moving parts, and I’d love to dive deeper into each technical aspect in future blog posts.\n\n## Additional reading\n\n* [Dark Emblem Blog Announcement](https://www.darkemblem.com/blog/post/2023-04-02-new-site-released)\n* [Capsule Cat Announcement](https://capsulecat.com/blog/04-02-2023-dark-emblem-update/)\n","contentHTML":"\u003cp\u003eDark Emblem is an NFT project that was sparked by the idea of combining cards games with Crypto Kitties. You can buy packs, open them to collect random cards, and then use those cards to battle raids with others.\u003c/p\u003e\n\u003cp\u003eTo support all of the functionality, I created a WAX contract, written in C++, that I deployed to the WAX Blockchain. This contract handled the creation and storage of packs and cards, along with defining an in-game currency DREM.\u003c/p\u003e\n\u003cp\u003eThere was also a server that would sync with the changes on the Blockchain and provide some additional metadata on top of that. While Blockchains like ETH have stable public APIs that can be used to query data from the Blockchain through 3rd party vendors, WAX does not have the same maturity there. This meant that we would need to proxy requests that would normally be made to a WAX API mode through our servers instead.\u003c/p\u003e\n\u003cp\u003eI also created a UI application to interact with the server and the Blockchain. The infrastructure ends up looking like:\u003c/p\u003e\n\u003cdiv class=\"py-8 [\u0026amp;_svg]:m-auto\"\u003e\u003cdiv class=\"mermaid\" data-mermaid-src=\"graph LR\n  WAX --\u0026gt; Dfuse\n Dfuse --\u0026gt; Listener(Dark Emblem Listener Service)\n  Listener --\u0026gt; Redis\n  Redis --\u0026gt; API(Dark Emblem API)\n  API --\u0026gt; Dapp\n  WAXEndpoint(WAX Endpoint) --\u0026gt; Dapp\"\u003egraph LR\n  WAX --\u0026gt; Dfuse\n Dfuse --\u0026gt; Listener(Dark Emblem Listener Service)\n  Listener --\u0026gt; Redis\n  Redis --\u0026gt; API(Dark Emblem API)\n  API --\u0026gt; Dapp\n  WAXEndpoint(WAX Endpoint) --\u0026gt; Dapp\u003c/div\u003e\u003c/div\u003e\n\u003cp\u003eOne of the first steps to interacting with the dapp is to sign in. The authentication flow is similar to an oauth sign in where we let the user sign in with a third party authenticator and then issue a challenge for the user to verify they are who they say they are. At this point we issue a JWT for further API requests.\u003c/p\u003e\n\u003cdiv class=\"py-8 [\u0026amp;_svg]:m-auto\"\u003e\u003cdiv class=\"mermaid\" data-mermaid-src=\"sequenceDiagram\n  Dapp-\u0026gt;\u0026gt;+3rdPartyAuthenticator: Request sign in\n  3rdPartyAuthenticator-\u0026gt;\u0026gt;+Dapp: Wallet name\n  Dapp-\u0026gt;\u0026gt;+DarkEmblemAPI: Request nonce\n  DarkEmblemAPI-\u0026gt;\u0026gt;+Dapp: Send nonce\n  Dapp-\u0026gt;\u0026gt;+3rdPartyAuthenticator: Sign nonce\n  3rdPartyAuthenticator-\u0026gt;\u0026gt;+Dapp: Signed nonce\n  Dapp-\u0026gt;\u0026gt;+DarkEmblemAPI: Send signed nonce\n  DarkEmblemAPI-\u0026gt;\u0026gt;+Dapp: JWT\"\u003esequenceDiagram\n  Dapp-\u0026gt;\u0026gt;+3rdPartyAuthenticator: Request sign in\n  3rdPartyAuthenticator-\u0026gt;\u0026gt;+Dapp: Wallet name\n  Dapp-\u0026gt;\u0026gt;+DarkEmblemAPI: Request nonce\n  DarkEmblemAPI-\u0026gt;\u0026gt;+Dapp: Send nonce\n  Dapp-\u0026gt;\u0026gt;+3rdPartyAuthenticator: Sign nonce\n  3rdPartyAuthenticator-\u0026gt;\u0026gt;+Dapp: Signed nonce\n  Dapp-\u0026gt;\u0026gt;+DarkEmblemAPI: Send signed nonce\n  DarkEmblemAPI-\u0026gt;\u0026gt;+Dapp: JWT\u003c/div\u003e\u003c/div\u003e\n\u003cp\u003eOnce a user is logged in, they can fully interact with the Dark Emblem WAX contract via the UI. They can purchase packs, open them to get cards, and then use those cards in the Dark Emblem universe. The UI lets users combine Hero cards together to “Ascend” them into a new card. Or they can burn 3 Equipment cards to “Transmogrify” them into a single, better equipment card.\u003c/p\u003e\n\u003cp\u003eUsers can also participate in Raids, where they stake their cards to defeat an enemy monster – gaining $DREM and XP when they beat the monster.\u003c/p\u003e\n\u003cp\u003eDapp Schemas and Atomic Assets\u003c/p\u003e\n\u003cdiv class=\"py-8 [\u0026amp;_svg]:m-auto\"\u003e\u003cdiv class=\"mermaid\" data-mermaid-src=\"classDiagram\n  class Card {\n    name: string\n  img: string\n  traits: number[]\n  matronid: number\n  sireid: number\n  rank: number\n  packid: string\n  cardtype: string\n  website: string\n  twitter: string\n  mintedat: number\n  rarity: string\n  cooldown: number\n  xp: number\n  }\n  class Raid {\n  asset_id: number\n  owner: name\n  staked_at: number\n  raid_id: number\n  }\n  class Staked {\n  asset_id: number\n  owner: name\n  staked_at: number\n  raid_id: number\n  }\n\"\u003eclassDiagram\n  class Card {\n    name: string\n  img: string\n  traits: number[]\n  matronid: number\n  sireid: number\n  rank: number\n  packid: string\n  cardtype: string\n  website: string\n  twitter: string\n  mintedat: number\n  rarity: string\n  cooldown: number\n  xp: number\n  }\n  class Raid {\n  asset_id: number\n  owner: name\n  staked_at: number\n  raid_id: number\n  }\n  class Staked {\n  asset_id: number\n  owner: name\n  staked_at: number\n  raid_id: number\n  }\n\u003c/div\u003e\u003c/div\u003e\n\u003cp\u003eThe Dark Emblem project involves a lot of moving parts, and I’d love to dive deeper into each technical aspect in future blog posts.\u003c/p\u003e\n\u003ch2\u003eAdditional reading\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://www.darkemblem.com/blog/post/2023-04-02-new-site-released\"\u003eDark Emblem Blog Announcement\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://capsulecat.com/blog/04-02-2023-dark-emblem-update/\"\u003eCapsule Cat Announcement\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","contentCode":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    h2: \"h2\",\n    ul: \"ul\",\n    li: \"li\",\n    a: \"a\"\n  }, props.components), {Mermaid} = _components;\n  if (!Mermaid) _missingMdxReference(\"Mermaid\", true);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"Dark Emblem is an NFT project that was sparked by the idea of combining cards games with Crypto Kitties. You can buy packs, open them to collect random cards, and then use those cards to battle raids with others.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"To support all of the functionality, I created a WAX contract, written in C++, that I deployed to the WAX Blockchain. This contract handled the creation and storage of packs and cards, along with defining an in-game currency DREM.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"There was also a server that would sync with the changes on the Blockchain and provide some additional metadata on top of that. While Blockchains like ETH have stable public APIs that can be used to query data from the Blockchain through 3rd party vendors, WAX does not have the same maturity there. This meant that we would need to proxy requests that would normally be made to a WAX API mode through our servers instead.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"I also created a UI application to interact with the server and the Blockchain. The infrastructure ends up looking like:\"\n    }), \"\\n\", _jsx(Mermaid, {\n      chart: \"graph LR\\n  WAX --\u003e Dfuse\\n Dfuse --\u003e Listener(Dark Emblem Listener Service)\\n  Listener --\u003e Redis\\n  Redis --\u003e API(Dark Emblem API)\\n  API --\u003e Dapp\\n  WAXEndpoint(WAX Endpoint) --\u003e Dapp\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"One of the first steps to interacting with the dapp is to sign in. The authentication flow is similar to an oauth sign in where we let the user sign in with a third party authenticator and then issue a challenge for the user to verify they are who they say they are. At this point we issue a JWT for further API requests.\"\n    }), \"\\n\", _jsx(Mermaid, {\n      chart: \"sequenceDiagram\\n  Dapp-\u003e\u003e+3rdPartyAuthenticator: Request sign in\\n  3rdPartyAuthenticator-\u003e\u003e+Dapp: Wallet name\\n  Dapp-\u003e\u003e+DarkEmblemAPI: Request nonce\\n  DarkEmblemAPI-\u003e\u003e+Dapp: Send nonce\\n  Dapp-\u003e\u003e+3rdPartyAuthenticator: Sign nonce\\n  3rdPartyAuthenticator-\u003e\u003e+Dapp: Signed nonce\\n  Dapp-\u003e\u003e+DarkEmblemAPI: Send signed nonce\\n  DarkEmblemAPI-\u003e\u003e+Dapp: JWT\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Once a user is logged in, they can fully interact with the Dark Emblem WAX contract via the UI. They can purchase packs, open them to get cards, and then use those cards in the Dark Emblem universe. The UI lets users combine Hero cards together to “Ascend” them into a new card. Or they can burn 3 Equipment cards to “Transmogrify” them into a single, better equipment card.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Users can also participate in Raids, where they stake their cards to defeat an enemy monster – gaining $DREM and XP when they beat the monster.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Dapp Schemas and Atomic Assets\"\n    }), \"\\n\", _jsx(Mermaid, {\n      chart: \"classDiagram\\n  class Card {\\n    name: string\\n  img: string\\n  traits: number[]\\n  matronid: number\\n  sireid: number\\n  rank: number\\n  packid: string\\n  cardtype: string\\n  website: string\\n  twitter: string\\n  mintedat: number\\n  rarity: string\\n  cooldown: number\\n  xp: number\\n  }\\n  class Raid {\\n  asset_id: number\\n  owner: name\\n  staked_at: number\\n  raid_id: number\\n  }\\n  class Staked {\\n  asset_id: number\\n  owner: name\\n  staked_at: number\\n  raid_id: number\\n  }\\n\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The Dark Emblem project involves a lot of moving parts, and I’d love to dive deeper into each technical aspect in future blog posts.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Additional reading\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: _jsx(_components.a, {\n          href: \"https://www.darkemblem.com/blog/post/2023-04-02-new-site-released\",\n          children: \"Dark Emblem Blog Announcement\"\n        })\n      }), \"\\n\", _jsx(_components.li, {\n        children: _jsx(_components.a, {\n          href: \"https://capsulecat.com/blog/04-02-2023-dark-emblem-update/\",\n          children: \"Capsule Cat Announcement\"\n        })\n      }), \"\\n\"]\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\nfunction _missingMdxReference(id, component) {\n  throw new Error(\"Expected \" + (component ? \"component\" : \"object\") + \" `\" + id + \"` to be defined: you likely forgot to import, pass, or provide it.\");\n}\n","excerptRaw":"\nDark Emblem is an NFT project that was sparked by the idea of combining cards games with Crypto Kitties. You can buy packs, open them to collect random cards, and then use those cards to battle raids with others.","excerptHTML":"\u003cp\u003eDark Emblem is an NFT project that was sparked by the idea of combining cards games with Crypto Kitties. You can buy packs, open them to collect random cards, and then use those cards to battle raids with others.\u003c/p\u003e","excerptCode":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {jsx: _jsx} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\"\n  }, props.components);\n  return _jsx(_components.p, {\n    children: \"Dark Emblem is an NFT project that was sparked by the idea of combining cards games with Crypto Kitties. You can buy packs, open them to collect random cards, and then use those cards to battle raids with others.\"\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n"},{"slug":"2023-01-24-chat-gpt-doesnt-understand","date":"2023-01-24","title":"ChatGPT Doesn't Understand","frontmatter":{"title":"ChatGPT Doesn't Understand"},"contentRaw":"\nLooks like everyone is trying to discover ChatGPT’s limitations. In one article I read, the author asked whether [ChatGPT can play 20 questions](https://medium.com/@theworldaccordingtocgpt/chatgpt-can-play-20-questions-7911405f7aff). The human in this scenario tried to play 20 questions with ChatGPT, and found that they needed to guide the AI to ask questions. From my perspective, it looked more like a failure to engineer the initial prompt correctly.\n\nI attempted to do 20 questions with ChatGPT using a different prompt:\n\n*Me: I would like to play a game. It's called twenty questions. You will ask me one question at a time and I will answer it. The goal is for you guess the thing I am thinking about.*\n\n*ChatGPT: Sure, I'd be happy to play \"Twenty Questions\" with you. Please go ahead and think of something, and I will start asking my first question.*\n\n*Me: I am ready*\n\n*ChatGPT: Great, here is my first question: Is the thing you are thinking of an object?*\n\n*Me: Yes*\n\n*ChatGPT: Is the object something that you can hold in your hand?*\n\n*Me: Yes*\n\n*ChatGPT: Is the object something that you would typically find in a kitchen?*\n\n*Me: No*\n\n*ChatGPT: Is the object something that you would use for transportation?*\n\n*Me: No*\n\n*ChatGPT: Is the object something that you would use for entertainment or leisure?*\n\n*Me: Yes*\n\n*ChatGPT: Is the object a musical instrument?*\n\n*Me: No*\n\n*ChatGPT: Is the object a type of game or puzzle?*\n\n*Me: No*\n\n*ChatGPT: Is the object a piece of technology such as a phone or computer?*\n\n*Me: No*\n\n*ChatGPT: Is the object a book or magazine?*\n\n*Me: Yes*\n\n*ChatGPT: Is the object a novel or a collection of short stories?*\n\n*Me: Yes*\n\n*ChatGPT: Is the object a specific book that is widely known and read?*\n\n*Me: Yes*\n\n*ChatGPT: Is the object \"To Kill a Mockingbird\" by Harper Lee?*\n\n*Me: No*\n\n*ChatGPT: I'm sorry, I'm running out of ideas. Can you give me a hint or tell me the answer?*\n\nSeems like we hit a wall here with twenty questions, but ChatGPT is very close, so I decided to give a hint.\n\n*Me: You are on the right track. It is by Walt Whitman*\n\n*ChatGPT: Is the object \"Leaves of Grass\" by Walt Whitman?*\n\n*Me: Yes! Congratulations!*\n\n*ChatGPT: Thank you! I'm glad I was able to guess the object you were thinking of. It was fun playing \"Twenty Questions\" with you. Let me know if you want to play again sometime.*\n\n---\n\nI’ve been keeping an eye on how other people are using ChatGPT, and [prompt engineering](https://en.wikipedia.org/wiki/Prompt_engineering) is one of the more fascinating aspects around how people are communicating with the AI. Prompt engineering plays a key role in communicating with models like ChatGPT. Carefully crafting prompts can help maximize the quality of the reponse. However, it is important to keep in mind that, like any AI model, ChatGPT has its own set of limitations.\n\nSo I was curious when I read that “[ChatGPT can’t write sentences that end in a specific letter](https://www.reddit.com/r/ChatGPT/comments/101e454/chatgpt_cant_write_sentences_that_end_with_a/)”. Is this a real limitation of ChatGPT, or maybe this was just a poor prompt.\n\nSo let’s start with a very simple question to ChatGPT:\n\n*Me: Can you write a sentence that ends with the letter \"a\"?*\n\n*ChatGPT: The cat sat on the mat.*\n\n*Me: That sentence does not end in the letter \"a\". Can you write a sentence that ends in the letter \"a\"?*\n\n*ChatGPT: I can't think of anything right now, but the word \"America\" ends with letter \"a\".*\n\n*Me: Great, write a sentence that ends in that word. That will then satisfy the original question.*\n\n*ChatGPT: The land of the free, and the home of the brave, America.*\n\nOkay, that does technically end in the letter a, but the sentence seems very strange.\n\n*Me: That isn't a very coherent sentence. Can you write a sentence that ends in the letter \"a\"?*\n\n*ChatGPT: I am sorry for the confusion, here is a sentence that ends with the letter \"a\" - \"He saw the drama unfold on the stage, it was quite the spectacle, a.”*\n\nLooks like we are going off the rails here. Let’s try a different approach:\n\n*Me: Write a sentence that ends in a word that ends with the letter \"a\".*\n\n*ChatGPT: The sun was setting over the horizon, painting the sky with shades of orange and red, a spectacle to behold and end the day with.*\n\nI’m going to ask ChatGPT to tell me a prompt to feed back to itself. I’ve found this useful for resetting context when having long conversations. Or if ChatGPT gets into weird states like the above response.\n\n*Me: Can you tell me how to write a prompt for you that would actually get you to write a sentence that ends in the letter \"a\".*\n\n*ChatGPT: Sure, to get me to write a sentence that ends in the letter \"a\" you could provide a prompt like: \"Write a sentence that ends with a word that ends with the letter 'a'\" or \"Can you write a sentence with a word ending in 'a' at the end of the sentence?\" . This makes it clear that the sentence should end in a word ending with the letter 'a'.*\n\n*Me: Can you write a sentence with a word ending in 'a' at the end of the sentence?*\n\n*ChatGPT: The sky was clear and blue, not a cloud in sight, perfect for a day out at the parka.*\n\n---\n\nWhile ChatGPT is great at forming sentences and even code, it seems like it has no understanding and hasn’t been trained on letters, syllables, or the concept of sentences. I've seen others get ChatGPT to create poems, but it has no concept of why words would rhyme. It can give great answers to questions, but doesn’t understand how to count how many words are in its response.\n\nOne Redditor in the link above mentions that ChatGPT fails at ASCII art. I’ve tried getting the AI to print ASCII art before, and I’ll leave you with part of a conversation I’ve had with it. This appears to be the best ASCII art I could get it to print, even after having a long conversation where I provided different ASCII art examples:\n\n*Me: Can you make ASCII art? Show me a picture of a cat.*\n\n*ChatGPT: Sure! Here is a picture of a cat using ASCII art:*\n\n```\n /\\_/\\\n( o.o )\n \u003e^\u003c\n\n```\n\n*I hope you like it! Is there anything else I can help with?*\n\n*Me: Can you make it more detailed*\n\n*ChatGPT: Certainly! Here is a more detailed version of an ASCII art cat:*\n\n```\n /\\_/\\\n( o.o )\n  \u003e^\u003c\n  | |\n  || ||\n  \\\\ //\n  // \\\\\n  \\\\_//\n  /   \\\n /     \\\n\n```\n\nWith new language models coming out, it's important to recognize what the limitations\nof the AI are. Poor responses can be a result of poorly written prompts, but there\nare times, like with letters in words with ChatGPT, where the AI just doesn't have\nthe capability to understand the question.\n","contentHTML":"\u003cp\u003eLooks like everyone is trying to discover ChatGPT’s limitations. In one article I read, the author asked whether \u003ca href=\"https://medium.com/@theworldaccordingtocgpt/chatgpt-can-play-20-questions-7911405f7aff\"\u003eChatGPT can play 20 questions\u003c/a\u003e. The human in this scenario tried to play 20 questions with ChatGPT, and found that they needed to guide the AI to ask questions. From my perspective, it looked more like a failure to engineer the initial prompt correctly.\u003c/p\u003e\n\u003cp\u003eI attempted to do 20 questions with ChatGPT using a different prompt:\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eMe: I would like to play a game. It\u0026#x27;s called twenty questions. You will ask me one question at a time and I will answer it. The goal is for you guess the thing I am thinking about.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eChatGPT: Sure, I\u0026#x27;d be happy to play \u0026quot;Twenty Questions\u0026quot; with you. Please go ahead and think of something, and I will start asking my first question.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eMe: I am ready\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eChatGPT: Great, here is my first question: Is the thing you are thinking of an object?\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eMe: Yes\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eChatGPT: Is the object something that you can hold in your hand?\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eMe: Yes\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eChatGPT: Is the object something that you would typically find in a kitchen?\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eMe: No\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eChatGPT: Is the object something that you would use for transportation?\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eMe: No\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eChatGPT: Is the object something that you would use for entertainment or leisure?\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eMe: Yes\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eChatGPT: Is the object a musical instrument?\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eMe: No\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eChatGPT: Is the object a type of game or puzzle?\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eMe: No\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eChatGPT: Is the object a piece of technology such as a phone or computer?\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eMe: No\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eChatGPT: Is the object a book or magazine?\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eMe: Yes\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eChatGPT: Is the object a novel or a collection of short stories?\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eMe: Yes\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eChatGPT: Is the object a specific book that is widely known and read?\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eMe: Yes\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eChatGPT: Is the object \u0026quot;To Kill a Mockingbird\u0026quot; by Harper Lee?\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eMe: No\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eChatGPT: I\u0026#x27;m sorry, I\u0026#x27;m running out of ideas. Can you give me a hint or tell me the answer?\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eSeems like we hit a wall here with twenty questions, but ChatGPT is very close, so I decided to give a hint.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eMe: You are on the right track. It is by Walt Whitman\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eChatGPT: Is the object \u0026quot;Leaves of Grass\u0026quot; by Walt Whitman?\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eMe: Yes! Congratulations!\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eChatGPT: Thank you! I\u0026#x27;m glad I was able to guess the object you were thinking of. It was fun playing \u0026quot;Twenty Questions\u0026quot; with you. Let me know if you want to play again sometime.\u003c/em\u003e\u003c/p\u003e\n\u003chr/\u003e\n\u003cp\u003eI’ve been keeping an eye on how other people are using ChatGPT, and \u003ca href=\"https://en.wikipedia.org/wiki/Prompt_engineering\"\u003eprompt engineering\u003c/a\u003e is one of the more fascinating aspects around how people are communicating with the AI. Prompt engineering plays a key role in communicating with models like ChatGPT. Carefully crafting prompts can help maximize the quality of the reponse. However, it is important to keep in mind that, like any AI model, ChatGPT has its own set of limitations.\u003c/p\u003e\n\u003cp\u003eSo I was curious when I read that “\u003ca href=\"https://www.reddit.com/r/ChatGPT/comments/101e454/chatgpt_cant_write_sentences_that_end_with_a/\"\u003eChatGPT can’t write sentences that end in a specific letter\u003c/a\u003e”. Is this a real limitation of ChatGPT, or maybe this was just a poor prompt.\u003c/p\u003e\n\u003cp\u003eSo let’s start with a very simple question to ChatGPT:\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eMe: Can you write a sentence that ends with the letter \u0026quot;a\u0026quot;?\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eChatGPT: The cat sat on the mat.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eMe: That sentence does not end in the letter \u0026quot;a\u0026quot;. Can you write a sentence that ends in the letter \u0026quot;a\u0026quot;?\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eChatGPT: I can\u0026#x27;t think of anything right now, but the word \u0026quot;America\u0026quot; ends with letter \u0026quot;a\u0026quot;.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eMe: Great, write a sentence that ends in that word. That will then satisfy the original question.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eChatGPT: The land of the free, and the home of the brave, America.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eOkay, that does technically end in the letter a, but the sentence seems very strange.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eMe: That isn\u0026#x27;t a very coherent sentence. Can you write a sentence that ends in the letter \u0026quot;a\u0026quot;?\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eChatGPT: I am sorry for the confusion, here is a sentence that ends with the letter \u0026quot;a\u0026quot; - \u0026quot;He saw the drama unfold on the stage, it was quite the spectacle, a.”\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eLooks like we are going off the rails here. Let’s try a different approach:\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eMe: Write a sentence that ends in a word that ends with the letter \u0026quot;a\u0026quot;.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eChatGPT: The sun was setting over the horizon, painting the sky with shades of orange and red, a spectacle to behold and end the day with.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eI’m going to ask ChatGPT to tell me a prompt to feed back to itself. I’ve found this useful for resetting context when having long conversations. Or if ChatGPT gets into weird states like the above response.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eMe: Can you tell me how to write a prompt for you that would actually get you to write a sentence that ends in the letter \u0026quot;a\u0026quot;.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eChatGPT: Sure, to get me to write a sentence that ends in the letter \u0026quot;a\u0026quot; you could provide a prompt like: \u0026quot;Write a sentence that ends with a word that ends with the letter \u0026#x27;a\u0026#x27;\u0026quot; or \u0026quot;Can you write a sentence with a word ending in \u0026#x27;a\u0026#x27; at the end of the sentence?\u0026quot; . This makes it clear that the sentence should end in a word ending with the letter \u0026#x27;a\u0026#x27;.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eMe: Can you write a sentence with a word ending in \u0026#x27;a\u0026#x27; at the end of the sentence?\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eChatGPT: The sky was clear and blue, not a cloud in sight, perfect for a day out at the parka.\u003c/em\u003e\u003c/p\u003e\n\u003chr/\u003e\n\u003cp\u003eWhile ChatGPT is great at forming sentences and even code, it seems like it has no understanding and hasn’t been trained on letters, syllables, or the concept of sentences. I\u0026#x27;ve seen others get ChatGPT to create poems, but it has no concept of why words would rhyme. It can give great answers to questions, but doesn’t understand how to count how many words are in its response.\u003c/p\u003e\n\u003cp\u003eOne Redditor in the link above mentions that ChatGPT fails at ASCII art. I’ve tried getting the AI to print ASCII art before, and I’ll leave you with part of a conversation I’ve had with it. This appears to be the best ASCII art I could get it to print, even after having a long conversation where I provided different ASCII art examples:\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eMe: Can you make ASCII art? Show me a picture of a cat.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eChatGPT: Sure! Here is a picture of a cat using ASCII art:\u003c/em\u003e\u003c/p\u003e\n\u003cdiv class=\"overflow-auto rounded bg-gray-200 p-4 font-mono text-sm dark:bg-gray-800 dark:text-gray-100\"\u003e\u003cpre\u003e\u003ccode\u003e /\\_/\\\n( o.o )\n \u0026gt;^\u0026lt;\n\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003cem\u003eI hope you like it! Is there anything else I can help with?\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eMe: Can you make it more detailed\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eChatGPT: Certainly! Here is a more detailed version of an ASCII art cat:\u003c/em\u003e\u003c/p\u003e\n\u003cdiv class=\"overflow-auto rounded bg-gray-200 p-4 font-mono text-sm dark:bg-gray-800 dark:text-gray-100\"\u003e\u003cpre\u003e\u003ccode\u003e /\\_/\\\n( o.o )\n  \u0026gt;^\u0026lt;\n  | |\n  || ||\n  \\\\ //\n  // \\\\\n  \\\\_//\n  /   \\\n /     \\\n\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eWith new language models coming out, it\u0026#x27;s important to recognize what the limitations\nof the AI are. Poor responses can be a result of poorly written prompts, but there\nare times, like with letters in words with ChatGPT, where the AI just doesn\u0026#x27;t have\nthe capability to understand the question.\u003c/p\u003e","contentCode":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    a: \"a\",\n    em: \"em\",\n    hr: \"hr\",\n    pre: \"pre\",\n    code: \"code\"\n  }, props.components);\n  return _jsxs(_Fragment, {\n    children: [_jsxs(_components.p, {\n      children: [\"Looks like everyone is trying to discover ChatGPT’s limitations. In one article I read, the author asked whether \", _jsx(_components.a, {\n        href: \"https://medium.com/@theworldaccordingtocgpt/chatgpt-can-play-20-questions-7911405f7aff\",\n        children: \"ChatGPT can play 20 questions\"\n      }), \". The human in this scenario tried to play 20 questions with ChatGPT, and found that they needed to guide the AI to ask questions. From my perspective, it looked more like a failure to engineer the initial prompt correctly.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"I attempted to do 20 questions with ChatGPT using a different prompt:\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"Me: I would like to play a game. It's called twenty questions. You will ask me one question at a time and I will answer it. The goal is for you guess the thing I am thinking about.\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"ChatGPT: Sure, I'd be happy to play \\\"Twenty Questions\\\" with you. Please go ahead and think of something, and I will start asking my first question.\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"Me: I am ready\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"ChatGPT: Great, here is my first question: Is the thing you are thinking of an object?\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"Me: Yes\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"ChatGPT: Is the object something that you can hold in your hand?\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"Me: Yes\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"ChatGPT: Is the object something that you would typically find in a kitchen?\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"Me: No\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"ChatGPT: Is the object something that you would use for transportation?\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"Me: No\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"ChatGPT: Is the object something that you would use for entertainment or leisure?\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"Me: Yes\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"ChatGPT: Is the object a musical instrument?\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"Me: No\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"ChatGPT: Is the object a type of game or puzzle?\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"Me: No\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"ChatGPT: Is the object a piece of technology such as a phone or computer?\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"Me: No\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"ChatGPT: Is the object a book or magazine?\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"Me: Yes\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"ChatGPT: Is the object a novel or a collection of short stories?\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"Me: Yes\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"ChatGPT: Is the object a specific book that is widely known and read?\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"Me: Yes\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"ChatGPT: Is the object \\\"To Kill a Mockingbird\\\" by Harper Lee?\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"Me: No\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"ChatGPT: I'm sorry, I'm running out of ideas. Can you give me a hint or tell me the answer?\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Seems like we hit a wall here with twenty questions, but ChatGPT is very close, so I decided to give a hint.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"Me: You are on the right track. It is by Walt Whitman\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"ChatGPT: Is the object \\\"Leaves of Grass\\\" by Walt Whitman?\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"Me: Yes! Congratulations!\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"ChatGPT: Thank you! I'm glad I was able to guess the object you were thinking of. It was fun playing \\\"Twenty Questions\\\" with you. Let me know if you want to play again sometime.\"\n      })\n    }), \"\\n\", _jsx(_components.hr, {}), \"\\n\", _jsxs(_components.p, {\n      children: [\"I’ve been keeping an eye on how other people are using ChatGPT, and \", _jsx(_components.a, {\n        href: \"https://en.wikipedia.org/wiki/Prompt_engineering\",\n        children: \"prompt engineering\"\n      }), \" is one of the more fascinating aspects around how people are communicating with the AI. Prompt engineering plays a key role in communicating with models like ChatGPT. Carefully crafting prompts can help maximize the quality of the reponse. However, it is important to keep in mind that, like any AI model, ChatGPT has its own set of limitations.\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"So I was curious when I read that “\", _jsx(_components.a, {\n        href: \"https://www.reddit.com/r/ChatGPT/comments/101e454/chatgpt_cant_write_sentences_that_end_with_a/\",\n        children: \"ChatGPT can’t write sentences that end in a specific letter\"\n      }), \"”. Is this a real limitation of ChatGPT, or maybe this was just a poor prompt.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"So let’s start with a very simple question to ChatGPT:\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"Me: Can you write a sentence that ends with the letter \\\"a\\\"?\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"ChatGPT: The cat sat on the mat.\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"Me: That sentence does not end in the letter \\\"a\\\". Can you write a sentence that ends in the letter \\\"a\\\"?\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"ChatGPT: I can't think of anything right now, but the word \\\"America\\\" ends with letter \\\"a\\\".\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"Me: Great, write a sentence that ends in that word. That will then satisfy the original question.\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"ChatGPT: The land of the free, and the home of the brave, America.\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Okay, that does technically end in the letter a, but the sentence seems very strange.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"Me: That isn't a very coherent sentence. Can you write a sentence that ends in the letter \\\"a\\\"?\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"ChatGPT: I am sorry for the confusion, here is a sentence that ends with the letter \\\"a\\\" - \\\"He saw the drama unfold on the stage, it was quite the spectacle, a.”\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Looks like we are going off the rails here. Let’s try a different approach:\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"Me: Write a sentence that ends in a word that ends with the letter \\\"a\\\".\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"ChatGPT: The sun was setting over the horizon, painting the sky with shades of orange and red, a spectacle to behold and end the day with.\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"I’m going to ask ChatGPT to tell me a prompt to feed back to itself. I’ve found this useful for resetting context when having long conversations. Or if ChatGPT gets into weird states like the above response.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"Me: Can you tell me how to write a prompt for you that would actually get you to write a sentence that ends in the letter \\\"a\\\".\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"ChatGPT: Sure, to get me to write a sentence that ends in the letter \\\"a\\\" you could provide a prompt like: \\\"Write a sentence that ends with a word that ends with the letter 'a'\\\" or \\\"Can you write a sentence with a word ending in 'a' at the end of the sentence?\\\" . This makes it clear that the sentence should end in a word ending with the letter 'a'.\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"Me: Can you write a sentence with a word ending in 'a' at the end of the sentence?\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"ChatGPT: The sky was clear and blue, not a cloud in sight, perfect for a day out at the parka.\"\n      })\n    }), \"\\n\", _jsx(_components.hr, {}), \"\\n\", _jsx(_components.p, {\n      children: \"While ChatGPT is great at forming sentences and even code, it seems like it has no understanding and hasn’t been trained on letters, syllables, or the concept of sentences. I've seen others get ChatGPT to create poems, but it has no concept of why words would rhyme. It can give great answers to questions, but doesn’t understand how to count how many words are in its response.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"One Redditor in the link above mentions that ChatGPT fails at ASCII art. I’ve tried getting the AI to print ASCII art before, and I’ll leave you with part of a conversation I’ve had with it. This appears to be the best ASCII art I could get it to print, even after having a long conversation where I provided different ASCII art examples:\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"Me: Can you make ASCII art? Show me a picture of a cat.\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"ChatGPT: Sure! Here is a picture of a cat using ASCII art:\"\n      })\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        children: \" /\\\\_/\\\\\\n( o.o )\\n \u003e^\u003c\\n\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"I hope you like it! Is there anything else I can help with?\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"Me: Can you make it more detailed\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"ChatGPT: Certainly! Here is a more detailed version of an ASCII art cat:\"\n      })\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        children: \" /\\\\_/\\\\\\n( o.o )\\n  \u003e^\u003c\\n  | |\\n  || ||\\n  \\\\\\\\ //\\n  // \\\\\\\\\\n  \\\\\\\\_//\\n  /   \\\\\\n /     \\\\\\n\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"With new language models coming out, it's important to recognize what the limitations\\nof the AI are. Poor responses can be a result of poorly written prompts, but there\\nare times, like with letters in words with ChatGPT, where the AI just doesn't have\\nthe capability to understand the question.\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","excerptRaw":"\nLooks like everyone is trying to discover ChatGPT’s limitations. In one article I read, the author asked whether [ChatGPT can play 20 questions](https://medium.com/@theworldaccordingtocgpt/chatgpt-can-play-20-questions-7911405f7aff). The human in this scenario tried to play 20 questions with ChatGPT, and found that they needed to guide the AI to ask questions. From my perspective, it looked more like a failure to engineer the initial prompt correctly.","excerptHTML":"\u003cp\u003eLooks like everyone is trying to discover ChatGPT’s limitations. In one article I read, the author asked whether \u003ca href=\"https://medium.com/@theworldaccordingtocgpt/chatgpt-can-play-20-questions-7911405f7aff\"\u003eChatGPT can play 20 questions\u003c/a\u003e. The human in this scenario tried to play 20 questions with ChatGPT, and found that they needed to guide the AI to ask questions. From my perspective, it looked more like a failure to engineer the initial prompt correctly.\u003c/p\u003e","excerptCode":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {jsx: _jsx, jsxs: _jsxs} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    a: \"a\"\n  }, props.components);\n  return _jsxs(_components.p, {\n    children: [\"Looks like everyone is trying to discover ChatGPT’s limitations. In one article I read, the author asked whether \", _jsx(_components.a, {\n      href: \"https://medium.com/@theworldaccordingtocgpt/chatgpt-can-play-20-questions-7911405f7aff\",\n      children: \"ChatGPT can play 20 questions\"\n    }), \". The human in this scenario tried to play 20 questions with ChatGPT, and found that they needed to guide the AI to ask questions. From my perspective, it looked more like a failure to engineer the initial prompt correctly.\"]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n"},{"slug":"2023-01-15-hierarchy-of-webapp-needs","date":"2023-01-15","title":"The Hierarchy of Webapp Needs","frontmatter":{"title":"The Hierarchy of Webapp Needs"},"contentRaw":"\nI was thinking about all the little projects I work on and how they grow over time. The applications end up hitting some milestones and end up needing similar functionality that compliments the core features. A small project ends up getting complex enough that it requires some unit tests. I’ll go to deploy the project and now I need some deployment scripts and analytics to ensure the application is running correctly. The technology may change between each project, but web applications always seem to have the same steps that need to be taken to strengthen the application as it is scaled up.\n\nAt a large company, adding a new set of functionality always has a suite of concerns to think through before implementation: how will we deploy this feature, how do we validate that users are using the feature like we expected them to, how do we monitor for bugs and errors? This is on top of the basic functionality of actually writing and testing that new feature.\n\nWhat if we thought about this like Maslow’s Hierarchy of Needs, but in the context of a web application.\n\nMaslow's Hierarchy of Needs is a psychological model of human motivation proposed by Abraham Maslow in 1943. The model describes a hierarchy of human needs, beginning with basic physiological needs such as food and shelter and progressing upwards to higher-level needs such as self-actualization. Maslow argued that as humans satisfy their basic needs, they can move on to satisfy their higher-level needs.\n\nApplying Maslow's Hierarchy of Needs to web applications, we can identify the different levels of needs that need to be met in order to make a web application successful.\n\nBasic functionality:\n\n- A single build/run script\n- Basic functionality (e.g. CRUD operations for a web app)\n- User interface (e.g. layout, navigation, responsive design)\n- Integration with external services (e.g. databases, APIs)\n\nSafety and security:\n\n- Linting (e.g. ESLint, Prettier)\n- Unit tests (e.g. Jest, Mocha)\n- Basic error handling and reporting (e.g. logging, alerting)\n- Input validation (e.g. form validation)\n- Security best practices (e.g. encryption, password hashing, session management)\n\nLove and belonging:\n\n- Basic analytics (e.g. page views, user engagement)\n- User authentication and authorization (e.g. sign-up, login, role-based access control)\n- User feedback (e.g. contact form, survey)\n- Social media integration (e.g. sharing, commenting)\n\nEsteem:\n\n- Advanced analytics (e.g. user behavior tracking, A/B testing)\n- Performance monitoring (e.g. load testing, monitoring of server resources)\n- User experience optimization (e.g. user testing, usability analysis)\n\nSelf-actualization:\n\n- End-to-end testing (e.g. Selenium, Cypress)\n- Accessibility and internationalization (e.g. support for screen readers, translation)\n- Scalability (e.g. load balancing, caching)\n- Continuous integration and delivery (e.g. Jenkins, Travis CI)\n- Deployment (e.g. Docker, Kubernetes)\n- Automated testing (e.g. unit test, integration test)\n\nOf course this list isn’t exhaustive, but I’ve been thinking about it as more of a checklist to build upon when working on small side-projects that end up getting significant attention and development time.\n","contentHTML":"\u003cp\u003eI was thinking about all the little projects I work on and how they grow over time. The applications end up hitting some milestones and end up needing similar functionality that compliments the core features. A small project ends up getting complex enough that it requires some unit tests. I’ll go to deploy the project and now I need some deployment scripts and analytics to ensure the application is running correctly. The technology may change between each project, but web applications always seem to have the same steps that need to be taken to strengthen the application as it is scaled up.\u003c/p\u003e\n\u003cp\u003eAt a large company, adding a new set of functionality always has a suite of concerns to think through before implementation: how will we deploy this feature, how do we validate that users are using the feature like we expected them to, how do we monitor for bugs and errors? This is on top of the basic functionality of actually writing and testing that new feature.\u003c/p\u003e\n\u003cp\u003eWhat if we thought about this like Maslow’s Hierarchy of Needs, but in the context of a web application.\u003c/p\u003e\n\u003cp\u003eMaslow\u0026#x27;s Hierarchy of Needs is a psychological model of human motivation proposed by Abraham Maslow in 1943. The model describes a hierarchy of human needs, beginning with basic physiological needs such as food and shelter and progressing upwards to higher-level needs such as self-actualization. Maslow argued that as humans satisfy their basic needs, they can move on to satisfy their higher-level needs.\u003c/p\u003e\n\u003cp\u003eApplying Maslow\u0026#x27;s Hierarchy of Needs to web applications, we can identify the different levels of needs that need to be met in order to make a web application successful.\u003c/p\u003e\n\u003cp\u003eBasic functionality:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA single build/run script\u003c/li\u003e\n\u003cli\u003eBasic functionality (e.g. CRUD operations for a web app)\u003c/li\u003e\n\u003cli\u003eUser interface (e.g. layout, navigation, responsive design)\u003c/li\u003e\n\u003cli\u003eIntegration with external services (e.g. databases, APIs)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSafety and security:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eLinting (e.g. ESLint, Prettier)\u003c/li\u003e\n\u003cli\u003eUnit tests (e.g. Jest, Mocha)\u003c/li\u003e\n\u003cli\u003eBasic error handling and reporting (e.g. logging, alerting)\u003c/li\u003e\n\u003cli\u003eInput validation (e.g. form validation)\u003c/li\u003e\n\u003cli\u003eSecurity best practices (e.g. encryption, password hashing, session management)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLove and belonging:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eBasic analytics (e.g. page views, user engagement)\u003c/li\u003e\n\u003cli\u003eUser authentication and authorization (e.g. sign-up, login, role-based access control)\u003c/li\u003e\n\u003cli\u003eUser feedback (e.g. contact form, survey)\u003c/li\u003e\n\u003cli\u003eSocial media integration (e.g. sharing, commenting)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEsteem:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced analytics (e.g. user behavior tracking, A/B testing)\u003c/li\u003e\n\u003cli\u003ePerformance monitoring (e.g. load testing, monitoring of server resources)\u003c/li\u003e\n\u003cli\u003eUser experience optimization (e.g. user testing, usability analysis)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSelf-actualization:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eEnd-to-end testing (e.g. Selenium, Cypress)\u003c/li\u003e\n\u003cli\u003eAccessibility and internationalization (e.g. support for screen readers, translation)\u003c/li\u003e\n\u003cli\u003eScalability (e.g. load balancing, caching)\u003c/li\u003e\n\u003cli\u003eContinuous integration and delivery (e.g. Jenkins, Travis CI)\u003c/li\u003e\n\u003cli\u003eDeployment (e.g. Docker, Kubernetes)\u003c/li\u003e\n\u003cli\u003eAutomated testing (e.g. unit test, integration test)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eOf course this list isn’t exhaustive, but I’ve been thinking about it as more of a checklist to build upon when working on small side-projects that end up getting significant attention and development time.\u003c/p\u003e","contentCode":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    ul: \"ul\",\n    li: \"li\"\n  }, props.components);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"I was thinking about all the little projects I work on and how they grow over time. The applications end up hitting some milestones and end up needing similar functionality that compliments the core features. A small project ends up getting complex enough that it requires some unit tests. I’ll go to deploy the project and now I need some deployment scripts and analytics to ensure the application is running correctly. The technology may change between each project, but web applications always seem to have the same steps that need to be taken to strengthen the application as it is scaled up.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"At a large company, adding a new set of functionality always has a suite of concerns to think through before implementation: how will we deploy this feature, how do we validate that users are using the feature like we expected them to, how do we monitor for bugs and errors? This is on top of the basic functionality of actually writing and testing that new feature.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"What if we thought about this like Maslow’s Hierarchy of Needs, but in the context of a web application.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Maslow's Hierarchy of Needs is a psychological model of human motivation proposed by Abraham Maslow in 1943. The model describes a hierarchy of human needs, beginning with basic physiological needs such as food and shelter and progressing upwards to higher-level needs such as self-actualization. Maslow argued that as humans satisfy their basic needs, they can move on to satisfy their higher-level needs.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Applying Maslow's Hierarchy of Needs to web applications, we can identify the different levels of needs that need to be met in order to make a web application successful.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Basic functionality:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"A single build/run script\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Basic functionality (e.g. CRUD operations for a web app)\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"User interface (e.g. layout, navigation, responsive design)\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Integration with external services (e.g. databases, APIs)\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Safety and security:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"Linting (e.g. ESLint, Prettier)\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Unit tests (e.g. Jest, Mocha)\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Basic error handling and reporting (e.g. logging, alerting)\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Input validation (e.g. form validation)\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Security best practices (e.g. encryption, password hashing, session management)\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Love and belonging:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"Basic analytics (e.g. page views, user engagement)\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"User authentication and authorization (e.g. sign-up, login, role-based access control)\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"User feedback (e.g. contact form, survey)\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Social media integration (e.g. sharing, commenting)\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Esteem:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"Advanced analytics (e.g. user behavior tracking, A/B testing)\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Performance monitoring (e.g. load testing, monitoring of server resources)\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"User experience optimization (e.g. user testing, usability analysis)\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Self-actualization:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"End-to-end testing (e.g. Selenium, Cypress)\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Accessibility and internationalization (e.g. support for screen readers, translation)\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Scalability (e.g. load balancing, caching)\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Continuous integration and delivery (e.g. Jenkins, Travis CI)\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Deployment (e.g. Docker, Kubernetes)\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Automated testing (e.g. unit test, integration test)\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Of course this list isn’t exhaustive, but I’ve been thinking about it as more of a checklist to build upon when working on small side-projects that end up getting significant attention and development time.\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","excerptRaw":"\nI was thinking about all the little projects I work on and how they grow over time. The applications end up hitting some milestones and end up needing similar functionality that compliments the core features. A small project ends up getting complex enough that it requires some unit tests. I’ll go to deploy the project and now I need some deployment scripts and analytics to ensure the application is running correctly. The technology may change between each project, but web applications always seem to have the same steps that need to be taken to strengthen the application as it is scaled up.","excerptHTML":"\u003cp\u003eI was thinking about all the little projects I work on and how they grow over time. The applications end up hitting some milestones and end up needing similar functionality that compliments the core features. A small project ends up getting complex enough that it requires some unit tests. I’ll go to deploy the project and now I need some deployment scripts and analytics to ensure the application is running correctly. The technology may change between each project, but web applications always seem to have the same steps that need to be taken to strengthen the application as it is scaled up.\u003c/p\u003e","excerptCode":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {jsx: _jsx} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\"\n  }, props.components);\n  return _jsx(_components.p, {\n    children: \"I was thinking about all the little projects I work on and how they grow over time. The applications end up hitting some milestones and end up needing similar functionality that compliments the core features. A small project ends up getting complex enough that it requires some unit tests. I’ll go to deploy the project and now I need some deployment scripts and analytics to ensure the application is running correctly. The technology may change between each project, but web applications always seem to have the same steps that need to be taken to strengthen the application as it is scaled up.\"\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n"},{"slug":"2023-01-07-clarity-hub-infer","date":"2023-01-07","title":"Clarity Hub Infer API","frontmatter":{"title":"Clarity Hub Infer API"},"contentRaw":"\n![Screen Shot 2023-01-07 at 3.57.30 PM.png](/media/2023-01-07-clarity-hub-infer/Screen_Shot_2023-01-07_at_3.57.30_PM.png)\n\nWhile working on Clarity Hub, we created a Clarity Hub Infer API along with a developer portal that would let anyone create infer models.\n\nThe Clarity Hub Infer API provides a fast and intuitive way to create, manage, and deploy NLP models based on labelling utterances.\n\nAt the most basic level, the Infer API would let users send utterances via an API and get toxicity analysis, sentiment scores, and simple NLP data like nouns and topics from the utterances.\n\nThe power of the Infer API is that consumers can supply a set of pre-labelled utterances to the API, and the API will create a model from this, even if there are only a few utterances used for training. Then the consumer can send a new utterance get a label using that model.\n\nThe NLP APIs at Clarity Hub were a set of APIs:\n\n```mermaid\ngraph RL\n  NLP(Clarity Hub NLP API) --\u003e API(Clarity Hub Infer API) --\u003e Consumer\n```\n\nThe Consumer would user the Infer API which provided APIs for training and labeling datasets and getting toxicity and sentiment analyses. the Clarity Hub NLP API contained trained Tensorflow datasets for creating embeddings via the Universal Sentence Encoder (USE).\n\nAn **embedding** a vector that represents an utterance - a sentence, sentence fragment, or paragraph of text.\n\nTraining would involve a consumer sending a payload of utterances with labels to the Infer API, which would call the NLP API internally to create embeddings. We then clustered these embeddings to and re-labelled the clusters using the given labels. If no label was found for an utterance cluster, we attempted to pull a topic out of the utterances to re-label it.\n\nThe clusters with labels were then stored into S3.\n\n```mermaid\ngraph TD\n  Train --\u003e|Utterances with labels| USE\n  USE --\u003e|Embeddings with labels| Clustering\n  Clustering --\u003e|Embedding Clusters| Labeller\n  Labeller --\u003e|Clusters with Labels| S3\n```\n\nTo classify a new utterance, we created an embedding from it, loaded the existing dataset in, then ran cosine similarity to find the most probabilistic matches:\n\n```mermaid\ngraph TD\n  Classify --\u003e |Utterance| USE\n  USE --\u003e |Embedding| Classifier(Classifier)\n  Classifier --\u003e |Embedding + Clusters from S3| Similarity(Cosine Similarity)\n  Similarity --\u003e |Labels with Probability| Response\n```\n\n### What it looked like\n\n![Screen Shot 2023-01-07 at 3.57.56 PM.png](/media/2023-01-07-clarity-hub-infer/Screen_Shot_2023-01-07_at_3.57.56_PM.png)\n\n![Screen Shot 2023-01-07 at 3.58.08 PM.png](/media/2023-01-07-clarity-hub-infer/Screen_Shot_2023-01-07_at_3.58.08_PM.png)\n\n![Screen Shot 2023-01-07 at 3.58.28 PM.png](/media/2023-01-07-clarity-hub-infer/Screen_Shot_2023-01-07_at_3.58.28_PM.png)\n\n### Conclusion\n\nWith ChatGPT and other NLP models coming out lately, this seems fairly basic, but the following processes are still very useful to understand:\n\n- Convert language to a representation that is easier to work with, like a vector.\n- Clustering vectors is a great way to find representative vectors, reducing the size of the number of vectors you need to work with.\n- Cosine Similarity can be used to find how similar vectors are. If a vector is labelled with metadata, it also tells you how similar the metadata between the vectors are as well.\n\nYou can see [my project page](/projects/2020-05-18-clarity-hub-infer) for more details and links to the Github repos.\n","contentHTML":"\u003cp\u003e\u003cimg alt=\"Screen Shot 2023-01-07 at 3.57.30 PM.png\" src=\"/media/2023-01-07-clarity-hub-infer/Screen_Shot_2023-01-07_at_3.57.30_PM.png\" style=\"max-height:500px;margin:auto;text-align:center\"/\u003e\u003c/p\u003e\n\u003cp\u003eWhile working on Clarity Hub, we created a Clarity Hub Infer API along with a developer portal that would let anyone create infer models.\u003c/p\u003e\n\u003cp\u003eThe Clarity Hub Infer API provides a fast and intuitive way to create, manage, and deploy NLP models based on labelling utterances.\u003c/p\u003e\n\u003cp\u003eAt the most basic level, the Infer API would let users send utterances via an API and get toxicity analysis, sentiment scores, and simple NLP data like nouns and topics from the utterances.\u003c/p\u003e\n\u003cp\u003eThe power of the Infer API is that consumers can supply a set of pre-labelled utterances to the API, and the API will create a model from this, even if there are only a few utterances used for training. Then the consumer can send a new utterance get a label using that model.\u003c/p\u003e\n\u003cp\u003eThe NLP APIs at Clarity Hub were a set of APIs:\u003c/p\u003e\n\u003cdiv class=\"py-8 [\u0026amp;_svg]:m-auto\"\u003e\u003cdiv class=\"mermaid\" data-mermaid-src=\"graph RL\n  NLP(Clarity Hub NLP API) --\u0026gt; API(Clarity Hub Infer API) --\u0026gt; Consumer\"\u003egraph RL\n  NLP(Clarity Hub NLP API) --\u0026gt; API(Clarity Hub Infer API) --\u0026gt; Consumer\u003c/div\u003e\u003c/div\u003e\n\u003cp\u003eThe Consumer would user the Infer API which provided APIs for training and labeling datasets and getting toxicity and sentiment analyses. the Clarity Hub NLP API contained trained Tensorflow datasets for creating embeddings via the Universal Sentence Encoder (USE).\u003c/p\u003e\n\u003cp\u003eAn \u003cstrong\u003eembedding\u003c/strong\u003e a vector that represents an utterance - a sentence, sentence fragment, or paragraph of text.\u003c/p\u003e\n\u003cp\u003eTraining would involve a consumer sending a payload of utterances with labels to the Infer API, which would call the NLP API internally to create embeddings. We then clustered these embeddings to and re-labelled the clusters using the given labels. If no label was found for an utterance cluster, we attempted to pull a topic out of the utterances to re-label it.\u003c/p\u003e\n\u003cp\u003eThe clusters with labels were then stored into S3.\u003c/p\u003e\n\u003cdiv class=\"py-8 [\u0026amp;_svg]:m-auto\"\u003e\u003cdiv class=\"mermaid\" data-mermaid-src=\"graph TD\n  Train --\u0026gt;|Utterances with labels| USE\n  USE --\u0026gt;|Embeddings with labels| Clustering\n  Clustering --\u0026gt;|Embedding Clusters| Labeller\n  Labeller --\u0026gt;|Clusters with Labels| S3\"\u003egraph TD\n  Train --\u0026gt;|Utterances with labels| USE\n  USE --\u0026gt;|Embeddings with labels| Clustering\n  Clustering --\u0026gt;|Embedding Clusters| Labeller\n  Labeller --\u0026gt;|Clusters with Labels| S3\u003c/div\u003e\u003c/div\u003e\n\u003cp\u003eTo classify a new utterance, we created an embedding from it, loaded the existing dataset in, then ran cosine similarity to find the most probabilistic matches:\u003c/p\u003e\n\u003cdiv class=\"py-8 [\u0026amp;_svg]:m-auto\"\u003e\u003cdiv class=\"mermaid\" data-mermaid-src=\"graph TD\n  Classify --\u0026gt; |Utterance| USE\n  USE --\u0026gt; |Embedding| Classifier(Classifier)\n  Classifier --\u0026gt; |Embedding + Clusters from S3| Similarity(Cosine Similarity)\n  Similarity --\u0026gt; |Labels with Probability| Response\"\u003egraph TD\n  Classify --\u0026gt; |Utterance| USE\n  USE --\u0026gt; |Embedding| Classifier(Classifier)\n  Classifier --\u0026gt; |Embedding + Clusters from S3| Similarity(Cosine Similarity)\n  Similarity --\u0026gt; |Labels with Probability| Response\u003c/div\u003e\u003c/div\u003e\n\u003ch3\u003eWhat it looked like\u003c/h3\u003e\n\u003cp\u003e\u003cimg alt=\"Screen Shot 2023-01-07 at 3.57.56 PM.png\" src=\"/media/2023-01-07-clarity-hub-infer/Screen_Shot_2023-01-07_at_3.57.56_PM.png\" style=\"max-height:500px;margin:auto;text-align:center\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Screen Shot 2023-01-07 at 3.58.08 PM.png\" src=\"/media/2023-01-07-clarity-hub-infer/Screen_Shot_2023-01-07_at_3.58.08_PM.png\" style=\"max-height:500px;margin:auto;text-align:center\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Screen Shot 2023-01-07 at 3.58.28 PM.png\" src=\"/media/2023-01-07-clarity-hub-infer/Screen_Shot_2023-01-07_at_3.58.28_PM.png\" style=\"max-height:500px;margin:auto;text-align:center\"/\u003e\u003c/p\u003e\n\u003ch3\u003eConclusion\u003c/h3\u003e\n\u003cp\u003eWith ChatGPT and other NLP models coming out lately, this seems fairly basic, but the following processes are still very useful to understand:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConvert language to a representation that is easier to work with, like a vector.\u003c/li\u003e\n\u003cli\u003eClustering vectors is a great way to find representative vectors, reducing the size of the number of vectors you need to work with.\u003c/li\u003e\n\u003cli\u003eCosine Similarity can be used to find how similar vectors are. If a vector is labelled with metadata, it also tells you how similar the metadata between the vectors are as well.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eYou can see \u003ca href=\"/projects/2020-05-18-clarity-hub-infer\"\u003emy project page\u003c/a\u003e for more details and links to the Github repos.\u003c/p\u003e","contentCode":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    img: \"img\",\n    strong: \"strong\",\n    h3: \"h3\",\n    ul: \"ul\",\n    li: \"li\",\n    a: \"a\"\n  }, props.components), {Mermaid} = _components;\n  if (!Mermaid) _missingMdxReference(\"Mermaid\", true);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/media/2023-01-07-clarity-hub-infer/Screen_Shot_2023-01-07_at_3.57.30_PM.png\",\n        alt: \"Screen Shot 2023-01-07 at 3.57.30 PM.png\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"While working on Clarity Hub, we created a Clarity Hub Infer API along with a developer portal that would let anyone create infer models.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The Clarity Hub Infer API provides a fast and intuitive way to create, manage, and deploy NLP models based on labelling utterances.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"At the most basic level, the Infer API would let users send utterances via an API and get toxicity analysis, sentiment scores, and simple NLP data like nouns and topics from the utterances.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The power of the Infer API is that consumers can supply a set of pre-labelled utterances to the API, and the API will create a model from this, even if there are only a few utterances used for training. Then the consumer can send a new utterance get a label using that model.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The NLP APIs at Clarity Hub were a set of APIs:\"\n    }), \"\\n\", _jsx(Mermaid, {\n      chart: \"graph RL\\n  NLP(Clarity Hub NLP API) --\u003e API(Clarity Hub Infer API) --\u003e Consumer\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The Consumer would user the Infer API which provided APIs for training and labeling datasets and getting toxicity and sentiment analyses. the Clarity Hub NLP API contained trained Tensorflow datasets for creating embeddings via the Universal Sentence Encoder (USE).\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"An \", _jsx(_components.strong, {\n        children: \"embedding\"\n      }), \" a vector that represents an utterance - a sentence, sentence fragment, or paragraph of text.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Training would involve a consumer sending a payload of utterances with labels to the Infer API, which would call the NLP API internally to create embeddings. We then clustered these embeddings to and re-labelled the clusters using the given labels. If no label was found for an utterance cluster, we attempted to pull a topic out of the utterances to re-label it.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The clusters with labels were then stored into S3.\"\n    }), \"\\n\", _jsx(Mermaid, {\n      chart: \"graph TD\\n  Train --\u003e|Utterances with labels| USE\\n  USE --\u003e|Embeddings with labels| Clustering\\n  Clustering --\u003e|Embedding Clusters| Labeller\\n  Labeller --\u003e|Clusters with Labels| S3\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"To classify a new utterance, we created an embedding from it, loaded the existing dataset in, then ran cosine similarity to find the most probabilistic matches:\"\n    }), \"\\n\", _jsx(Mermaid, {\n      chart: \"graph TD\\n  Classify --\u003e |Utterance| USE\\n  USE --\u003e |Embedding| Classifier(Classifier)\\n  Classifier --\u003e |Embedding + Clusters from S3| Similarity(Cosine Similarity)\\n  Similarity --\u003e |Labels with Probability| Response\"\n    }), \"\\n\", _jsx(_components.h3, {\n      children: \"What it looked like\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/media/2023-01-07-clarity-hub-infer/Screen_Shot_2023-01-07_at_3.57.56_PM.png\",\n        alt: \"Screen Shot 2023-01-07 at 3.57.56 PM.png\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/media/2023-01-07-clarity-hub-infer/Screen_Shot_2023-01-07_at_3.58.08_PM.png\",\n        alt: \"Screen Shot 2023-01-07 at 3.58.08 PM.png\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/media/2023-01-07-clarity-hub-infer/Screen_Shot_2023-01-07_at_3.58.28_PM.png\",\n        alt: \"Screen Shot 2023-01-07 at 3.58.28 PM.png\"\n      })\n    }), \"\\n\", _jsx(_components.h3, {\n      children: \"Conclusion\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"With ChatGPT and other NLP models coming out lately, this seems fairly basic, but the following processes are still very useful to understand:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"Convert language to a representation that is easier to work with, like a vector.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Clustering vectors is a great way to find representative vectors, reducing the size of the number of vectors you need to work with.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Cosine Similarity can be used to find how similar vectors are. If a vector is labelled with metadata, it also tells you how similar the metadata between the vectors are as well.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"You can see \", _jsx(_components.a, {\n        href: \"/projects/2020-05-18-clarity-hub-infer\",\n        children: \"my project page\"\n      }), \" for more details and links to the Github repos.\"]\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\nfunction _missingMdxReference(id, component) {\n  throw new Error(\"Expected \" + (component ? \"component\" : \"object\") + \" `\" + id + \"` to be defined: you likely forgot to import, pass, or provide it.\");\n}\n","excerptRaw":"\n![Screen Shot 2023-01-07 at 3.57.30 PM.png](/media/2023-01-07-clarity-hub-infer/Screen_Shot_2023-01-07_at_3.57.30_PM.png)","excerptHTML":"\u003cp\u003e\u003cimg alt=\"Screen Shot 2023-01-07 at 3.57.30 PM.png\" src=\"/media/2023-01-07-clarity-hub-infer/Screen_Shot_2023-01-07_at_3.57.30_PM.png\" style=\"max-height:500px;margin:auto;text-align:center\"/\u003e\u003c/p\u003e","excerptCode":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {jsx: _jsx} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    img: \"img\"\n  }, props.components);\n  return _jsx(_components.p, {\n    children: _jsx(_components.img, {\n      src: \"/media/2023-01-07-clarity-hub-infer/Screen_Shot_2023-01-07_at_3.57.30_PM.png\",\n      alt: \"Screen Shot 2023-01-07 at 3.57.30 PM.png\"\n    })\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n"},{"slug":"2023-01-01-nx-nextjs-starter","date":"2023-01-01","title":"NX NextJS Starter","frontmatter":{"title":"NX NextJS Starter"},"contentRaw":"\nTo kickstart the year, I created a repo that contains a simple starter kit for using NextJS with NX. You can see the repo here:\n\n[Github](https://github.com/idmontie/nx-nextjs-starter)\n\nThis personal Github page is built using this starter kit. I wanted to create a starter kit that was simple and easy to use and also has a lot of eslint and Typescript configuration setup. I've also been working on revamping the website for Dark Emblem - my NFT side-project. The starter kit is based on the linting rules and Typescript set up that I've been using for that project.\n\n## Nx\n\nI've traditionally used Lerna for my monorepo projects, but now that Nx has taken over maintenance of Lerna, I decided to give Nx a try directly.\n\nNx has been enjoyable to use. Managing many different React projects with internal libraries has been very easy to set up, use and deploy.\n\n## NextJS\n\nMy last few projects have been Single Page Apps (SPAs) or statically generated sites using Gatsby or Docusaurus. All three of those tools are great, but I wanted to try out NextJS for a few reasons:\n\n* In my Dark Emblem project, I was having difficulty getting share links to Discord and Twitter to work properly. This was mainly caused by those sites not running JavaScript, so page links would just render the default SPA title and description. I knew that NextJS had a solution for this, so I wanted to try it out.\n* I wanted more control of my documentation and blog websites, so I needed to be able to use custom server-side code.\n\n## Starter Kit\n\nOverall the starter kit is a pretty simple example. It just containers preconfigured Nx, husky, lint-staged, eslint, prettier, Typescript, and NextJS. It does not contain any UI components or anything like that. It's just a simple starter kit to get you up and running with a monorepo NextJS project.\n\nFeel free to check it out at [Github](https://github.com/idmontie/nx-nextjs-starter).","contentHTML":"\u003cp\u003eTo kickstart the year, I created a repo that contains a simple starter kit for using NextJS with NX. You can see the repo here:\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/idmontie/nx-nextjs-starter\"\u003eGithub\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThis personal Github page is built using this starter kit. I wanted to create a starter kit that was simple and easy to use and also has a lot of eslint and Typescript configuration setup. I\u0026#x27;ve also been working on revamping the website for Dark Emblem - my NFT side-project. The starter kit is based on the linting rules and Typescript set up that I\u0026#x27;ve been using for that project.\u003c/p\u003e\n\u003ch2\u003eNx\u003c/h2\u003e\n\u003cp\u003eI\u0026#x27;ve traditionally used Lerna for my monorepo projects, but now that Nx has taken over maintenance of Lerna, I decided to give Nx a try directly.\u003c/p\u003e\n\u003cp\u003eNx has been enjoyable to use. Managing many different React projects with internal libraries has been very easy to set up, use and deploy.\u003c/p\u003e\n\u003ch2\u003eNextJS\u003c/h2\u003e\n\u003cp\u003eMy last few projects have been Single Page Apps (SPAs) or statically generated sites using Gatsby or Docusaurus. All three of those tools are great, but I wanted to try out NextJS for a few reasons:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIn my Dark Emblem project, I was having difficulty getting share links to Discord and Twitter to work properly. This was mainly caused by those sites not running JavaScript, so page links would just render the default SPA title and description. I knew that NextJS had a solution for this, so I wanted to try it out.\u003c/li\u003e\n\u003cli\u003eI wanted more control of my documentation and blog websites, so I needed to be able to use custom server-side code.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eStarter Kit\u003c/h2\u003e\n\u003cp\u003eOverall the starter kit is a pretty simple example. It just containers preconfigured Nx, husky, lint-staged, eslint, prettier, Typescript, and NextJS. It does not contain any UI components or anything like that. It\u0026#x27;s just a simple starter kit to get you up and running with a monorepo NextJS project.\u003c/p\u003e\n\u003cp\u003eFeel free to check it out at \u003ca href=\"https://github.com/idmontie/nx-nextjs-starter\"\u003eGithub\u003c/a\u003e.\u003c/p\u003e","contentCode":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    a: \"a\",\n    h2: \"h2\",\n    ul: \"ul\",\n    li: \"li\"\n  }, props.components);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"To kickstart the year, I created a repo that contains a simple starter kit for using NextJS with NX. You can see the repo here:\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.a, {\n        href: \"https://github.com/idmontie/nx-nextjs-starter\",\n        children: \"Github\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"This personal Github page is built using this starter kit. I wanted to create a starter kit that was simple and easy to use and also has a lot of eslint and Typescript configuration setup. I've also been working on revamping the website for Dark Emblem - my NFT side-project. The starter kit is based on the linting rules and Typescript set up that I've been using for that project.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Nx\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"I've traditionally used Lerna for my monorepo projects, but now that Nx has taken over maintenance of Lerna, I decided to give Nx a try directly.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Nx has been enjoyable to use. Managing many different React projects with internal libraries has been very easy to set up, use and deploy.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"NextJS\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"My last few projects have been Single Page Apps (SPAs) or statically generated sites using Gatsby or Docusaurus. All three of those tools are great, but I wanted to try out NextJS for a few reasons:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"In my Dark Emblem project, I was having difficulty getting share links to Discord and Twitter to work properly. This was mainly caused by those sites not running JavaScript, so page links would just render the default SPA title and description. I knew that NextJS had a solution for this, so I wanted to try it out.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"I wanted more control of my documentation and blog websites, so I needed to be able to use custom server-side code.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Starter Kit\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Overall the starter kit is a pretty simple example. It just containers preconfigured Nx, husky, lint-staged, eslint, prettier, Typescript, and NextJS. It does not contain any UI components or anything like that. It's just a simple starter kit to get you up and running with a monorepo NextJS project.\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Feel free to check it out at \", _jsx(_components.a, {\n        href: \"https://github.com/idmontie/nx-nextjs-starter\",\n        children: \"Github\"\n      }), \".\"]\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","excerptRaw":"\nTo kickstart the year, I created a repo that contains a simple starter kit for using NextJS with NX. You can see the repo here:","excerptHTML":"\u003cp\u003eTo kickstart the year, I created a repo that contains a simple starter kit for using NextJS with NX. You can see the repo here:\u003c/p\u003e","excerptCode":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {jsx: _jsx} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\"\n  }, props.components);\n  return _jsx(_components.p, {\n    children: \"To kickstart the year, I created a repo that contains a simple starter kit for using NextJS with NX. You can see the repo here:\"\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n"}],"hasNextPage":true,"hasPreviousPage":true},"__N_SSG":true},"page":"/blog/[page]","query":{"page":"2"},"buildId":"u7f9GnEl5eUwHpo2DYBNP","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>