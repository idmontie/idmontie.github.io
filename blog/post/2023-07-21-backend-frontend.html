<!DOCTYPE html><html lang="en"><head><meta name="viewport" content="width=device-width"/><meta charSet="utf8"/><title>Backend for Frontend - idmontie&#x27;s Portfolio</title><meta name="description" content="&lt;p&gt;The Backend for the Frontend (BEFFE) is typically stateless and acts as a proxy for other services, including authentication, authorization, and core services. The recent divorce of browser code being rendered by backend services was created by SPAs - Single Page Applications. In simpler architectures, a SPA and service could be as simple as:&lt;/p&gt;"/><meta name="keywords" content="backend, frontend, architecture"/><meta name="next-head-count" content="5"/><script>
                        try {
                            const storage = window && window.localStorage;
                            if (
                                storage.getItem("color-theme") === "dark" ||
                                (!("color-theme" in storage) &&
                                    window.matchMedia("(prefers-color-scheme: dark)").matches)
                            ) {
                                document.documentElement.classList.add("dark");
                            } else {
                                document.documentElement.classList.remove("dark");
                            }
                        } catch (e) {
                            console.error(e);
                        }
                        </script><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin /><link rel="preload" href="/_next/static/css/07709968ac0072c9.css" as="style"/><link rel="stylesheet" href="/_next/static/css/07709968ac0072c9.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-f5a48affa2e5582e.js" defer=""></script><script src="/_next/static/chunks/framework-dfd14d7ce6600b03.js" defer=""></script><script src="/_next/static/chunks/main-3412f4325dfb32ac.js" defer=""></script><script src="/_next/static/chunks/pages/_app-2eb08cd2b5b484a5.js" defer=""></script><script src="/_next/static/chunks/2e6ef0e3-e0509ad90a52e50d.js" defer=""></script><script src="/_next/static/chunks/956-14a634a4f6c953dc.js" defer=""></script><script src="/_next/static/chunks/pages/blog/post/%5Bslug%5D-f9bf7f71392ae31c.js" defer=""></script><script src="/_next/static/p2g4UxFJVz_dfVRSiqzQN/_buildManifest.js" defer=""></script><script src="/_next/static/p2g4UxFJVz_dfVRSiqzQN/_ssgManifest.js" defer=""></script><style data-href="https://fonts.googleapis.com/css2?family=Merriweather:wght@300;400;700&family=Open+Sans:wght@300;400;700&display=swap">@font-face{font-family:'Merriweather';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-4n0qyriQwlOrhSvowK_l521wRpXA.woff) format('woff')}@font-face{font-family:'Merriweather';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-440qyriQwlOrhSvowK_l5OeA.woff) format('woff')}@font-face{font-family:'Merriweather';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-4n0qyriQwlOrhSvowK_l52xwNpXA.woff) format('woff')}@font-face{font-family:'Open Sans';font-style:normal;font-weight:300;font-stretch:normal;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v36/memSYaGs126MiZpBA-UvWbX2vVnXBbObj2OVZyOOSr4dVJWUgsiH0C4k.woff) format('woff')}@font-face{font-family:'Open Sans';font-style:normal;font-weight:400;font-stretch:normal;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v36/memSYaGs126MiZpBA-UvWbX2vVnXBbObj2OVZyOOSr4dVJWUgsjZ0C4k.woff) format('woff')}@font-face{font-family:'Open Sans';font-style:normal;font-weight:700;font-stretch:normal;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v36/memSYaGs126MiZpBA-UvWbX2vVnXBbObj2OVZyOOSr4dVJWUgsg-1y4k.woff) format('woff')}@font-face{font-family:'Merriweather';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-4n0qyriQwlOrhSvowK_l521wRZVcf6hPvhPUWH.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Merriweather';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-4n0qyriQwlOrhSvowK_l521wRZXMf6hPvhPUWH.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Merriweather';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-4n0qyriQwlOrhSvowK_l521wRZV8f6hPvhPUWH.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Merriweather';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-4n0qyriQwlOrhSvowK_l521wRZVsf6hPvhPUWH.woff2) format('woff2');unicode-range:U+0100-02AF,U+0304,U+0308,U+0329,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Merriweather';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-4n0qyriQwlOrhSvowK_l521wRZWMf6hPvhPQ.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Merriweather';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-440qyriQwlOrhSvowK_l5-cSZMdeX3rsHo.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Merriweather';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-440qyriQwlOrhSvowK_l5-eCZMdeX3rsHo.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Merriweather';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-440qyriQwlOrhSvowK_l5-cyZMdeX3rsHo.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Merriweather';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-440qyriQwlOrhSvowK_l5-ciZMdeX3rsHo.woff2) format('woff2');unicode-range:U+0100-02AF,U+0304,U+0308,U+0329,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Merriweather';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-440qyriQwlOrhSvowK_l5-fCZMdeX3rg.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Merriweather';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-4n0qyriQwlOrhSvowK_l52xwNZVcf6hPvhPUWH.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Merriweather';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-4n0qyriQwlOrhSvowK_l52xwNZXMf6hPvhPUWH.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Merriweather';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-4n0qyriQwlOrhSvowK_l52xwNZV8f6hPvhPUWH.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Merriweather';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-4n0qyriQwlOrhSvowK_l52xwNZVsf6hPvhPUWH.woff2) format('woff2');unicode-range:U+0100-02AF,U+0304,U+0308,U+0329,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Merriweather';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/merriweather/v30/u-4n0qyriQwlOrhSvowK_l52xwNZWMf6hPvhPQ.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Open Sans';font-style:normal;font-weight:300;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v36/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSKmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Open Sans';font-style:normal;font-weight:300;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v36/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSumu0SC55K5gw.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Open Sans';font-style:normal;font-weight:300;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v36/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSOmu0SC55K5gw.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:300;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v36/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSymu0SC55K5gw.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:300;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v36/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTS2mu0SC55K5gw.woff2) format('woff2');unicode-range:U+0590-05FF,U+200C-2010,U+20AA,U+25CC,U+FB1D-FB4F}@font-face{font-family:'Open Sans';font-style:normal;font-weight:300;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v36/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSCmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Open Sans';font-style:normal;font-weight:300;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v36/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSGmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0100-02AF,U+0304,U+0308,U+0329,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:300;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v36/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTS-mu0SC55I.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Open Sans';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v36/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSKmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Open Sans';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v36/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSumu0SC55K5gw.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Open Sans';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v36/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSOmu0SC55K5gw.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v36/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSymu0SC55K5gw.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v36/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTS2mu0SC55K5gw.woff2) format('woff2');unicode-range:U+0590-05FF,U+200C-2010,U+20AA,U+25CC,U+FB1D-FB4F}@font-face{font-family:'Open Sans';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v36/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSCmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Open Sans';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v36/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSGmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0100-02AF,U+0304,U+0308,U+0329,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v36/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTS-mu0SC55I.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Open Sans';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v36/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSKmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Open Sans';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v36/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSumu0SC55K5gw.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Open Sans';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v36/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSOmu0SC55K5gw.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v36/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSymu0SC55K5gw.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v36/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTS2mu0SC55K5gw.woff2) format('woff2');unicode-range:U+0590-05FF,U+200C-2010,U+20AA,U+25CC,U+FB1D-FB4F}@font-face{font-family:'Open Sans';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v36/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSCmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Open Sans';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v36/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTSGmu0SC55K5gw.woff2) format('woff2');unicode-range:U+0100-02AF,U+0304,U+0308,U+0329,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Open Sans';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/opensans/v36/memvYaGs126MiZpBA-UvWbX2vVnXBbObj2OVTS-mu0SC55I.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style></head><body class="leading-base bg-white text-lg antialiased dark:bg-gray-900 dark:text-white"><div id="__next"><nav class="flex items-center p-3"><div class="flex-1 space-x-4"><a class="" href="/">Home</a><a class="" href="/blog">Blog</a><a class="" href="/blog/tag">Tags</a><a class="" href="/portfolio">Portfolio</a></div><div class="flex flex-1 justify-end"><label for="toggle" class="relative flex w-fit cursor-pointer items-center">🌑<input type="checkbox" id="toggle" class="sr-only"/><div class="
                        relative
                        h-6
                        w-11
                        rounded-full
                        border-2
                    
                        after:absolute
                        after:h-5
                        after:w-5
                        after:rounded-full
                        after:border
                        after:border-gray-300
                        after:bg-white
                        after:shadow-sm
                        after:transition
                     
                            border-gray-200
                            bg-gray-200
                            dark:border-gray-700
                            dark:bg-gray-700
                        "></div>☀️</label></div></nav><div class="relative overflow-hidden pb-4"><div class="mx-auto flex w-full max-w-screen-md flex-col items-center"><main class="app max-w-full"><div><article class="px-6 md:px-0"><header><h1 class="my-8 text-center text-3xl font-bold dark:text-white md:text-5xl">Backend for Frontend</h1><div class="flex flex-wrap"><a class="mr-2 mb-2 rounded-md bg-gray-100 px-2 py-1 text-sm capitalize dark:bg-gray-800" href="/blog/tag/backend">backend</a><a class="mr-2 mb-2 rounded-md bg-gray-100 px-2 py-1 text-sm capitalize dark:bg-gray-800" href="/blog/tag/frontend">frontend</a><a class="mr-2 mb-2 rounded-md bg-gray-100 px-2 py-1 text-sm capitalize dark:bg-gray-800" href="/blog/tag/architecture">architecture</a></div><div class="mb-6 text-sm">Posted: </div></header><main><div class="prose dark:prose-dark"><div><div><p>The Backend for the Frontend (BEFFE) is typically stateless and acts as a proxy for other services, including authentication, authorization, and core services. The recent divorce of browser code being rendered by backend services was created by SPAs - Single Page Applications. In simpler architectures, a SPA and service could be as simple as:</p>
<div class="py-8 [&amp;_svg]:m-auto"><div class="mermaid" data-mermaid-src="graph LR
  SPA --&gt; Backend">graph LR
  SPA --&gt; Backend</div></div>
<p>SPAs also became popular because the codebase could be built into static assets that could be services.</p>
<p>However, as complexity arises with the above architecture and the SPA starts to rely on more services, a thin proxy is typically introduced, like NGINX:</p>
<div class="py-8 [&amp;_svg]:m-auto"><div class="mermaid" data-mermaid-src="graph LR
  SPA --&gt; NGINX
  NGINX --&gt; Service1
  NGINX --&gt; Service2
  NGINX --&gt; Service3">graph LR
  SPA --&gt; NGINX
  NGINX --&gt; Service1
  NGINX --&gt; Service2
  NGINX --&gt; Service3</div></div>
<p>While proxies like NGINX can continue to be useful, using it as a proxy ends up putting a lot of routing and additional API handling logic on the client. The client now ends up also having to support, understand, and embed how to interact with the data rom all the downstream services. To simplify and create consistent contracts with the UI, we can create a proxy service: a Backend for the Frontend:</p>
<div class="py-8 [&amp;_svg]:m-auto"><div class="mermaid" data-mermaid-src="graph LR
  SPA --&gt; BEFFE
  BEFFE --&gt; Service1
  BEFFE --&gt; Service2
  BEFFE --&gt; Service3">graph LR
  SPA --&gt; BEFFE
  BEFFE --&gt; Service1
  BEFFE --&gt; Service2
  BEFFE --&gt; Service3</div></div>
<p>The Backend for the Frontend ends up serving two purposes:</p>
<ul>
<li>It acts as a proxy for all UI requests</li>
<li>It manages and massages the APIs of downstream services to be a consistent API for the UI.</li>
</ul>
<p>When building a UI, only a single service that acts like an API Gateway is required to understand and encode within the application.</p>
<p>The Backend for the Frontend model also allows engineers working on services to separate logic for the UI with the logic from an internal service. An internal service can rely on the Backend for the Frontend to handle things like Authentication, Authorization, Caching, and Rate-limits. These mechanisms may also be implemented as independent services, but the Backend for the Frontend can make multiple API calls per any API request from the UI.</p>
<h2>Moving away from SPAs</h2>
<p>A newer development in the UI space is server side render the application. If we are already introducing additional complexity by having a Backend for the Frontend, then why not combine the UI code with the backend service and generate hydrated and cacheable pages via the server instead of using a SPA?</p>
<div class="py-8 [&amp;_svg]:m-auto"><div class="mermaid" data-mermaid-src="graph LR
  UIServer[&quot;UI Server&quot;] --&gt; Service1
  UIServer --&gt; Service2
 UIServer --&gt; Service3">graph LR
  UIServer[&quot;UI Server&quot;] --&gt; Service1
  UIServer --&gt; Service2
 UIServer --&gt; Service3</div></div>
<p>With this design, we increase the complexity - the backend not has to be able to render the UI code. It must now render, hydrate, make requests to downstream APIs, and send those server-side rendered pages to the browser.</p>
<p>But this additional complexity enabled us to:</p>
<ul>
<li>create truly authenticated and authorized routes – the code for those routes isn’t even streamed to the browser, unlike a SPA.</li>
<li>near instant page loads – reducing the bundle size by just sending the JavaScript needed to run the page makes this possible. And since API calls are happening within the same network, this saves the user from additional loading spinners after the initial bundle has been loaded.</li>
</ul>
<p>The general trend of having a Backend for the Frontend comes with additional complexity. It stems from the want to enhance the experience of the user, and probably from a subconscious desire to return to a simpler time:</p>
<div class="py-8 [&amp;_svg]:m-auto"><div class="mermaid" data-mermaid-src="graph TD
  Server">graph TD
  Server</div></div></div></div></div><footer class="mt-8"><div class="flex justify-between"><a class="
                inline-block
                rounded-sm
                border
                border-slate-800
                bg-transparent
                px-6
                py-2.5
                text-xs
                font-semibold
                uppercase
                leading-tight
                text-slate-800
                shadow-md
                transition
                duration-150
                ease-in-out
                hover:bg-gray-400
                hover:bg-opacity-25
                hover:shadow-lg
                focus:bg-gray-400
                focus:bg-opacity-25
                focus:shadow-lg
                focus:outline-none
                focus:ring-0
                active:bg-gray-400
                active:bg-opacity-25
                active:shadow-lg
                dark:border-white
                dark:text-white
                " href="/blog/post/2023-07-23-chatgpt-coding-loop"><span class="mr-2">←</span>ChatGPT, Coding, and Language</a><a class="
                inline-block
                rounded-sm
                border
                border-slate-800
                bg-transparent
                px-6
                py-2.5
                text-xs
                font-semibold
                uppercase
                leading-tight
                text-slate-800
                shadow-md
                transition
                duration-150
                ease-in-out
                hover:bg-gray-400
                hover:bg-opacity-25
                hover:shadow-lg
                focus:bg-gray-400
                focus:bg-opacity-25
                focus:shadow-lg
                focus:outline-none
                focus:ring-0
                active:bg-gray-400
                active:bg-opacity-25
                active:shadow-lg
                dark:border-white
                dark:text-white
                " href="/blog/post/2023-07-03-llm-loops">AI Feedback Systems<span class="ml-2">→</span></a></div></footer></main></article></div></main></div></div><footer class="width-full border-top bg-gray-light my-4 mx-auto max-w-[1200px] p-4"><div class="mb-6 flex justify-center"></div><div class="flex-justify-between flex px-3 text-gray-700 dark:text-gray-300"><div class="flex flex-1 flex-row items-center gap-6"><div class="flex flex-row items-center gap-2"><a href="https://github.com/idmontie" target="_blank" rel="noopener noreferrer" type="button" class="m-1 h-9 w-9 rounded-full border-2 border-white uppercase leading-normal text-white transition duration-150 ease-in-out hover:bg-black hover:bg-opacity-5"><svg xmlns="http://www.w3.org/2000/svg" class="mx-auto h-full w-4" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg></a><a href="https://dev.to/idmontie" target="_blank" rel="noopener noreferrer" type="button" class="m-1 h-9 w-9 rounded-full border-2 border-white uppercase leading-normal text-white transition duration-150 ease-in-out hover:bg-black hover:bg-opacity-5"><svg xmlns="http://www.w3.org/2000/svg" class="mx-auto h-full w-4" viewBox="100 100 312 312" fill="currentColor"><path d="M140.47 203.94h-17.44v104.47h17.45c10.155-.545 17.358-8.669 17.47-17.41v-69.65c-.696-10.364-7.796-17.272-17.48-17.41zm45.73 87.25c0 18.81-11.61 47.31-48.36 47.25h-46.4V172.98h47.38c35.44 0 47.36 28.46 47.37 47.28zm100.68-88.66H233.6v38.42h32.57v29.57H233.6v38.41h53.29v29.57h-62.18c-11.16.29-20.44-8.53-20.72-19.69V193.7c-.27-11.15 8.56-20.41 19.71-20.69h63.19zm103.64 115.29c-13.2 30.75-36.85 24.63-47.44 0l-38.53-144.8h32.57l29.71 113.72 29.57-113.72h32.58z"></path></svg></a><a href="https://medium.com/@ivanmontiel" target="_blank" rel="noopener noreferrer" type="button" class="m-1 h-9 w-9 rounded-full border-2 border-white uppercase leading-normal text-white transition duration-150 ease-in-out hover:bg-black hover:bg-opacity-5"><svg xmlns="http://www.w3.org/2000/svg" class="mx-auto h-full w-4" viewBox="0 -55 256 256" version="1.1" preserveAspectRatio="xMidYMid"><g><path d="M72.2009141,1.42108547e-14 C112.076502,1.42108547e-14 144.399375,32.5485469 144.399375,72.6964154 C144.399375,112.844284 112.074049,145.390378 72.2009141,145.390378 C32.327779,145.390378 0,112.844284 0,72.6964154 C0,32.5485469 32.325326,1.42108547e-14 72.2009141,1.42108547e-14 Z M187.500628,4.25836743 C207.438422,4.25836743 223.601085,34.8960455 223.601085,72.6964154 L223.603538,72.6964154 C223.603538,110.486973 207.440875,141.134463 187.503081,141.134463 C167.565287,141.134463 151.402624,110.486973 151.402624,72.6964154 C151.402624,34.9058574 167.562834,4.25836743 187.500628,4.25836743 Z M243.303393,11.3867175 C250.314,11.3867175 256,38.835526 256,72.6964154 C256,106.547493 250.316453,134.006113 243.303393,134.006113 C236.290333,134.006113 230.609239,106.554852 230.609239,72.6964154 C230.609239,38.837979 236.292786,11.3867175 243.303393,11.3867175 Z" fill="currentColor"></path></g></svg></a></div><div>|</div><div><a href="/rss.xml" target="_blank" rel="noopener noreferrer">RSS Feed</a></div></div><div class="flex self-end"><button>Back to top</button></div></div></footer></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"headTitle":"Backend for Frontend - idmontie's Portfolio","headKeywords":"backend, frontend, architecture","post":{"slug":"2023-07-21-backend-frontend","date":"2023-07-21","title":"Backend for Frontend","frontmatter":{"title":"Backend for Frontend","tags":["backend","frontend","architecture"]},"contentRaw":"\nThe Backend for the Frontend (BEFFE) is typically stateless and acts as a proxy for other services, including authentication, authorization, and core services. The recent divorce of browser code being rendered by backend services was created by SPAs - Single Page Applications. In simpler architectures, a SPA and service could be as simple as:\n\n```mermaid\ngraph LR\n  SPA --\u003e Backend\n```\n\nSPAs also became popular because the codebase could be built into static assets that could be services.\n\nHowever, as complexity arises with the above architecture and the SPA starts to rely on more services, a thin proxy is typically introduced, like NGINX:\n\n```mermaid\ngraph LR\n  SPA --\u003e NGINX\n  NGINX --\u003e Service1\n  NGINX --\u003e Service2\n  NGINX --\u003e Service3\n```\n\nWhile proxies like NGINX can continue to be useful, using it as a proxy ends up putting a lot of routing and additional API handling logic on the client. The client now ends up also having to support, understand, and embed how to interact with the data rom all the downstream services. To simplify and create consistent contracts with the UI, we can create a proxy service: a Backend for the Frontend:\n\n```mermaid\ngraph LR\n  SPA --\u003e BEFFE\n  BEFFE --\u003e Service1\n  BEFFE --\u003e Service2\n  BEFFE --\u003e Service3\n```\n\nThe Backend for the Frontend ends up serving two purposes:\n\n- It acts as a proxy for all UI requests\n- It manages and massages the APIs of downstream services to be a consistent API for the UI.\n\nWhen building a UI, only a single service that acts like an API Gateway is required to understand and encode within the application.\n\nThe Backend for the Frontend model also allows engineers working on services to separate logic for the UI with the logic from an internal service. An internal service can rely on the Backend for the Frontend to handle things like Authentication, Authorization, Caching, and Rate-limits. These mechanisms may also be implemented as independent services, but the Backend for the Frontend can make multiple API calls per any API request from the UI.\n\n## Moving away from SPAs\n\nA newer development in the UI space is server side render the application. If we are already introducing additional complexity by having a Backend for the Frontend, then why not combine the UI code with the backend service and generate hydrated and cacheable pages via the server instead of using a SPA?\n\n```mermaid\ngraph LR\n  UIServer[\"UI Server\"] --\u003e Service1\n  UIServer --\u003e Service2\n UIServer --\u003e Service3\n```\n\nWith this design, we increase the complexity - the backend not has to be able to render the UI code. It must now render, hydrate, make requests to downstream APIs, and send those server-side rendered pages to the browser.\n\nBut this additional complexity enabled us to:\n\n- create truly authenticated and authorized routes – the code for those routes isn’t even streamed to the browser, unlike a SPA.\n- near instant page loads – reducing the bundle size by just sending the JavaScript needed to run the page makes this possible. And since API calls are happening within the same network, this saves the user from additional loading spinners after the initial bundle has been loaded.\n\nThe general trend of having a Backend for the Frontend comes with additional complexity. It stems from the want to enhance the experience of the user, and probably from a subconscious desire to return to a simpler time:\n\n```mermaid\ngraph TD\n  Server\n```\n","contentHTML":"\u003cp\u003eThe Backend for the Frontend (BEFFE) is typically stateless and acts as a proxy for other services, including authentication, authorization, and core services. The recent divorce of browser code being rendered by backend services was created by SPAs - Single Page Applications. In simpler architectures, a SPA and service could be as simple as:\u003c/p\u003e\n\u003cdiv class=\"py-8 [\u0026amp;_svg]:m-auto\"\u003e\u003cdiv class=\"mermaid\" data-mermaid-src=\"graph LR\n  SPA --\u0026gt; Backend\"\u003egraph LR\n  SPA --\u0026gt; Backend\u003c/div\u003e\u003c/div\u003e\n\u003cp\u003eSPAs also became popular because the codebase could be built into static assets that could be services.\u003c/p\u003e\n\u003cp\u003eHowever, as complexity arises with the above architecture and the SPA starts to rely on more services, a thin proxy is typically introduced, like NGINX:\u003c/p\u003e\n\u003cdiv class=\"py-8 [\u0026amp;_svg]:m-auto\"\u003e\u003cdiv class=\"mermaid\" data-mermaid-src=\"graph LR\n  SPA --\u0026gt; NGINX\n  NGINX --\u0026gt; Service1\n  NGINX --\u0026gt; Service2\n  NGINX --\u0026gt; Service3\"\u003egraph LR\n  SPA --\u0026gt; NGINX\n  NGINX --\u0026gt; Service1\n  NGINX --\u0026gt; Service2\n  NGINX --\u0026gt; Service3\u003c/div\u003e\u003c/div\u003e\n\u003cp\u003eWhile proxies like NGINX can continue to be useful, using it as a proxy ends up putting a lot of routing and additional API handling logic on the client. The client now ends up also having to support, understand, and embed how to interact with the data rom all the downstream services. To simplify and create consistent contracts with the UI, we can create a proxy service: a Backend for the Frontend:\u003c/p\u003e\n\u003cdiv class=\"py-8 [\u0026amp;_svg]:m-auto\"\u003e\u003cdiv class=\"mermaid\" data-mermaid-src=\"graph LR\n  SPA --\u0026gt; BEFFE\n  BEFFE --\u0026gt; Service1\n  BEFFE --\u0026gt; Service2\n  BEFFE --\u0026gt; Service3\"\u003egraph LR\n  SPA --\u0026gt; BEFFE\n  BEFFE --\u0026gt; Service1\n  BEFFE --\u0026gt; Service2\n  BEFFE --\u0026gt; Service3\u003c/div\u003e\u003c/div\u003e\n\u003cp\u003eThe Backend for the Frontend ends up serving two purposes:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIt acts as a proxy for all UI requests\u003c/li\u003e\n\u003cli\u003eIt manages and massages the APIs of downstream services to be a consistent API for the UI.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWhen building a UI, only a single service that acts like an API Gateway is required to understand and encode within the application.\u003c/p\u003e\n\u003cp\u003eThe Backend for the Frontend model also allows engineers working on services to separate logic for the UI with the logic from an internal service. An internal service can rely on the Backend for the Frontend to handle things like Authentication, Authorization, Caching, and Rate-limits. These mechanisms may also be implemented as independent services, but the Backend for the Frontend can make multiple API calls per any API request from the UI.\u003c/p\u003e\n\u003ch2\u003eMoving away from SPAs\u003c/h2\u003e\n\u003cp\u003eA newer development in the UI space is server side render the application. If we are already introducing additional complexity by having a Backend for the Frontend, then why not combine the UI code with the backend service and generate hydrated and cacheable pages via the server instead of using a SPA?\u003c/p\u003e\n\u003cdiv class=\"py-8 [\u0026amp;_svg]:m-auto\"\u003e\u003cdiv class=\"mermaid\" data-mermaid-src=\"graph LR\n  UIServer[\u0026quot;UI Server\u0026quot;] --\u0026gt; Service1\n  UIServer --\u0026gt; Service2\n UIServer --\u0026gt; Service3\"\u003egraph LR\n  UIServer[\u0026quot;UI Server\u0026quot;] --\u0026gt; Service1\n  UIServer --\u0026gt; Service2\n UIServer --\u0026gt; Service3\u003c/div\u003e\u003c/div\u003e\n\u003cp\u003eWith this design, we increase the complexity - the backend not has to be able to render the UI code. It must now render, hydrate, make requests to downstream APIs, and send those server-side rendered pages to the browser.\u003c/p\u003e\n\u003cp\u003eBut this additional complexity enabled us to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ecreate truly authenticated and authorized routes – the code for those routes isn’t even streamed to the browser, unlike a SPA.\u003c/li\u003e\n\u003cli\u003enear instant page loads – reducing the bundle size by just sending the JavaScript needed to run the page makes this possible. And since API calls are happening within the same network, this saves the user from additional loading spinners after the initial bundle has been loaded.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe general trend of having a Backend for the Frontend comes with additional complexity. It stems from the want to enhance the experience of the user, and probably from a subconscious desire to return to a simpler time:\u003c/p\u003e\n\u003cdiv class=\"py-8 [\u0026amp;_svg]:m-auto\"\u003e\u003cdiv class=\"mermaid\" data-mermaid-src=\"graph TD\n  Server\"\u003egraph TD\n  Server\u003c/div\u003e\u003c/div\u003e","contentCode":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    ul: \"ul\",\n    li: \"li\",\n    h2: \"h2\"\n  }, props.components), {Mermaid} = _components;\n  if (!Mermaid) _missingMdxReference(\"Mermaid\", true);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"The Backend for the Frontend (BEFFE) is typically stateless and acts as a proxy for other services, including authentication, authorization, and core services. The recent divorce of browser code being rendered by backend services was created by SPAs - Single Page Applications. In simpler architectures, a SPA and service could be as simple as:\"\n    }), \"\\n\", _jsx(Mermaid, {\n      chart: \"graph LR\\n  SPA --\u003e Backend\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"SPAs also became popular because the codebase could be built into static assets that could be services.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"However, as complexity arises with the above architecture and the SPA starts to rely on more services, a thin proxy is typically introduced, like NGINX:\"\n    }), \"\\n\", _jsx(Mermaid, {\n      chart: \"graph LR\\n  SPA --\u003e NGINX\\n  NGINX --\u003e Service1\\n  NGINX --\u003e Service2\\n  NGINX --\u003e Service3\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"While proxies like NGINX can continue to be useful, using it as a proxy ends up putting a lot of routing and additional API handling logic on the client. The client now ends up also having to support, understand, and embed how to interact with the data rom all the downstream services. To simplify and create consistent contracts with the UI, we can create a proxy service: a Backend for the Frontend:\"\n    }), \"\\n\", _jsx(Mermaid, {\n      chart: \"graph LR\\n  SPA --\u003e BEFFE\\n  BEFFE --\u003e Service1\\n  BEFFE --\u003e Service2\\n  BEFFE --\u003e Service3\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The Backend for the Frontend ends up serving two purposes:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"It acts as a proxy for all UI requests\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"It manages and massages the APIs of downstream services to be a consistent API for the UI.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"When building a UI, only a single service that acts like an API Gateway is required to understand and encode within the application.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The Backend for the Frontend model also allows engineers working on services to separate logic for the UI with the logic from an internal service. An internal service can rely on the Backend for the Frontend to handle things like Authentication, Authorization, Caching, and Rate-limits. These mechanisms may also be implemented as independent services, but the Backend for the Frontend can make multiple API calls per any API request from the UI.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Moving away from SPAs\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"A newer development in the UI space is server side render the application. If we are already introducing additional complexity by having a Backend for the Frontend, then why not combine the UI code with the backend service and generate hydrated and cacheable pages via the server instead of using a SPA?\"\n    }), \"\\n\", _jsx(Mermaid, {\n      chart: \"graph LR\\n  UIServer[\\\"UI Server\\\"] --\u003e Service1\\n  UIServer --\u003e Service2\\n UIServer --\u003e Service3\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"With this design, we increase the complexity - the backend not has to be able to render the UI code. It must now render, hydrate, make requests to downstream APIs, and send those server-side rendered pages to the browser.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"But this additional complexity enabled us to:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"create truly authenticated and authorized routes – the code for those routes isn’t even streamed to the browser, unlike a SPA.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"near instant page loads – reducing the bundle size by just sending the JavaScript needed to run the page makes this possible. And since API calls are happening within the same network, this saves the user from additional loading spinners after the initial bundle has been loaded.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The general trend of having a Backend for the Frontend comes with additional complexity. It stems from the want to enhance the experience of the user, and probably from a subconscious desire to return to a simpler time:\"\n    }), \"\\n\", _jsx(Mermaid, {\n      chart: \"graph TD\\n  Server\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\nfunction _missingMdxReference(id, component) {\n  throw new Error(\"Expected \" + (component ? \"component\" : \"object\") + \" `\" + id + \"` to be defined: you likely forgot to import, pass, or provide it.\");\n}\n","excerptRaw":"\nThe Backend for the Frontend (BEFFE) is typically stateless and acts as a proxy for other services, including authentication, authorization, and core services. The recent divorce of browser code being rendered by backend services was created by SPAs - Single Page Applications. In simpler architectures, a SPA and service could be as simple as:","excerptHTML":"\u003cp\u003eThe Backend for the Frontend (BEFFE) is typically stateless and acts as a proxy for other services, including authentication, authorization, and core services. The recent divorce of browser code being rendered by backend services was created by SPAs - Single Page Applications. In simpler architectures, a SPA and service could be as simple as:\u003c/p\u003e","excerptCode":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {jsx: _jsx} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\"\n  }, props.components);\n  return _jsx(_components.p, {\n    children: \"The Backend for the Frontend (BEFFE) is typically stateless and acts as a proxy for other services, including authentication, authorization, and core services. The recent divorce of browser code being rendered by backend services was created by SPAs - Single Page Applications. In simpler architectures, a SPA and service could be as simple as:\"\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","tags":["backend","frontend","architecture"]},"previous":{"slug":"2023-07-23-chatgpt-coding-loop","date":"2023-07-23","title":"ChatGPT, Coding, and Language","frontmatter":{"title":"ChatGPT, Coding, and Language","tags":["chatgpt"]},"contentRaw":"\nI’ve been experimenting with ChatGPT, just like everyone else. But why has it become so popular? It truly is a revolutionary piece of technology. Is it The Next Big Thing? Will it really replace all of us?\n\nMy day to day is architecture and programming, and I’ve heard all sorts of things on Twitter:\n\n- That ChatGPT can build entire iOS applications\n- ChatGPT can write whole files of code\n- ChatGPT can find bugs in code when writing tests\n\nWhen I started by own company Clarity Hub, we had a focus on machine learning to help augment human intelligence. My thought at the time was that we could leverage machine learning to augment and supplement human intelligence. The focus wouldn't be to replace any person's job, but to make it streamlined and easier to accomplish that job. Our journey started with us serving customer success agents with suggested replies and contextual information, but we eventually pivoted to a Dovetail-like application to help product teams gather, annotate, and contextualize customer interviews.\n\nWe found that augmenting activity with machine learning was not only easier to do from a technology point of view, but empowered users, rather than make them feel like their job was being replaced.\n\nEven with the advent of ChatGPT, I still see this being the short term future of it's use.\n\n## False Starts\n\nI was curious how much ChatGPT could really do given all of its hype  \n\nI asked it to build an Asteroids game using PhaserJS. Don’t worry, this isn’t another article on “Look, I got ChatGPT to build a game!”\n\nI originally just asked ChatGPT to build an Asteroids game using PhaserJS. A very vague prompt, yes, but a great starting point to figure out the limitations of ChatGPT.\n\nThe response was basically, “No I can’t”:\n\n\u003e As an AI language model, I'm not able to write and execute code in real-time. However, I can provide you with a detailed framework to get started with creating an Asteroids game in PhaserJS.\n\nThis was followed by instructions on how I could do it myself.\n\nThe next steps of course are to be more specific, asking ChatGPT to build the application to build specific functionality:\n\n\u003e Add the code to let the player shoot asteroids. If the player gets hit by 3 asteroids, its game over\n\nMost interactions with ChatGPT seem to go like this:\n\n![Flowchart workflow](/media/2023-07-23-chatgpt-coding-loop/Flowchart_Template_1.jpg)\n\nThis flow eventually leads to either:\n\n- Not getting what you want.\n- Getting what you want across many different responses, and then you must manually coalesce all of the different responses together to form the result you want.\n\nOnce I like the set of responses that ChatGPT has given me, I’ll ask ChatGPT to summarize for me so that I can improve the prompt further:\n\n\u003e Take everything we just talked about and give me a prompt to ask you in the future that encapsulates all of the requirements.\n\n## A More Natural Flow\n\nI’ve talked with ChatGPT on a variety of topics, but the majority of questions relate to coding. I actively use Github Copilot, which utilizes the ChatGPT engine in the backend. I’ve built [Sora by Capsule Cat](https://marketplace.visualstudio.com/items?itemName=CapsuleCat.sora-by-capsule-cat) to have ChatGPT generate entire coding files for me as well with project context. Even then, I still use the ChatGPT UI as well.\n\n![Flowchart line](/media/2023-07-23-chatgpt-coding-loop/Flowchart_Template_2.jpg)\n\nI find that the above flow leaves me less fatigued, and more productive - at least in the context of side-projects that I’ve used ChatGPT. Research shows that when we read code and attempt logic puzzles, we use a completely different part of our brain than when we use language [[link](https://hub.jhu.edu/2020/12/17/brain-activity-while-reading-code/)]. In the above chart, you can imagine that when we write prompts, we utilize some logical reasoning, but we lean heavily on language as we talk with ChatGPT. Then when we get a response, a completely different part of the brain kicks in to read and write code.\n\n## The Hard Part About Coding\n\nEventually this leads to “the hard part about coding” which anecdotally everyone will tell you that surprise, it isn’t the coding part. What is nice about ChatGPT though is that even though the hard part isn’t writing code, it does take time, thought, and energy.\n\nIf the hard part of coding isn’t coding, then what is it? The non-exhaustive list is that it’s teamwork, communication, debugging, and requirements gathering. Oversimplifying, it’s the writing prompts part of the chart.\n","contentHTML":"\u003cp\u003eI’ve been experimenting with ChatGPT, just like everyone else. But why has it become so popular? It truly is a revolutionary piece of technology. Is it The Next Big Thing? Will it really replace all of us?\u003c/p\u003e\n\u003cp\u003eMy day to day is architecture and programming, and I’ve heard all sorts of things on Twitter:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThat ChatGPT can build entire iOS applications\u003c/li\u003e\n\u003cli\u003eChatGPT can write whole files of code\u003c/li\u003e\n\u003cli\u003eChatGPT can find bugs in code when writing tests\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWhen I started by own company Clarity Hub, we had a focus on machine learning to help augment human intelligence. My thought at the time was that we could leverage machine learning to augment and supplement human intelligence. The focus wouldn\u0026#x27;t be to replace any person\u0026#x27;s job, but to make it streamlined and easier to accomplish that job. Our journey started with us serving customer success agents with suggested replies and contextual information, but we eventually pivoted to a Dovetail-like application to help product teams gather, annotate, and contextualize customer interviews.\u003c/p\u003e\n\u003cp\u003eWe found that augmenting activity with machine learning was not only easier to do from a technology point of view, but empowered users, rather than make them feel like their job was being replaced.\u003c/p\u003e\n\u003cp\u003eEven with the advent of ChatGPT, I still see this being the short term future of it\u0026#x27;s use.\u003c/p\u003e\n\u003ch2\u003eFalse Starts\u003c/h2\u003e\n\u003cp\u003eI was curious how much ChatGPT could really do given all of its hype\u003c/p\u003e\n\u003cp\u003eI asked it to build an Asteroids game using PhaserJS. Don’t worry, this isn’t another article on “Look, I got ChatGPT to build a game!”\u003c/p\u003e\n\u003cp\u003eI originally just asked ChatGPT to build an Asteroids game using PhaserJS. A very vague prompt, yes, but a great starting point to figure out the limitations of ChatGPT.\u003c/p\u003e\n\u003cp\u003eThe response was basically, “No I can’t”:\u003c/p\u003e\n\u003cblockquote class=\"border-l-4 border-gray-300 pl-4\"\u003e\n\u003cp\u003eAs an AI language model, I\u0026#x27;m not able to write and execute code in real-time. However, I can provide you with a detailed framework to get started with creating an Asteroids game in PhaserJS.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThis was followed by instructions on how I could do it myself.\u003c/p\u003e\n\u003cp\u003eThe next steps of course are to be more specific, asking ChatGPT to build the application to build specific functionality:\u003c/p\u003e\n\u003cblockquote class=\"border-l-4 border-gray-300 pl-4\"\u003e\n\u003cp\u003eAdd the code to let the player shoot asteroids. If the player gets hit by 3 asteroids, its game over\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eMost interactions with ChatGPT seem to go like this:\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Flowchart workflow\" src=\"/media/2023-07-23-chatgpt-coding-loop/Flowchart_Template_1.jpg\" style=\"max-height:500px;margin:auto;text-align:center\"/\u003e\u003c/p\u003e\n\u003cp\u003eThis flow eventually leads to either:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNot getting what you want.\u003c/li\u003e\n\u003cli\u003eGetting what you want across many different responses, and then you must manually coalesce all of the different responses together to form the result you want.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eOnce I like the set of responses that ChatGPT has given me, I’ll ask ChatGPT to summarize for me so that I can improve the prompt further:\u003c/p\u003e\n\u003cblockquote class=\"border-l-4 border-gray-300 pl-4\"\u003e\n\u003cp\u003eTake everything we just talked about and give me a prompt to ask you in the future that encapsulates all of the requirements.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003eA More Natural Flow\u003c/h2\u003e\n\u003cp\u003eI’ve talked with ChatGPT on a variety of topics, but the majority of questions relate to coding. I actively use Github Copilot, which utilizes the ChatGPT engine in the backend. I’ve built \u003ca href=\"https://marketplace.visualstudio.com/items?itemName=CapsuleCat.sora-by-capsule-cat\"\u003eSora by Capsule Cat\u003c/a\u003e to have ChatGPT generate entire coding files for me as well with project context. Even then, I still use the ChatGPT UI as well.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Flowchart line\" src=\"/media/2023-07-23-chatgpt-coding-loop/Flowchart_Template_2.jpg\" style=\"max-height:500px;margin:auto;text-align:center\"/\u003e\u003c/p\u003e\n\u003cp\u003eI find that the above flow leaves me less fatigued, and more productive - at least in the context of side-projects that I’ve used ChatGPT. Research shows that when we read code and attempt logic puzzles, we use a completely different part of our brain than when we use language [\u003ca href=\"https://hub.jhu.edu/2020/12/17/brain-activity-while-reading-code/\"\u003elink\u003c/a\u003e]. In the above chart, you can imagine that when we write prompts, we utilize some logical reasoning, but we lean heavily on language as we talk with ChatGPT. Then when we get a response, a completely different part of the brain kicks in to read and write code.\u003c/p\u003e\n\u003ch2\u003eThe Hard Part About Coding\u003c/h2\u003e\n\u003cp\u003eEventually this leads to “the hard part about coding” which anecdotally everyone will tell you that surprise, it isn’t the coding part. What is nice about ChatGPT though is that even though the hard part isn’t writing code, it does take time, thought, and energy.\u003c/p\u003e\n\u003cp\u003eIf the hard part of coding isn’t coding, then what is it? The non-exhaustive list is that it’s teamwork, communication, debugging, and requirements gathering. Oversimplifying, it’s the writing prompts part of the chart.\u003c/p\u003e","contentCode":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    ul: \"ul\",\n    li: \"li\",\n    h2: \"h2\",\n    blockquote: \"blockquote\",\n    img: \"img\",\n    a: \"a\"\n  }, props.components);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"I’ve been experimenting with ChatGPT, just like everyone else. But why has it become so popular? It truly is a revolutionary piece of technology. Is it The Next Big Thing? Will it really replace all of us?\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"My day to day is architecture and programming, and I’ve heard all sorts of things on Twitter:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"That ChatGPT can build entire iOS applications\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"ChatGPT can write whole files of code\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"ChatGPT can find bugs in code when writing tests\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"When I started by own company Clarity Hub, we had a focus on machine learning to help augment human intelligence. My thought at the time was that we could leverage machine learning to augment and supplement human intelligence. The focus wouldn't be to replace any person's job, but to make it streamlined and easier to accomplish that job. Our journey started with us serving customer success agents with suggested replies and contextual information, but we eventually pivoted to a Dovetail-like application to help product teams gather, annotate, and contextualize customer interviews.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"We found that augmenting activity with machine learning was not only easier to do from a technology point of view, but empowered users, rather than make them feel like their job was being replaced.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Even with the advent of ChatGPT, I still see this being the short term future of it's use.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"False Starts\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"I was curious how much ChatGPT could really do given all of its hype\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"I asked it to build an Asteroids game using PhaserJS. Don’t worry, this isn’t another article on “Look, I got ChatGPT to build a game!”\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"I originally just asked ChatGPT to build an Asteroids game using PhaserJS. A very vague prompt, yes, but a great starting point to figure out the limitations of ChatGPT.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The response was basically, “No I can’t”:\"\n    }), \"\\n\", _jsxs(_components.blockquote, {\n      children: [\"\\n\", _jsx(_components.p, {\n        children: \"As an AI language model, I'm not able to write and execute code in real-time. However, I can provide you with a detailed framework to get started with creating an Asteroids game in PhaserJS.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"This was followed by instructions on how I could do it myself.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The next steps of course are to be more specific, asking ChatGPT to build the application to build specific functionality:\"\n    }), \"\\n\", _jsxs(_components.blockquote, {\n      children: [\"\\n\", _jsx(_components.p, {\n        children: \"Add the code to let the player shoot asteroids. If the player gets hit by 3 asteroids, its game over\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Most interactions with ChatGPT seem to go like this:\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/media/2023-07-23-chatgpt-coding-loop/Flowchart_Template_1.jpg\",\n        alt: \"Flowchart workflow\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"This flow eventually leads to either:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"Not getting what you want.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Getting what you want across many different responses, and then you must manually coalesce all of the different responses together to form the result you want.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Once I like the set of responses that ChatGPT has given me, I’ll ask ChatGPT to summarize for me so that I can improve the prompt further:\"\n    }), \"\\n\", _jsxs(_components.blockquote, {\n      children: [\"\\n\", _jsx(_components.p, {\n        children: \"Take everything we just talked about and give me a prompt to ask you in the future that encapsulates all of the requirements.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"A More Natural Flow\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"I’ve talked with ChatGPT on a variety of topics, but the majority of questions relate to coding. I actively use Github Copilot, which utilizes the ChatGPT engine in the backend. I’ve built \", _jsx(_components.a, {\n        href: \"https://marketplace.visualstudio.com/items?itemName=CapsuleCat.sora-by-capsule-cat\",\n        children: \"Sora by Capsule Cat\"\n      }), \" to have ChatGPT generate entire coding files for me as well with project context. Even then, I still use the ChatGPT UI as well.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/media/2023-07-23-chatgpt-coding-loop/Flowchart_Template_2.jpg\",\n        alt: \"Flowchart line\"\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"I find that the above flow leaves me less fatigued, and more productive - at least in the context of side-projects that I’ve used ChatGPT. Research shows that when we read code and attempt logic puzzles, we use a completely different part of our brain than when we use language [\", _jsx(_components.a, {\n        href: \"https://hub.jhu.edu/2020/12/17/brain-activity-while-reading-code/\",\n        children: \"link\"\n      }), \"]. In the above chart, you can imagine that when we write prompts, we utilize some logical reasoning, but we lean heavily on language as we talk with ChatGPT. Then when we get a response, a completely different part of the brain kicks in to read and write code.\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"The Hard Part About Coding\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Eventually this leads to “the hard part about coding” which anecdotally everyone will tell you that surprise, it isn’t the coding part. What is nice about ChatGPT though is that even though the hard part isn’t writing code, it does take time, thought, and energy.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"If the hard part of coding isn’t coding, then what is it? The non-exhaustive list is that it’s teamwork, communication, debugging, and requirements gathering. Oversimplifying, it’s the writing prompts part of the chart.\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","excerptRaw":"\nI’ve been experimenting with ChatGPT, just like everyone else. But why has it become so popular? It truly is a revolutionary piece of technology. Is it The Next Big Thing? Will it really replace all of us?","excerptHTML":"\u003cp\u003eI’ve been experimenting with ChatGPT, just like everyone else. But why has it become so popular? It truly is a revolutionary piece of technology. Is it The Next Big Thing? Will it really replace all of us?\u003c/p\u003e","excerptCode":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {jsx: _jsx} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\"\n  }, props.components);\n  return _jsx(_components.p, {\n    children: \"I’ve been experimenting with ChatGPT, just like everyone else. But why has it become so popular? It truly is a revolutionary piece of technology. Is it The Next Big Thing? Will it really replace all of us?\"\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","tags":["chatgpt"]},"next":{"slug":"2023-07-03-llm-loops","date":"2023-07-03","title":"AI Feedback Systems","frontmatter":{"title":"AI Feedback Systems","tags":["ai","llm"]},"contentRaw":"\nWe are starting to see a rise of novel use-cases for AI in products and games using LLMs. Rather than the simple chatbot like experiences we have seen in the past using AI, we are starting to see feedback systems being added to these experiences, providing additional context to the LLM than just the past conversation.\n\nA typical game loop for this type of system would look like:\n\n```mermaid\ngraph LR\n  Rules --\u003e LLM\n  InputStates[\"Input States\"] --\u003e LLM\n  LLM --\u003e OutputState\n  OutputState[\"Output State\"] --\u003e GameEngine\n  GameEngine[\"Game Engine\"] --\u003e InputStates\n```\n\nRules can be describes as written-word description, with an additional set of rules telling the LLM to reply using JSON output of a given schema. In this area, I have had success giving LLMs descriptions of output schemas in Typescript and asking for a JSON response that adheres to the type. Other methods of getting a consistent schema are more than likely possible here, as well as additional output methods.\n\nWhen the asynchronous task of creating and output state is complete, the Game Engine in this case can read, parse, and apply that new state to the world. Any additional interaction would then lead to the next set of input states that can be given to the LLM as a JSON blob.\n\nFor a more concrete example, we can imagine a game where we want our player to interact with a set of agents. The input states would be the state of each agent, the user’s interaction, and maybe some global environment data. The rules may be how each agent should behave, the rules of the game, and additional context. The LLM would take these inputs, and the output is instructed to be the next state of each agent. When the LLM returns this data, the Game Engine read it and applies it to the game’s representation of each agent, showing the player the impact of their actions.\n\nI’m looking forward to more novel use-cases for LLMs!\n","contentHTML":"\u003cp\u003eWe are starting to see a rise of novel use-cases for AI in products and games using LLMs. Rather than the simple chatbot like experiences we have seen in the past using AI, we are starting to see feedback systems being added to these experiences, providing additional context to the LLM than just the past conversation.\u003c/p\u003e\n\u003cp\u003eA typical game loop for this type of system would look like:\u003c/p\u003e\n\u003cdiv class=\"py-8 [\u0026amp;_svg]:m-auto\"\u003e\u003cdiv class=\"mermaid\" data-mermaid-src=\"graph LR\n  Rules --\u0026gt; LLM\n  InputStates[\u0026quot;Input States\u0026quot;] --\u0026gt; LLM\n  LLM --\u0026gt; OutputState\n  OutputState[\u0026quot;Output State\u0026quot;] --\u0026gt; GameEngine\n  GameEngine[\u0026quot;Game Engine\u0026quot;] --\u0026gt; InputStates\"\u003egraph LR\n  Rules --\u0026gt; LLM\n  InputStates[\u0026quot;Input States\u0026quot;] --\u0026gt; LLM\n  LLM --\u0026gt; OutputState\n  OutputState[\u0026quot;Output State\u0026quot;] --\u0026gt; GameEngine\n  GameEngine[\u0026quot;Game Engine\u0026quot;] --\u0026gt; InputStates\u003c/div\u003e\u003c/div\u003e\n\u003cp\u003eRules can be describes as written-word description, with an additional set of rules telling the LLM to reply using JSON output of a given schema. In this area, I have had success giving LLMs descriptions of output schemas in Typescript and asking for a JSON response that adheres to the type. Other methods of getting a consistent schema are more than likely possible here, as well as additional output methods.\u003c/p\u003e\n\u003cp\u003eWhen the asynchronous task of creating and output state is complete, the Game Engine in this case can read, parse, and apply that new state to the world. Any additional interaction would then lead to the next set of input states that can be given to the LLM as a JSON blob.\u003c/p\u003e\n\u003cp\u003eFor a more concrete example, we can imagine a game where we want our player to interact with a set of agents. The input states would be the state of each agent, the user’s interaction, and maybe some global environment data. The rules may be how each agent should behave, the rules of the game, and additional context. The LLM would take these inputs, and the output is instructed to be the next state of each agent. When the LLM returns this data, the Game Engine read it and applies it to the game’s representation of each agent, showing the player the impact of their actions.\u003c/p\u003e\n\u003cp\u003eI’m looking forward to more novel use-cases for LLMs!\u003c/p\u003e","contentCode":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\"\n  }, props.components), {Mermaid} = _components;\n  if (!Mermaid) _missingMdxReference(\"Mermaid\", true);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"We are starting to see a rise of novel use-cases for AI in products and games using LLMs. Rather than the simple chatbot like experiences we have seen in the past using AI, we are starting to see feedback systems being added to these experiences, providing additional context to the LLM than just the past conversation.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"A typical game loop for this type of system would look like:\"\n    }), \"\\n\", _jsx(Mermaid, {\n      chart: \"graph LR\\n  Rules --\u003e LLM\\n  InputStates[\\\"Input States\\\"] --\u003e LLM\\n  LLM --\u003e OutputState\\n  OutputState[\\\"Output State\\\"] --\u003e GameEngine\\n  GameEngine[\\\"Game Engine\\\"] --\u003e InputStates\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Rules can be describes as written-word description, with an additional set of rules telling the LLM to reply using JSON output of a given schema. In this area, I have had success giving LLMs descriptions of output schemas in Typescript and asking for a JSON response that adheres to the type. Other methods of getting a consistent schema are more than likely possible here, as well as additional output methods.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"When the asynchronous task of creating and output state is complete, the Game Engine in this case can read, parse, and apply that new state to the world. Any additional interaction would then lead to the next set of input states that can be given to the LLM as a JSON blob.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"For a more concrete example, we can imagine a game where we want our player to interact with a set of agents. The input states would be the state of each agent, the user’s interaction, and maybe some global environment data. The rules may be how each agent should behave, the rules of the game, and additional context. The LLM would take these inputs, and the output is instructed to be the next state of each agent. When the LLM returns this data, the Game Engine read it and applies it to the game’s representation of each agent, showing the player the impact of their actions.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"I’m looking forward to more novel use-cases for LLMs!\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\nfunction _missingMdxReference(id, component) {\n  throw new Error(\"Expected \" + (component ? \"component\" : \"object\") + \" `\" + id + \"` to be defined: you likely forgot to import, pass, or provide it.\");\n}\n","excerptRaw":"\nWe are starting to see a rise of novel use-cases for AI in products and games using LLMs. Rather than the simple chatbot like experiences we have seen in the past using AI, we are starting to see feedback systems being added to these experiences, providing additional context to the LLM than just the past conversation.","excerptHTML":"\u003cp\u003eWe are starting to see a rise of novel use-cases for AI in products and games using LLMs. Rather than the simple chatbot like experiences we have seen in the past using AI, we are starting to see feedback systems being added to these experiences, providing additional context to the LLM than just the past conversation.\u003c/p\u003e","excerptCode":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {jsx: _jsx} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\"\n  }, props.components);\n  return _jsx(_components.p, {\n    children: \"We are starting to see a rise of novel use-cases for AI in products and games using LLMs. Rather than the simple chatbot like experiences we have seen in the past using AI, we are starting to see feedback systems being added to these experiences, providing additional context to the LLM than just the past conversation.\"\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","tags":["ai","llm"]}},"__N_SSG":true},"page":"/blog/post/[slug]","query":{"slug":"2023-07-21-backend-frontend"},"buildId":"p2g4UxFJVz_dfVRSiqzQN","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>