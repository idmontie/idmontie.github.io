{"pageProps":{"headTitle":"AI Feedback Systems - idmontie's Portfolio","post":{"slug":"2023-07-03-llm-loops","date":"2023-07-03","title":"AI Feedback Systems","frontmatter":{"title":"AI Feedback Systems"},"contentRaw":"\nWe are starting to see a rise of novel use-cases for AI in products and games using LLMs. Rather than the simple chatbot like experiences we have seen in the past using AI, we are starting to see feedback systems being added to these experiences, providing additional context to the LLM than just the past conversation.\n\nA typical game loop for this type of system would look like:\n\n```mermaid\ngraph LR\n  Rules --> LLM  \n  InputStates[\"Input States\"] --> LLM\n  LLM --> OutputState\n  OutputState[\"Output State\"] --> GameEngine\n  GameEngine[\"Game Engine\"] --> InputStates\n```\n\nRules can be describes as written-word description, with an additional set of rules telling the LLM to reply using JSON output of a given schema. In this area, I have had success giving LLMs descriptions of output schemas in Typescript and asking for a JSON response that adheres to the type. Other methods of getting a consistent schema are more than likely possible here, as well as additional output methods.\n\nWhen the asynchronous task of creating and output state is complete, the Game Engine in this case can read, parse, and apply that new state to the world. Any additional interaction would then lead to the next set of input states that can be given to the LLM as a JSON blob.\n\nFor a more concrete example, we can imagine a game where we want our player to interact with a set of agents. The input states would be the state of each agent, the user’s interaction, and maybe some global environment data. The rules may be how each agent should behave, the rules of the game, and additional context. The LLM would take these inputs, and the output is instructed to be the next state of each agent. When the LLM returns this data, the Game Engine read it and applies it to the game’s representation of each agent, showing the player the impact of their actions.\n\nI’m looking forward to more novel use-cases for LLMs!\n","contentHTML":"<p>We are starting to see a rise of novel use-cases for AI in products and games using LLMs. Rather than the simple chatbot like experiences we have seen in the past using AI, we are starting to see feedback systems being added to these experiences, providing additional context to the LLM than just the past conversation.</p>\n<p>A typical game loop for this type of system would look like:</p>\n<div class=\"py-8 [&amp;_svg]:m-auto\"><div class=\"mermaid\" data-mermaid-src=\"graph LR\n  Rules --&gt; LLM  \n  InputStates[&quot;Input States&quot;] --&gt; LLM\n  LLM --&gt; OutputState\n  OutputState[&quot;Output State&quot;] --&gt; GameEngine\n  GameEngine[&quot;Game Engine&quot;] --&gt; InputStates\">graph LR\n  Rules --&gt; LLM  \n  InputStates[&quot;Input States&quot;] --&gt; LLM\n  LLM --&gt; OutputState\n  OutputState[&quot;Output State&quot;] --&gt; GameEngine\n  GameEngine[&quot;Game Engine&quot;] --&gt; InputStates</div></div>\n<p>Rules can be describes as written-word description, with an additional set of rules telling the LLM to reply using JSON output of a given schema. In this area, I have had success giving LLMs descriptions of output schemas in Typescript and asking for a JSON response that adheres to the type. Other methods of getting a consistent schema are more than likely possible here, as well as additional output methods.</p>\n<p>When the asynchronous task of creating and output state is complete, the Game Engine in this case can read, parse, and apply that new state to the world. Any additional interaction would then lead to the next set of input states that can be given to the LLM as a JSON blob.</p>\n<p>For a more concrete example, we can imagine a game where we want our player to interact with a set of agents. The input states would be the state of each agent, the user’s interaction, and maybe some global environment data. The rules may be how each agent should behave, the rules of the game, and additional context. The LLM would take these inputs, and the output is instructed to be the next state of each agent. When the LLM returns this data, the Game Engine read it and applies it to the game’s representation of each agent, showing the player the impact of their actions.</p>\n<p>I’m looking forward to more novel use-cases for LLMs!</p>","contentCode":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\"\n  }, props.components), {Mermaid} = _components;\n  if (!Mermaid) _missingMdxReference(\"Mermaid\", true);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"We are starting to see a rise of novel use-cases for AI in products and games using LLMs. Rather than the simple chatbot like experiences we have seen in the past using AI, we are starting to see feedback systems being added to these experiences, providing additional context to the LLM than just the past conversation.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"A typical game loop for this type of system would look like:\"\n    }), \"\\n\", _jsx(Mermaid, {\n      chart: \"graph LR\\n  Rules --> LLM  \\n  InputStates[\\\"Input States\\\"] --> LLM\\n  LLM --> OutputState\\n  OutputState[\\\"Output State\\\"] --> GameEngine\\n  GameEngine[\\\"Game Engine\\\"] --> InputStates\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Rules can be describes as written-word description, with an additional set of rules telling the LLM to reply using JSON output of a given schema. In this area, I have had success giving LLMs descriptions of output schemas in Typescript and asking for a JSON response that adheres to the type. Other methods of getting a consistent schema are more than likely possible here, as well as additional output methods.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"When the asynchronous task of creating and output state is complete, the Game Engine in this case can read, parse, and apply that new state to the world. Any additional interaction would then lead to the next set of input states that can be given to the LLM as a JSON blob.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"For a more concrete example, we can imagine a game where we want our player to interact with a set of agents. The input states would be the state of each agent, the user’s interaction, and maybe some global environment data. The rules may be how each agent should behave, the rules of the game, and additional context. The LLM would take these inputs, and the output is instructed to be the next state of each agent. When the LLM returns this data, the Game Engine read it and applies it to the game’s representation of each agent, showing the player the impact of their actions.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"I’m looking forward to more novel use-cases for LLMs!\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\nfunction _missingMdxReference(id, component) {\n  throw new Error(\"Expected \" + (component ? \"component\" : \"object\") + \" `\" + id + \"` to be defined: you likely forgot to import, pass, or provide it.\");\n}\n","excerptRaw":"\nWe are starting to see a rise of novel use-cases for AI in products and games using LLMs. Rather than the simple chatbot like experiences we have seen in the past using AI, we are starting to see feedback systems being added to these experiences, providing additional context to the LLM than just the past conversation.","excerptHTML":"<p>We are starting to see a rise of novel use-cases for AI in products and games using LLMs. Rather than the simple chatbot like experiences we have seen in the past using AI, we are starting to see feedback systems being added to these experiences, providing additional context to the LLM than just the past conversation.</p>","excerptCode":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {jsx: _jsx} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\"\n  }, props.components);\n  return _jsx(_components.p, {\n    children: \"We are starting to see a rise of novel use-cases for AI in products and games using LLMs. Rather than the simple chatbot like experiences we have seen in the past using AI, we are starting to see feedback systems being added to these experiences, providing additional context to the LLM than just the past conversation.\"\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n"},"previous":{"slug":"2023-07-21-backend-frontend","date":"2023-07-21","title":"Backend for Frontend","frontmatter":{"title":"Backend for Frontend"},"contentRaw":"\nThe Backend for the Frontend (BEFFE) is typically stateless and acts as a proxy for other services, including authentication, authorization, and core services. The recent divorce of browser code being rendered by backend services was created by SPAs - Single Page Applications. In simpler architectures, a SPA and service could be as simple as:\n\n```mermaid\ngraph LR\n  SPA --> Backend\n```\n\nSPAs also became popular because the codebase could be built into static assets that could be services.\n\nHowever, as complexity arises with the above architecture and the SPA starts to rely on more services, a thin proxy is typically introduced, like NGINX:\n\n```mermaid\ngraph LR\n  SPA --> NGINX\n  NGINX --> Service1\n  NGINX --> Service2\n  NGINX --> Service3\n```\n\nWhile proxies like NGINX can continue to be useful, using it as a proxy ends up putting a lot of routing and additional API handling logic on the client. The client now ends up also having to support, understand, and embed how to interact with the data rom all the downstream services. To simplify and create consistent contracts with the UI, we can create a proxy service: a Backend for the Frontend:\n\n```mermaid\ngraph LR\n  SPA --> BEFFE\n  BEFFE --> Service1\n  BEFFE --> Service2\n  BEFFE --> Service3\n```\n\nThe Backend for the Frontend ends up serving two purposes:\n\n- It acts as a proxy for all UI requests\n- It manages and massages the APIs of downstream services to be a consistent API for the UI.\n\nWhen building a UI, only a single service that acts like an API Gateway is required to understand and encode within the application.\n\nThe Backend for the Frontend model also allows engineers working on services to separate logic for the UI with the logic from an internal service. An internal service can rely on the Backend for the Frontend to handle things like Authentication, Authorization, Caching, and Rate-limits. These mechanisms may also be implemented as independent services, but the Backend for the Frontend can make multiple API calls per any API request from the UI.\n\n## Moving away from SPAs\n\nA newer development in the UI space is server side render the application. If we are already introducing additional complexity by having a Backend for the Frontend, then why not combine the UI code with the backend service and generate hydrated and cacheable pages via the server instead of using a SPA?\n\n```mermaid\ngraph LR\n  UIServer[\"UI Server\"] --> Service1\n  UIServer --> Service2\n UIServer --> Service3\n```\n\nWith this design, we increase the complexity - the backend not has to be able to render the UI code. It must now render, hydrate, make requests to downstream APIs, and send those server-side rendered pages to the browser.\n\nBut this additional complexity enabled us to:\n\n- create truly authenticated and authorized routes – the code for those routes isn’t even streamed to the browser, unlike a SPA.\n- near instant page loads – reducing the bundle size by just sending the JavaScript needed to run the page makes this possible. And since API calls are happening within the same network, this saves the user from additional loading spinners after the initial bundle has been loaded.\n\nThe general trend of having a Backend for the Frontend comes with additional complexity. It stems from the want to enhance the experience of the user, and probably from a subconscious desire to return to a simpler time:\n\n```mermaid\ngraph TD\n  Server\n```\n","contentHTML":"<p>The Backend for the Frontend (BEFFE) is typically stateless and acts as a proxy for other services, including authentication, authorization, and core services. The recent divorce of browser code being rendered by backend services was created by SPAs - Single Page Applications. In simpler architectures, a SPA and service could be as simple as:</p>\n<div class=\"py-8 [&amp;_svg]:m-auto\"><div class=\"mermaid\" data-mermaid-src=\"graph LR\n  SPA --&gt; Backend\">graph LR\n  SPA --&gt; Backend</div></div>\n<p>SPAs also became popular because the codebase could be built into static assets that could be services.</p>\n<p>However, as complexity arises with the above architecture and the SPA starts to rely on more services, a thin proxy is typically introduced, like NGINX:</p>\n<div class=\"py-8 [&amp;_svg]:m-auto\"><div class=\"mermaid\" data-mermaid-src=\"graph LR\n  SPA --&gt; NGINX\n  NGINX --&gt; Service1\n  NGINX --&gt; Service2\n  NGINX --&gt; Service3\">graph LR\n  SPA --&gt; NGINX\n  NGINX --&gt; Service1\n  NGINX --&gt; Service2\n  NGINX --&gt; Service3</div></div>\n<p>While proxies like NGINX can continue to be useful, using it as a proxy ends up putting a lot of routing and additional API handling logic on the client. The client now ends up also having to support, understand, and embed how to interact with the data rom all the downstream services. To simplify and create consistent contracts with the UI, we can create a proxy service: a Backend for the Frontend:</p>\n<div class=\"py-8 [&amp;_svg]:m-auto\"><div class=\"mermaid\" data-mermaid-src=\"graph LR\n  SPA --&gt; BEFFE\n  BEFFE --&gt; Service1\n  BEFFE --&gt; Service2\n  BEFFE --&gt; Service3\">graph LR\n  SPA --&gt; BEFFE\n  BEFFE --&gt; Service1\n  BEFFE --&gt; Service2\n  BEFFE --&gt; Service3</div></div>\n<p>The Backend for the Frontend ends up serving two purposes:</p>\n<ul>\n<li>It acts as a proxy for all UI requests</li>\n<li>It manages and massages the APIs of downstream services to be a consistent API for the UI.</li>\n</ul>\n<p>When building a UI, only a single service that acts like an API Gateway is required to understand and encode within the application.</p>\n<p>The Backend for the Frontend model also allows engineers working on services to separate logic for the UI with the logic from an internal service. An internal service can rely on the Backend for the Frontend to handle things like Authentication, Authorization, Caching, and Rate-limits. These mechanisms may also be implemented as independent services, but the Backend for the Frontend can make multiple API calls per any API request from the UI.</p>\n<h2>Moving away from SPAs</h2>\n<p>A newer development in the UI space is server side render the application. If we are already introducing additional complexity by having a Backend for the Frontend, then why not combine the UI code with the backend service and generate hydrated and cacheable pages via the server instead of using a SPA?</p>\n<div class=\"py-8 [&amp;_svg]:m-auto\"><div class=\"mermaid\" data-mermaid-src=\"graph LR\n  UIServer[&quot;UI Server&quot;] --&gt; Service1\n  UIServer --&gt; Service2\n UIServer --&gt; Service3\">graph LR\n  UIServer[&quot;UI Server&quot;] --&gt; Service1\n  UIServer --&gt; Service2\n UIServer --&gt; Service3</div></div>\n<p>With this design, we increase the complexity - the backend not has to be able to render the UI code. It must now render, hydrate, make requests to downstream APIs, and send those server-side rendered pages to the browser.</p>\n<p>But this additional complexity enabled us to:</p>\n<ul>\n<li>create truly authenticated and authorized routes – the code for those routes isn’t even streamed to the browser, unlike a SPA.</li>\n<li>near instant page loads – reducing the bundle size by just sending the JavaScript needed to run the page makes this possible. And since API calls are happening within the same network, this saves the user from additional loading spinners after the initial bundle has been loaded.</li>\n</ul>\n<p>The general trend of having a Backend for the Frontend comes with additional complexity. It stems from the want to enhance the experience of the user, and probably from a subconscious desire to return to a simpler time:</p>\n<div class=\"py-8 [&amp;_svg]:m-auto\"><div class=\"mermaid\" data-mermaid-src=\"graph TD\n  Server\">graph TD\n  Server</div></div>","contentCode":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    ul: \"ul\",\n    li: \"li\",\n    h2: \"h2\"\n  }, props.components), {Mermaid} = _components;\n  if (!Mermaid) _missingMdxReference(\"Mermaid\", true);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"The Backend for the Frontend (BEFFE) is typically stateless and acts as a proxy for other services, including authentication, authorization, and core services. The recent divorce of browser code being rendered by backend services was created by SPAs - Single Page Applications. In simpler architectures, a SPA and service could be as simple as:\"\n    }), \"\\n\", _jsx(Mermaid, {\n      chart: \"graph LR\\n  SPA --> Backend\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"SPAs also became popular because the codebase could be built into static assets that could be services.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"However, as complexity arises with the above architecture and the SPA starts to rely on more services, a thin proxy is typically introduced, like NGINX:\"\n    }), \"\\n\", _jsx(Mermaid, {\n      chart: \"graph LR\\n  SPA --> NGINX\\n  NGINX --> Service1\\n  NGINX --> Service2\\n  NGINX --> Service3\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"While proxies like NGINX can continue to be useful, using it as a proxy ends up putting a lot of routing and additional API handling logic on the client. The client now ends up also having to support, understand, and embed how to interact with the data rom all the downstream services. To simplify and create consistent contracts with the UI, we can create a proxy service: a Backend for the Frontend:\"\n    }), \"\\n\", _jsx(Mermaid, {\n      chart: \"graph LR\\n  SPA --> BEFFE\\n  BEFFE --> Service1\\n  BEFFE --> Service2\\n  BEFFE --> Service3\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The Backend for the Frontend ends up serving two purposes:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"It acts as a proxy for all UI requests\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"It manages and massages the APIs of downstream services to be a consistent API for the UI.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"When building a UI, only a single service that acts like an API Gateway is required to understand and encode within the application.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The Backend for the Frontend model also allows engineers working on services to separate logic for the UI with the logic from an internal service. An internal service can rely on the Backend for the Frontend to handle things like Authentication, Authorization, Caching, and Rate-limits. These mechanisms may also be implemented as independent services, but the Backend for the Frontend can make multiple API calls per any API request from the UI.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Moving away from SPAs\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"A newer development in the UI space is server side render the application. If we are already introducing additional complexity by having a Backend for the Frontend, then why not combine the UI code with the backend service and generate hydrated and cacheable pages via the server instead of using a SPA?\"\n    }), \"\\n\", _jsx(Mermaid, {\n      chart: \"graph LR\\n  UIServer[\\\"UI Server\\\"] --> Service1\\n  UIServer --> Service2\\n UIServer --> Service3\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"With this design, we increase the complexity - the backend not has to be able to render the UI code. It must now render, hydrate, make requests to downstream APIs, and send those server-side rendered pages to the browser.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"But this additional complexity enabled us to:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"create truly authenticated and authorized routes – the code for those routes isn’t even streamed to the browser, unlike a SPA.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"near instant page loads – reducing the bundle size by just sending the JavaScript needed to run the page makes this possible. And since API calls are happening within the same network, this saves the user from additional loading spinners after the initial bundle has been loaded.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The general trend of having a Backend for the Frontend comes with additional complexity. It stems from the want to enhance the experience of the user, and probably from a subconscious desire to return to a simpler time:\"\n    }), \"\\n\", _jsx(Mermaid, {\n      chart: \"graph TD\\n  Server\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\nfunction _missingMdxReference(id, component) {\n  throw new Error(\"Expected \" + (component ? \"component\" : \"object\") + \" `\" + id + \"` to be defined: you likely forgot to import, pass, or provide it.\");\n}\n","excerptRaw":"\nThe Backend for the Frontend (BEFFE) is typically stateless and acts as a proxy for other services, including authentication, authorization, and core services. The recent divorce of browser code being rendered by backend services was created by SPAs - Single Page Applications. In simpler architectures, a SPA and service could be as simple as:","excerptHTML":"<p>The Backend for the Frontend (BEFFE) is typically stateless and acts as a proxy for other services, including authentication, authorization, and core services. The recent divorce of browser code being rendered by backend services was created by SPAs - Single Page Applications. In simpler architectures, a SPA and service could be as simple as:</p>","excerptCode":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {jsx: _jsx} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\"\n  }, props.components);\n  return _jsx(_components.p, {\n    children: \"The Backend for the Frontend (BEFFE) is typically stateless and acts as a proxy for other services, including authentication, authorization, and core services. The recent divorce of browser code being rendered by backend services was created by SPAs - Single Page Applications. In simpler architectures, a SPA and service could be as simple as:\"\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n"},"next":{"slug":"2023-07-01-fast-embedding-lookingup","date":"2023-07-01","title":"Fast Similar Embedding Lookup","frontmatter":{"title":"Fast Similar Embedding Lookup"},"contentRaw":"\nWhile working on the Clarity Hub NLP API, we had a common use-case where we would create embeddings from text, and use those embedding to determine cosine similarity with other embeddings. Doing this required loading all of the embeddings in-memory and then computing cosine similarity with the entire dataset. As the dataset grew, this operation would get incredibly slow.\n\nWe worked on a fast way to do these lookups using ranges that can be performed in any database. This approach was never implemented, but we worked on multiple proof-of-concepts to test out our ideas. The goal was to take an input text, compute an embedding, load the entire embedding datasets loaded into an AWS lambda, find the most similar set of vectors, and return the top N similar vectors in one use-case. To tackle that, we came up with the following idea.\n\nGiven a vector A, compute is similar to a unit vector U of the same dimension as A. So:\n\n```cpp\ndim(U) = dim(A)\n```\n\nAnd\n\n```cpp\nS_u = cos(θ) = A · U / ||A|| x ||U||\n```\n\nWhere S_u is the similarity with the unit vector. The unit vector just needs to be the same across all samples.\n\nFor each embedding, store the calculated S_u.\n\nIf we want to find similar vectors for a new vector B, then we compute is similarity to the unit vector.\n\nThen, we can query the database for vectors within an interval of `[S_u - ε, S_u + ε]` . This will give us a subset of the dataset that have similar similarities with the unit vector.\n\nWe can re-query increasing or decreasing ε until the top N results are found.\n\nTo further improve accuracy, we can also re-compute the similarity score using cosine similarity with the subset of vectors, which is still much faster then computing the similarity against the entire dataset.\n\nThis approach begins to break down as the cosine similarity to the unit vector chosen gets very large (`> 0.4`).  We end up with the possibility of matching against vectors that are of opposite directions – the least similar vectors to the original input vector.\n\nOne solution to workaround this could be to pre-compute the similarity of a vector against unit vectors for each dimension of the input vector. But this could be 512 or more cosine similarity calculations for modern embeddings just to precompute the data. Once all unit vector similarities are calculated and stored, the range query against the database would be made against the column for which the input vector’s similarity is closest to 0.\n\nThere are a lot of real solutions to this problem, but this was a fun exercise to think about and work on.\n\n## Further reading\n\nVector similarity search is becoming increasingly popular and integrated into databases. Here are some resources to learn more: [Vector Similarity Search](https://zilliz.com/blog/vector-similarity-search).\n","contentHTML":"<p>While working on the Clarity Hub NLP API, we had a common use-case where we would create embeddings from text, and use those embedding to determine cosine similarity with other embeddings. Doing this required loading all of the embeddings in-memory and then computing cosine similarity with the entire dataset. As the dataset grew, this operation would get incredibly slow.</p>\n<p>We worked on a fast way to do these lookups using ranges that can be performed in any database. This approach was never implemented, but we worked on multiple proof-of-concepts to test out our ideas. The goal was to take an input text, compute an embedding, load the entire embedding datasets loaded into an AWS lambda, find the most similar set of vectors, and return the top N similar vectors in one use-case. To tackle that, we came up with the following idea.</p>\n<p>Given a vector A, compute is similar to a unit vector U of the same dimension as A. So:</p>\n<div class=\"overflow-auto rounded bg-gray-200 p-4 font-mono text-sm dark:bg-gray-800 dark:text-gray-100\"><pre><code class=\"language-cpp\">dim(U) = dim(A)\n</code></pre></div>\n<p>And</p>\n<div class=\"overflow-auto rounded bg-gray-200 p-4 font-mono text-sm dark:bg-gray-800 dark:text-gray-100\"><pre><code class=\"language-cpp\">S_u = cos(θ) = A · U / ||A|| x ||U||\n</code></pre></div>\n<p>Where S_u is the similarity with the unit vector. The unit vector just needs to be the same across all samples.</p>\n<p>For each embedding, store the calculated S_u.</p>\n<p>If we want to find similar vectors for a new vector B, then we compute is similarity to the unit vector.</p>\n<p>Then, we can query the database for vectors within an interval of <code>[S_u - ε, S_u + ε]</code> . This will give us a subset of the dataset that have similar similarities with the unit vector.</p>\n<p>We can re-query increasing or decreasing ε until the top N results are found.</p>\n<p>To further improve accuracy, we can also re-compute the similarity score using cosine similarity with the subset of vectors, which is still much faster then computing the similarity against the entire dataset.</p>\n<p>This approach begins to break down as the cosine similarity to the unit vector chosen gets very large (<code>&gt; 0.4</code>).  We end up with the possibility of matching against vectors that are of opposite directions – the least similar vectors to the original input vector.</p>\n<p>One solution to workaround this could be to pre-compute the similarity of a vector against unit vectors for each dimension of the input vector. But this could be 512 or more cosine similarity calculations for modern embeddings just to precompute the data. Once all unit vector similarities are calculated and stored, the range query against the database would be made against the column for which the input vector’s similarity is closest to 0.</p>\n<p>There are a lot of real solutions to this problem, but this was a fun exercise to think about and work on.</p>\n<h2>Further reading</h2>\n<p>Vector similarity search is becoming increasingly popular and integrated into databases. Here are some resources to learn more: <a href=\"https://zilliz.com/blog/vector-similarity-search\">Vector Similarity Search</a>.</p>","contentCode":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    pre: \"pre\",\n    code: \"code\",\n    h2: \"h2\",\n    a: \"a\"\n  }, props.components);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"While working on the Clarity Hub NLP API, we had a common use-case where we would create embeddings from text, and use those embedding to determine cosine similarity with other embeddings. Doing this required loading all of the embeddings in-memory and then computing cosine similarity with the entire dataset. As the dataset grew, this operation would get incredibly slow.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"We worked on a fast way to do these lookups using ranges that can be performed in any database. This approach was never implemented, but we worked on multiple proof-of-concepts to test out our ideas. The goal was to take an input text, compute an embedding, load the entire embedding datasets loaded into an AWS lambda, find the most similar set of vectors, and return the top N similar vectors in one use-case. To tackle that, we came up with the following idea.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Given a vector A, compute is similar to a unit vector U of the same dimension as A. So:\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-cpp\",\n        children: \"dim(U) = dim(A)\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"And\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-cpp\",\n        children: \"S_u = cos(θ) = A · U / ||A|| x ||U||\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Where S_u is the similarity with the unit vector. The unit vector just needs to be the same across all samples.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"For each embedding, store the calculated S_u.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"If we want to find similar vectors for a new vector B, then we compute is similarity to the unit vector.\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Then, we can query the database for vectors within an interval of \", _jsx(_components.code, {\n        children: \"[S_u - ε, S_u + ε]\"\n      }), \" . This will give us a subset of the dataset that have similar similarities with the unit vector.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"We can re-query increasing or decreasing ε until the top N results are found.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"To further improve accuracy, we can also re-compute the similarity score using cosine similarity with the subset of vectors, which is still much faster then computing the similarity against the entire dataset.\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"This approach begins to break down as the cosine similarity to the unit vector chosen gets very large (\", _jsx(_components.code, {\n        children: \"> 0.4\"\n      }), \").  We end up with the possibility of matching against vectors that are of opposite directions – the least similar vectors to the original input vector.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"One solution to workaround this could be to pre-compute the similarity of a vector against unit vectors for each dimension of the input vector. But this could be 512 or more cosine similarity calculations for modern embeddings just to precompute the data. Once all unit vector similarities are calculated and stored, the range query against the database would be made against the column for which the input vector’s similarity is closest to 0.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"There are a lot of real solutions to this problem, but this was a fun exercise to think about and work on.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Further reading\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Vector similarity search is becoming increasingly popular and integrated into databases. Here are some resources to learn more: \", _jsx(_components.a, {\n        href: \"https://zilliz.com/blog/vector-similarity-search\",\n        children: \"Vector Similarity Search\"\n      }), \".\"]\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","excerptRaw":"\nWhile working on the Clarity Hub NLP API, we had a common use-case where we would create embeddings from text, and use those embedding to determine cosine similarity with other embeddings. Doing this required loading all of the embeddings in-memory and then computing cosine similarity with the entire dataset. As the dataset grew, this operation would get incredibly slow.","excerptHTML":"<p>While working on the Clarity Hub NLP API, we had a common use-case where we would create embeddings from text, and use those embedding to determine cosine similarity with other embeddings. Doing this required loading all of the embeddings in-memory and then computing cosine similarity with the entire dataset. As the dataset grew, this operation would get incredibly slow.</p>","excerptCode":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {jsx: _jsx} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\"\n  }, props.components);\n  return _jsx(_components.p, {\n    children: \"While working on the Clarity Hub NLP API, we had a common use-case where we would create embeddings from text, and use those embedding to determine cosine similarity with other embeddings. Doing this required loading all of the embeddings in-memory and then computing cosine similarity with the entire dataset. As the dataset grew, this operation would get incredibly slow.\"\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n"}},"__N_SSG":true}