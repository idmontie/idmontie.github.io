{"pageProps":{"headTitle":"ai posts - idmontie's Portfolio","headKeywords":"ai","tag":"ai","posts":[{"slug":"2025-04-21-agents-agents-agents","date":"2025-04-21","title":"Agents, Agents, Agents","frontmatter":{"title":"Agents, Agents, Agents","tags":["ai","apis"]},"contentRaw":"\nIn 2008, Steve Ballmer once famously said “[Developers, developers, developers](https://www.youtube.com/watch?v=Vhh_GeBPOhs)” in a highly meme-able video. He was trying to emphasize the importance of software developers in business and that their importance was only going to continue to grow. Microsoft shifted to try to support software developers, noticing that if they supported developers, they would build better software for Windows, which in turn would make people want to use Windows.\n\nThe 2010s saw companies like “Twilio” have guiding questions like Jeff Lawson’s “Why Isn’t That and API?”. This guiding principle was based around the idea that if you build great APIs for software developers, they would want to use them, and in doing so, spend money on those services. For that entire decade, companies would start, build for engineers, and use the fly-wheel effect to become multi-billion dollar companies.\n\n![Why isn't that an API?](/media/2025-04-21-agents-agents-agents/why-isnt-that-an-api.png)\n\nNow, in the age of LLMs (Large Language Models) and AI, my assumption about the future is that we are going to find that “AI Agents” are the next Big Thing. We will probably see software engineers coding not to make APIs that other developers can use, but so that AI Agents can use. With [OpenAI introducing Operator](https://openai.com/index/introducing-operator/), their AI that can do tasks for you autonomously, the trajectory looks clear for companies that want to partner in this space: make products that will interface with AI Agents*.\n\nAnd this isn’t going to only apply to software engineering. Even those in marketing are starting to think about how their content can be better consumed by the crawlers that feed into the training data for these LLMs. In “[AI Slop Suspicion and Writing Back](https://benjamincongdon.me/blog/2025/01/25/AI-Slop-Suspicion-and-Writing-Back/)”, the author hypothesizes about the future of SEO. Will we be writing not for other people to reach our content via search, but to see our content as part of an LLM’s answer to a question? If people are starting to transition to asking LLMs like ChatGPT a question first before they even Google, then wouldn’t getting your name and your thoughts ingested by the massive corpus of training data be beneficial in a sense, similar to a web-crawler indexing for search?\n\nThese are questions I think we will see answered very soon, and some of the answers may not be ones we like. We may very well be starting to write content, apps, and APIs not for other people, but for machines.\n\n*Of course, not everyone is going to jump on this bandwagon. There are companies right now that make concerted efforts to make sure only humans use their site. They don’t have APIs, they stick their entire site behind a Cloudflare captcha check. These aren’t the companies I’m talking about as they weren’t trying to bridge into the larger software developer ecosystem.\n","contentHTML":"<p>In 2008, Steve Ballmer once famously said “<a href=\"https://www.youtube.com/watch?v=Vhh_GeBPOhs\">Developers, developers, developers</a>” in a highly meme-able video. He was trying to emphasize the importance of software developers in business and that their importance was only going to continue to grow. Microsoft shifted to try to support software developers, noticing that if they supported developers, they would build better software for Windows, which in turn would make people want to use Windows.</p>\n<p>The 2010s saw companies like “Twilio” have guiding questions like Jeff Lawson’s “Why Isn’t That and API?”. This guiding principle was based around the idea that if you build great APIs for software developers, they would want to use them, and in doing so, spend money on those services. For that entire decade, companies would start, build for engineers, and use the fly-wheel effect to become multi-billion dollar companies.</p>\n<p><img alt=\"Why isn&#x27;t that an API?\" src=\"/media/2025-04-21-agents-agents-agents/why-isnt-that-an-api.png\" style=\"max-height:500px;margin:auto;text-align:center\"/></p>\n<p>Now, in the age of LLMs (Large Language Models) and AI, my assumption about the future is that we are going to find that “AI Agents” are the next Big Thing. We will probably see software engineers coding not to make APIs that other developers can use, but so that AI Agents can use. With <a href=\"https://openai.com/index/introducing-operator/\">OpenAI introducing Operator</a>, their AI that can do tasks for you autonomously, the trajectory looks clear for companies that want to partner in this space: make products that will interface with AI Agents*.</p>\n<p>And this isn’t going to only apply to software engineering. Even those in marketing are starting to think about how their content can be better consumed by the crawlers that feed into the training data for these LLMs. In “<a href=\"https://benjamincongdon.me/blog/2025/01/25/AI-Slop-Suspicion-and-Writing-Back/\">AI Slop Suspicion and Writing Back</a>”, the author hypothesizes about the future of SEO. Will we be writing not for other people to reach our content via search, but to see our content as part of an LLM’s answer to a question? If people are starting to transition to asking LLMs like ChatGPT a question first before they even Google, then wouldn’t getting your name and your thoughts ingested by the massive corpus of training data be beneficial in a sense, similar to a web-crawler indexing for search?</p>\n<p>These are questions I think we will see answered very soon, and some of the answers may not be ones we like. We may very well be starting to write content, apps, and APIs not for other people, but for machines.</p>\n<p>*Of course, not everyone is going to jump on this bandwagon. There are companies right now that make concerted efforts to make sure only humans use their site. They don’t have APIs, they stick their entire site behind a Cloudflare captcha check. These aren’t the companies I’m talking about as they weren’t trying to bridge into the larger software developer ecosystem.</p>","contentCode":"\"use strict\";\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = {\n    a: \"a\",\n    img: \"img\",\n    p: \"p\",\n    ...props.components\n  };\n  return _jsxs(_Fragment, {\n    children: [_jsxs(_components.p, {\n      children: [\"In 2008, Steve Ballmer once famously said “\", _jsx(_components.a, {\n        href: \"https://www.youtube.com/watch?v=Vhh_GeBPOhs\",\n        children: \"Developers, developers, developers\"\n      }), \"” in a highly meme-able video. He was trying to emphasize the importance of software developers in business and that their importance was only going to continue to grow. Microsoft shifted to try to support software developers, noticing that if they supported developers, they would build better software for Windows, which in turn would make people want to use Windows.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The 2010s saw companies like “Twilio” have guiding questions like Jeff Lawson’s “Why Isn’t That and API?”. This guiding principle was based around the idea that if you build great APIs for software developers, they would want to use them, and in doing so, spend money on those services. For that entire decade, companies would start, build for engineers, and use the fly-wheel effect to become multi-billion dollar companies.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/media/2025-04-21-agents-agents-agents/why-isnt-that-an-api.png\",\n        alt: \"Why isn't that an API?\"\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Now, in the age of LLMs (Large Language Models) and AI, my assumption about the future is that we are going to find that “AI Agents” are the next Big Thing. We will probably see software engineers coding not to make APIs that other developers can use, but so that AI Agents can use. With \", _jsx(_components.a, {\n        href: \"https://openai.com/index/introducing-operator/\",\n        children: \"OpenAI introducing Operator\"\n      }), \", their AI that can do tasks for you autonomously, the trajectory looks clear for companies that want to partner in this space: make products that will interface with AI Agents*.\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"And this isn’t going to only apply to software engineering. Even those in marketing are starting to think about how their content can be better consumed by the crawlers that feed into the training data for these LLMs. In “\", _jsx(_components.a, {\n        href: \"https://benjamincongdon.me/blog/2025/01/25/AI-Slop-Suspicion-and-Writing-Back/\",\n        children: \"AI Slop Suspicion and Writing Back\"\n      }), \"”, the author hypothesizes about the future of SEO. Will we be writing not for other people to reach our content via search, but to see our content as part of an LLM’s answer to a question? If people are starting to transition to asking LLMs like ChatGPT a question first before they even Google, then wouldn’t getting your name and your thoughts ingested by the massive corpus of training data be beneficial in a sense, similar to a web-crawler indexing for search?\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"These are questions I think we will see answered very soon, and some of the answers may not be ones we like. We may very well be starting to write content, apps, and APIs not for other people, but for machines.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"*Of course, not everyone is going to jump on this bandwagon. There are companies right now that make concerted efforts to make sure only humans use their site. They don’t have APIs, they stick their entire site behind a Cloudflare captcha check. These aren’t the companies I’m talking about as they weren’t trying to bridge into the larger software developer ecosystem.\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, {\n    ...props,\n    children: _jsx(_createMdxContent, {\n      ...props\n    })\n  }) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","excerptRaw":"\nIn 2008, Steve Ballmer once famously said “[Developers, developers, developers](https://www.youtube.com/watch?v=Vhh_GeBPOhs)” in a highly meme-able video. He was trying to emphasize the importance of software developers in business and that their importance was only going to continue to grow. Microsoft shifted to try to support software developers, noticing that if they supported developers, they would build better software for Windows, which in turn would make people want to use Windows.","excerptHTML":"<p>In 2008, Steve Ballmer once famously said “<a href=\"https://www.youtube.com/watch?v=Vhh_GeBPOhs\">Developers, developers, developers</a>” in a highly meme-able video. He was trying to emphasize the importance of software developers in business and that their importance was only going to continue to grow. Microsoft shifted to try to support software developers, noticing that if they supported developers, they would build better software for Windows, which in turn would make people want to use Windows.</p>","excerptCode":"\"use strict\";\nconst {jsx: _jsx, jsxs: _jsxs} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = {\n    a: \"a\",\n    p: \"p\",\n    ...props.components\n  };\n  return _jsxs(_components.p, {\n    children: [\"In 2008, Steve Ballmer once famously said “\", _jsx(_components.a, {\n      href: \"https://www.youtube.com/watch?v=Vhh_GeBPOhs\",\n      children: \"Developers, developers, developers\"\n    }), \"” in a highly meme-able video. He was trying to emphasize the importance of software developers in business and that their importance was only going to continue to grow. Microsoft shifted to try to support software developers, noticing that if they supported developers, they would build better software for Windows, which in turn would make people want to use Windows.\"]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, {\n    ...props,\n    children: _jsx(_createMdxContent, {\n      ...props\n    })\n  }) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","tags":["ai","apis"]},{"slug":"2023-07-03-llm-loops","date":"2023-07-03","title":"AI Feedback Systems","frontmatter":{"title":"AI Feedback Systems","tags":["ai","llm"]},"contentRaw":"\nWe are starting to see a rise of novel use-cases for AI in products and games using LLMs. Rather than the simple chatbot like experiences we have seen in the past using AI, we are starting to see feedback systems being added to these experiences, providing additional context to the LLM than just the past conversation.\n\nA typical game loop for this type of system would look like:\n\n```mermaid\ngraph LR\n  Rules --> LLM\n  InputStates[\"Input States\"] --> LLM\n  LLM --> OutputState\n  OutputState[\"Output State\"] --> GameEngine\n  GameEngine[\"Game Engine\"] --> InputStates\n```\n\nRules can be describes as written-word description, with an additional set of rules telling the LLM to reply using JSON output of a given schema. In this area, I have had success giving LLMs descriptions of output schemas in Typescript and asking for a JSON response that adheres to the type. Other methods of getting a consistent schema are more than likely possible here, as well as additional output methods.\n\nWhen the asynchronous task of creating and output state is complete, the Game Engine in this case can read, parse, and apply that new state to the world. Any additional interaction would then lead to the next set of input states that can be given to the LLM as a JSON blob.\n\nFor a more concrete example, we can imagine a game where we want our player to interact with a set of agents. The input states would be the state of each agent, the user’s interaction, and maybe some global environment data. The rules may be how each agent should behave, the rules of the game, and additional context. The LLM would take these inputs, and the output is instructed to be the next state of each agent. When the LLM returns this data, the Game Engine read it and applies it to the game’s representation of each agent, showing the player the impact of their actions.\n\nI’m looking forward to more novel use-cases for LLMs!\n","contentHTML":"<p>We are starting to see a rise of novel use-cases for AI in products and games using LLMs. Rather than the simple chatbot like experiences we have seen in the past using AI, we are starting to see feedback systems being added to these experiences, providing additional context to the LLM than just the past conversation.</p>\n<p>A typical game loop for this type of system would look like:</p>\n<mermaid chart=\"graph LR\n  Rules --&gt; LLM\n  InputStates[&quot;Input States&quot;] --&gt; LLM\n  LLM --&gt; OutputState\n  OutputState[&quot;Output State&quot;] --&gt; GameEngine\n  GameEngine[&quot;Game Engine&quot;] --&gt; InputStates\"></mermaid>\n<p>Rules can be describes as written-word description, with an additional set of rules telling the LLM to reply using JSON output of a given schema. In this area, I have had success giving LLMs descriptions of output schemas in Typescript and asking for a JSON response that adheres to the type. Other methods of getting a consistent schema are more than likely possible here, as well as additional output methods.</p>\n<p>When the asynchronous task of creating and output state is complete, the Game Engine in this case can read, parse, and apply that new state to the world. Any additional interaction would then lead to the next set of input states that can be given to the LLM as a JSON blob.</p>\n<p>For a more concrete example, we can imagine a game where we want our player to interact with a set of agents. The input states would be the state of each agent, the user’s interaction, and maybe some global environment data. The rules may be how each agent should behave, the rules of the game, and additional context. The LLM would take these inputs, and the output is instructed to be the next state of each agent. When the LLM returns this data, the Game Engine read it and applies it to the game’s representation of each agent, showing the player the impact of their actions.</p>\n<p>I’m looking forward to more novel use-cases for LLMs!</p>","contentCode":"\"use strict\";\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = {\n    mermaid: \"mermaid\",\n    p: \"p\",\n    ...props.components\n  };\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"We are starting to see a rise of novel use-cases for AI in products and games using LLMs. Rather than the simple chatbot like experiences we have seen in the past using AI, we are starting to see feedback systems being added to these experiences, providing additional context to the LLM than just the past conversation.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"A typical game loop for this type of system would look like:\"\n    }), \"\\n\", _jsx(_components.mermaid, {\n      chart: \"graph LR\\n  Rules --> LLM\\n  InputStates[\\\"Input States\\\"] --> LLM\\n  LLM --> OutputState\\n  OutputState[\\\"Output State\\\"] --> GameEngine\\n  GameEngine[\\\"Game Engine\\\"] --> InputStates\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Rules can be describes as written-word description, with an additional set of rules telling the LLM to reply using JSON output of a given schema. In this area, I have had success giving LLMs descriptions of output schemas in Typescript and asking for a JSON response that adheres to the type. Other methods of getting a consistent schema are more than likely possible here, as well as additional output methods.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"When the asynchronous task of creating and output state is complete, the Game Engine in this case can read, parse, and apply that new state to the world. Any additional interaction would then lead to the next set of input states that can be given to the LLM as a JSON blob.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"For a more concrete example, we can imagine a game where we want our player to interact with a set of agents. The input states would be the state of each agent, the user’s interaction, and maybe some global environment data. The rules may be how each agent should behave, the rules of the game, and additional context. The LLM would take these inputs, and the output is instructed to be the next state of each agent. When the LLM returns this data, the Game Engine read it and applies it to the game’s representation of each agent, showing the player the impact of their actions.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"I’m looking forward to more novel use-cases for LLMs!\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, {\n    ...props,\n    children: _jsx(_createMdxContent, {\n      ...props\n    })\n  }) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","excerptRaw":"\nWe are starting to see a rise of novel use-cases for AI in products and games using LLMs. Rather than the simple chatbot like experiences we have seen in the past using AI, we are starting to see feedback systems being added to these experiences, providing additional context to the LLM than just the past conversation.","excerptHTML":"<p>We are starting to see a rise of novel use-cases for AI in products and games using LLMs. Rather than the simple chatbot like experiences we have seen in the past using AI, we are starting to see feedback systems being added to these experiences, providing additional context to the LLM than just the past conversation.</p>","excerptCode":"\"use strict\";\nconst {jsx: _jsx} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = {\n    p: \"p\",\n    ...props.components\n  };\n  return _jsx(_components.p, {\n    children: \"We are starting to see a rise of novel use-cases for AI in products and games using LLMs. Rather than the simple chatbot like experiences we have seen in the past using AI, we are starting to see feedback systems being added to these experiences, providing additional context to the LLM than just the past conversation.\"\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, {\n    ...props,\n    children: _jsx(_createMdxContent, {\n      ...props\n    })\n  }) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","tags":["ai","llm"]}]},"__N_SSG":true}