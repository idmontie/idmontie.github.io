{"pageProps":{"headTitle":"ChatGPT, Coding, and Language - idmontie's Portfolio","headKeywords":"chatgpt","post":{"slug":"2023-07-23-chatgpt-coding-loop","date":"2023-07-23","title":"ChatGPT, Coding, and Language","frontmatter":{"title":"ChatGPT, Coding, and Language","tags":["chatgpt"]},"contentRaw":"\nI’ve been experimenting with ChatGPT, just like everyone else. But why has it become so popular? It truly is a revolutionary piece of technology. Is it The Next Big Thing? Will it really replace all of us?\n\nMy day to day is architecture and programming, and I’ve heard all sorts of things on Twitter:\n\n- That ChatGPT can build entire iOS applications\n- ChatGPT can write whole files of code\n- ChatGPT can find bugs in code when writing tests\n\nWhen I started by own company Clarity Hub, we had a focus on machine learning to help augment human intelligence. My thought at the time was that we could leverage machine learning to augment and supplement human intelligence. The focus wouldn't be to replace any person's job, but to make it streamlined and easier to accomplish that job. Our journey started with us serving customer success agents with suggested replies and contextual information, but we eventually pivoted to a Dovetail-like application to help product teams gather, annotate, and contextualize customer interviews.\n\nWe found that augmenting activity with machine learning was not only easier to do from a technology point of view, but empowered users, rather than make them feel like their job was being replaced.\n\nEven with the advent of ChatGPT, I still see this being the short term future of it's use.\n\n## False Starts\n\nI was curious how much ChatGPT could really do given all of its hype  \n\nI asked it to build an Asteroids game using PhaserJS. Don’t worry, this isn’t another article on “Look, I got ChatGPT to build a game!”\n\nI originally just asked ChatGPT to build an Asteroids game using PhaserJS. A very vague prompt, yes, but a great starting point to figure out the limitations of ChatGPT.\n\nThe response was basically, “No I can’t”:\n\n> As an AI language model, I'm not able to write and execute code in real-time. However, I can provide you with a detailed framework to get started with creating an Asteroids game in PhaserJS.\n\nThis was followed by instructions on how I could do it myself.\n\nThe next steps of course are to be more specific, asking ChatGPT to build the application to build specific functionality:\n\n> Add the code to let the player shoot asteroids. If the player gets hit by 3 asteroids, its game over\n\nMost interactions with ChatGPT seem to go like this:\n\n![Flowchart workflow](/media/2023-07-23-chatgpt-coding-loop/Flowchart_Template_1.jpg)\n\nThis flow eventually leads to either:\n\n- Not getting what you want.\n- Getting what you want across many different responses, and then you must manually coalesce all of the different responses together to form the result you want.\n\nOnce I like the set of responses that ChatGPT has given me, I’ll ask ChatGPT to summarize for me so that I can improve the prompt further:\n\n> Take everything we just talked about and give me a prompt to ask you in the future that encapsulates all of the requirements.\n\n## A More Natural Flow\n\nI’ve talked with ChatGPT on a variety of topics, but the majority of questions relate to coding. I actively use Github Copilot, which utilizes the ChatGPT engine in the backend. I’ve built [Sora by Capsule Cat](https://marketplace.visualstudio.com/items?itemName=CapsuleCat.sora-by-capsule-cat) to have ChatGPT generate entire coding files for me as well with project context. Even then, I still use the ChatGPT UI as well.\n\n![Flowchart line](/media/2023-07-23-chatgpt-coding-loop/Flowchart_Template_2.jpg)\n\nI find that the above flow leaves me less fatigued, and more productive - at least in the context of side-projects that I’ve used ChatGPT. Research shows that when we read code and attempt logic puzzles, we use a completely different part of our brain than when we use language [[link](https://hub.jhu.edu/2020/12/17/brain-activity-while-reading-code/)]. In the above chart, you can imagine that when we write prompts, we utilize some logical reasoning, but we lean heavily on language as we talk with ChatGPT. Then when we get a response, a completely different part of the brain kicks in to read and write code.\n\n## The Hard Part About Coding\n\nEventually this leads to “the hard part about coding” which anecdotally everyone will tell you that surprise, it isn’t the coding part. What is nice about ChatGPT though is that even though the hard part isn’t writing code, it does take time, thought, and energy.\n\nIf the hard part of coding isn’t coding, then what is it? The non-exhaustive list is that it’s teamwork, communication, debugging, and requirements gathering. Oversimplifying, it’s the writing prompts part of the chart.\n","contentHTML":"<p>I’ve been experimenting with ChatGPT, just like everyone else. But why has it become so popular? It truly is a revolutionary piece of technology. Is it The Next Big Thing? Will it really replace all of us?</p>\n<p>My day to day is architecture and programming, and I’ve heard all sorts of things on Twitter:</p>\n<ul>\n<li>That ChatGPT can build entire iOS applications</li>\n<li>ChatGPT can write whole files of code</li>\n<li>ChatGPT can find bugs in code when writing tests</li>\n</ul>\n<p>When I started by own company Clarity Hub, we had a focus on machine learning to help augment human intelligence. My thought at the time was that we could leverage machine learning to augment and supplement human intelligence. The focus wouldn&#x27;t be to replace any person&#x27;s job, but to make it streamlined and easier to accomplish that job. Our journey started with us serving customer success agents with suggested replies and contextual information, but we eventually pivoted to a Dovetail-like application to help product teams gather, annotate, and contextualize customer interviews.</p>\n<p>We found that augmenting activity with machine learning was not only easier to do from a technology point of view, but empowered users, rather than make them feel like their job was being replaced.</p>\n<p>Even with the advent of ChatGPT, I still see this being the short term future of it&#x27;s use.</p>\n<h2>False Starts</h2>\n<p>I was curious how much ChatGPT could really do given all of its hype</p>\n<p>I asked it to build an Asteroids game using PhaserJS. Don’t worry, this isn’t another article on “Look, I got ChatGPT to build a game!”</p>\n<p>I originally just asked ChatGPT to build an Asteroids game using PhaserJS. A very vague prompt, yes, but a great starting point to figure out the limitations of ChatGPT.</p>\n<p>The response was basically, “No I can’t”:</p>\n<blockquote class=\"border-l-4 border-gray-300 pl-4\">\n<p>As an AI language model, I&#x27;m not able to write and execute code in real-time. However, I can provide you with a detailed framework to get started with creating an Asteroids game in PhaserJS.</p>\n</blockquote>\n<p>This was followed by instructions on how I could do it myself.</p>\n<p>The next steps of course are to be more specific, asking ChatGPT to build the application to build specific functionality:</p>\n<blockquote class=\"border-l-4 border-gray-300 pl-4\">\n<p>Add the code to let the player shoot asteroids. If the player gets hit by 3 asteroids, its game over</p>\n</blockquote>\n<p>Most interactions with ChatGPT seem to go like this:</p>\n<p><img alt=\"Flowchart workflow\" src=\"/media/2023-07-23-chatgpt-coding-loop/Flowchart_Template_1.jpg\" style=\"max-height:500px;margin:auto;text-align:center\"/></p>\n<p>This flow eventually leads to either:</p>\n<ul>\n<li>Not getting what you want.</li>\n<li>Getting what you want across many different responses, and then you must manually coalesce all of the different responses together to form the result you want.</li>\n</ul>\n<p>Once I like the set of responses that ChatGPT has given me, I’ll ask ChatGPT to summarize for me so that I can improve the prompt further:</p>\n<blockquote class=\"border-l-4 border-gray-300 pl-4\">\n<p>Take everything we just talked about and give me a prompt to ask you in the future that encapsulates all of the requirements.</p>\n</blockquote>\n<h2>A More Natural Flow</h2>\n<p>I’ve talked with ChatGPT on a variety of topics, but the majority of questions relate to coding. I actively use Github Copilot, which utilizes the ChatGPT engine in the backend. I’ve built <a href=\"https://marketplace.visualstudio.com/items?itemName=CapsuleCat.sora-by-capsule-cat\">Sora by Capsule Cat</a> to have ChatGPT generate entire coding files for me as well with project context. Even then, I still use the ChatGPT UI as well.</p>\n<p><img alt=\"Flowchart line\" src=\"/media/2023-07-23-chatgpt-coding-loop/Flowchart_Template_2.jpg\" style=\"max-height:500px;margin:auto;text-align:center\"/></p>\n<p>I find that the above flow leaves me less fatigued, and more productive - at least in the context of side-projects that I’ve used ChatGPT. Research shows that when we read code and attempt logic puzzles, we use a completely different part of our brain than when we use language [<a href=\"https://hub.jhu.edu/2020/12/17/brain-activity-while-reading-code/\">link</a>]. In the above chart, you can imagine that when we write prompts, we utilize some logical reasoning, but we lean heavily on language as we talk with ChatGPT. Then when we get a response, a completely different part of the brain kicks in to read and write code.</p>\n<h2>The Hard Part About Coding</h2>\n<p>Eventually this leads to “the hard part about coding” which anecdotally everyone will tell you that surprise, it isn’t the coding part. What is nice about ChatGPT though is that even though the hard part isn’t writing code, it does take time, thought, and energy.</p>\n<p>If the hard part of coding isn’t coding, then what is it? The non-exhaustive list is that it’s teamwork, communication, debugging, and requirements gathering. Oversimplifying, it’s the writing prompts part of the chart.</p>","contentCode":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    ul: \"ul\",\n    li: \"li\",\n    h2: \"h2\",\n    blockquote: \"blockquote\",\n    img: \"img\",\n    a: \"a\"\n  }, props.components);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"I’ve been experimenting with ChatGPT, just like everyone else. But why has it become so popular? It truly is a revolutionary piece of technology. Is it The Next Big Thing? Will it really replace all of us?\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"My day to day is architecture and programming, and I’ve heard all sorts of things on Twitter:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"That ChatGPT can build entire iOS applications\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"ChatGPT can write whole files of code\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"ChatGPT can find bugs in code when writing tests\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"When I started by own company Clarity Hub, we had a focus on machine learning to help augment human intelligence. My thought at the time was that we could leverage machine learning to augment and supplement human intelligence. The focus wouldn't be to replace any person's job, but to make it streamlined and easier to accomplish that job. Our journey started with us serving customer success agents with suggested replies and contextual information, but we eventually pivoted to a Dovetail-like application to help product teams gather, annotate, and contextualize customer interviews.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"We found that augmenting activity with machine learning was not only easier to do from a technology point of view, but empowered users, rather than make them feel like their job was being replaced.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Even with the advent of ChatGPT, I still see this being the short term future of it's use.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"False Starts\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"I was curious how much ChatGPT could really do given all of its hype\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"I asked it to build an Asteroids game using PhaserJS. Don’t worry, this isn’t another article on “Look, I got ChatGPT to build a game!”\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"I originally just asked ChatGPT to build an Asteroids game using PhaserJS. A very vague prompt, yes, but a great starting point to figure out the limitations of ChatGPT.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The response was basically, “No I can’t”:\"\n    }), \"\\n\", _jsxs(_components.blockquote, {\n      children: [\"\\n\", _jsx(_components.p, {\n        children: \"As an AI language model, I'm not able to write and execute code in real-time. However, I can provide you with a detailed framework to get started with creating an Asteroids game in PhaserJS.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"This was followed by instructions on how I could do it myself.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The next steps of course are to be more specific, asking ChatGPT to build the application to build specific functionality:\"\n    }), \"\\n\", _jsxs(_components.blockquote, {\n      children: [\"\\n\", _jsx(_components.p, {\n        children: \"Add the code to let the player shoot asteroids. If the player gets hit by 3 asteroids, its game over\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Most interactions with ChatGPT seem to go like this:\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/media/2023-07-23-chatgpt-coding-loop/Flowchart_Template_1.jpg\",\n        alt: \"Flowchart workflow\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"This flow eventually leads to either:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"Not getting what you want.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Getting what you want across many different responses, and then you must manually coalesce all of the different responses together to form the result you want.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Once I like the set of responses that ChatGPT has given me, I’ll ask ChatGPT to summarize for me so that I can improve the prompt further:\"\n    }), \"\\n\", _jsxs(_components.blockquote, {\n      children: [\"\\n\", _jsx(_components.p, {\n        children: \"Take everything we just talked about and give me a prompt to ask you in the future that encapsulates all of the requirements.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"A More Natural Flow\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"I’ve talked with ChatGPT on a variety of topics, but the majority of questions relate to coding. I actively use Github Copilot, which utilizes the ChatGPT engine in the backend. I’ve built \", _jsx(_components.a, {\n        href: \"https://marketplace.visualstudio.com/items?itemName=CapsuleCat.sora-by-capsule-cat\",\n        children: \"Sora by Capsule Cat\"\n      }), \" to have ChatGPT generate entire coding files for me as well with project context. Even then, I still use the ChatGPT UI as well.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/media/2023-07-23-chatgpt-coding-loop/Flowchart_Template_2.jpg\",\n        alt: \"Flowchart line\"\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"I find that the above flow leaves me less fatigued, and more productive - at least in the context of side-projects that I’ve used ChatGPT. Research shows that when we read code and attempt logic puzzles, we use a completely different part of our brain than when we use language [\", _jsx(_components.a, {\n        href: \"https://hub.jhu.edu/2020/12/17/brain-activity-while-reading-code/\",\n        children: \"link\"\n      }), \"]. In the above chart, you can imagine that when we write prompts, we utilize some logical reasoning, but we lean heavily on language as we talk with ChatGPT. Then when we get a response, a completely different part of the brain kicks in to read and write code.\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"The Hard Part About Coding\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Eventually this leads to “the hard part about coding” which anecdotally everyone will tell you that surprise, it isn’t the coding part. What is nice about ChatGPT though is that even though the hard part isn’t writing code, it does take time, thought, and energy.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"If the hard part of coding isn’t coding, then what is it? The non-exhaustive list is that it’s teamwork, communication, debugging, and requirements gathering. Oversimplifying, it’s the writing prompts part of the chart.\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","excerptRaw":"\nI’ve been experimenting with ChatGPT, just like everyone else. But why has it become so popular? It truly is a revolutionary piece of technology. Is it The Next Big Thing? Will it really replace all of us?","excerptHTML":"<p>I’ve been experimenting with ChatGPT, just like everyone else. But why has it become so popular? It truly is a revolutionary piece of technology. Is it The Next Big Thing? Will it really replace all of us?</p>","excerptCode":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {jsx: _jsx} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\"\n  }, props.components);\n  return _jsx(_components.p, {\n    children: \"I’ve been experimenting with ChatGPT, just like everyone else. But why has it become so popular? It truly is a revolutionary piece of technology. Is it The Next Big Thing? Will it really replace all of us?\"\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","tags":["chatgpt"]},"previous":{"slug":"2023-07-24-tsc-debugging","date":"2023-07-24","title":"Debugging slow tsc","frontmatter":{"title":"Debugging slow tsc","tags":["typescript"]},"contentRaw":"\nI was working on a project and I noticed the `tsc` check that we ran on pre-commit hooks had become dramatically slower. I was seeing near instant times to type check the entire project go to agonizingly slow minutes to check the project. I was able to narrow the range of commits to some change we made within a month window. I’m sure I could have narrowed it down by continuing to git bisect and re-running `time npx tsc`, but it was much easier to just check out main and try out the following:\n\n```bash\n# remove any pre-built definitions to get a good baseline\nrm -r .tsBuildInfo\n\n# Get some baseline stats\ntime npx tsc\n```\n\nI was seeing values around (times have been modified for blogging purposes):\n\n```bash\nreal 1m30.000s\nuser 2m0.000s\nsys 0m3.000s\n```\n\nAfter running the baseline trace, I could analyze the `tsc` trace and use a nifty tool called `analyze-trace` that will report hotspots:\n\n```bash\nnpm i -g @typescript/analyze-trace\n\nnpx tsc --generateTrace ./.trace\nnpx analyze-trace ./.trace\n```\n\nThis command will spit out which files are hotspots in your codebase. To my surprise, I was seeing build outputs from webpack as a hotspot.\n\nTurns out the `tsconfig.json` had been slightly modified and accidentally included the build folder. Adding the build folder to the excludes entry sped up the tsc command by quite a bit:\n\n```bash\nreal 0m30.000s\nuser 0m47.000s\nsys 0m2.000s\n```\n\nNot only did the trace help me identify the major hotspot in the code, but also\nindicated some additional areas that I could improve upon going forward.\n","contentHTML":"<p>I was working on a project and I noticed the <code>tsc</code> check that we ran on pre-commit hooks had become dramatically slower. I was seeing near instant times to type check the entire project go to agonizingly slow minutes to check the project. I was able to narrow the range of commits to some change we made within a month window. I’m sure I could have narrowed it down by continuing to git bisect and re-running <code>time npx tsc</code>, but it was much easier to just check out main and try out the following:</p>\n<div class=\"overflow-auto rounded bg-gray-200 p-4 font-mono text-sm dark:bg-gray-800 dark:text-gray-100\"><pre><code class=\"language-bash\"># remove any pre-built definitions to get a good baseline\nrm -r .tsBuildInfo\n\n# Get some baseline stats\ntime npx tsc\n</code></pre></div>\n<p>I was seeing values around (times have been modified for blogging purposes):</p>\n<div class=\"overflow-auto rounded bg-gray-200 p-4 font-mono text-sm dark:bg-gray-800 dark:text-gray-100\"><pre><code class=\"language-bash\">real 1m30.000s\nuser 2m0.000s\nsys 0m3.000s\n</code></pre></div>\n<p>After running the baseline trace, I could analyze the <code>tsc</code> trace and use a nifty tool called <code>analyze-trace</code> that will report hotspots:</p>\n<div class=\"overflow-auto rounded bg-gray-200 p-4 font-mono text-sm dark:bg-gray-800 dark:text-gray-100\"><pre><code class=\"language-bash\">npm i -g @typescript/analyze-trace\n\nnpx tsc --generateTrace ./.trace\nnpx analyze-trace ./.trace\n</code></pre></div>\n<p>This command will spit out which files are hotspots in your codebase. To my surprise, I was seeing build outputs from webpack as a hotspot.</p>\n<p>Turns out the <code>tsconfig.json</code> had been slightly modified and accidentally included the build folder. Adding the build folder to the excludes entry sped up the tsc command by quite a bit:</p>\n<div class=\"overflow-auto rounded bg-gray-200 p-4 font-mono text-sm dark:bg-gray-800 dark:text-gray-100\"><pre><code class=\"language-bash\">real 0m30.000s\nuser 0m47.000s\nsys 0m2.000s\n</code></pre></div>\n<p>Not only did the trace help me identify the major hotspot in the code, but also\nindicated some additional areas that I could improve upon going forward.</p>","contentCode":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    code: \"code\",\n    pre: \"pre\"\n  }, props.components);\n  return _jsxs(_Fragment, {\n    children: [_jsxs(_components.p, {\n      children: [\"I was working on a project and I noticed the \", _jsx(_components.code, {\n        children: \"tsc\"\n      }), \" check that we ran on pre-commit hooks had become dramatically slower. I was seeing near instant times to type check the entire project go to agonizingly slow minutes to check the project. I was able to narrow the range of commits to some change we made within a month window. I’m sure I could have narrowed it down by continuing to git bisect and re-running \", _jsx(_components.code, {\n        children: \"time npx tsc\"\n      }), \", but it was much easier to just check out main and try out the following:\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-bash\",\n        children: \"# remove any pre-built definitions to get a good baseline\\nrm -r .tsBuildInfo\\n\\n# Get some baseline stats\\ntime npx tsc\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"I was seeing values around (times have been modified for blogging purposes):\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-bash\",\n        children: \"real 1m30.000s\\nuser 2m0.000s\\nsys 0m3.000s\\n\"\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"After running the baseline trace, I could analyze the \", _jsx(_components.code, {\n        children: \"tsc\"\n      }), \" trace and use a nifty tool called \", _jsx(_components.code, {\n        children: \"analyze-trace\"\n      }), \" that will report hotspots:\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-bash\",\n        children: \"npm i -g @typescript/analyze-trace\\n\\nnpx tsc --generateTrace ./.trace\\nnpx analyze-trace ./.trace\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"This command will spit out which files are hotspots in your codebase. To my surprise, I was seeing build outputs from webpack as a hotspot.\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Turns out the \", _jsx(_components.code, {\n        children: \"tsconfig.json\"\n      }), \" had been slightly modified and accidentally included the build folder. Adding the build folder to the excludes entry sped up the tsc command by quite a bit:\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-bash\",\n        children: \"real 0m30.000s\\nuser 0m47.000s\\nsys 0m2.000s\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Not only did the trace help me identify the major hotspot in the code, but also\\nindicated some additional areas that I could improve upon going forward.\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","excerptRaw":"\nI was working on a project and I noticed the `tsc` check that we ran on pre-commit hooks had become dramatically slower. I was seeing near instant times to type check the entire project go to agonizingly slow minutes to check the project. I was able to narrow the range of commits to some change we made within a month window. I’m sure I could have narrowed it down by continuing to git bisect and re-running `time npx tsc`, but it was much easier to just check out main and try out the following:","excerptHTML":"<p>I was working on a project and I noticed the <code>tsc</code> check that we ran on pre-commit hooks had become dramatically slower. I was seeing near instant times to type check the entire project go to agonizingly slow minutes to check the project. I was able to narrow the range of commits to some change we made within a month window. I’m sure I could have narrowed it down by continuing to git bisect and re-running <code>time npx tsc</code>, but it was much easier to just check out main and try out the following:</p>","excerptCode":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {jsx: _jsx, jsxs: _jsxs} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    code: \"code\"\n  }, props.components);\n  return _jsxs(_components.p, {\n    children: [\"I was working on a project and I noticed the \", _jsx(_components.code, {\n      children: \"tsc\"\n    }), \" check that we ran on pre-commit hooks had become dramatically slower. I was seeing near instant times to type check the entire project go to agonizingly slow minutes to check the project. I was able to narrow the range of commits to some change we made within a month window. I’m sure I could have narrowed it down by continuing to git bisect and re-running \", _jsx(_components.code, {\n      children: \"time npx tsc\"\n    }), \", but it was much easier to just check out main and try out the following:\"]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","tags":["typescript"]},"next":{"slug":"2023-07-21-backend-frontend","date":"2023-07-21","title":"Backend for Frontend","frontmatter":{"title":"Backend for Frontend","tags":["backend","frontend","architecture"]},"contentRaw":"\nThe Backend for the Frontend (BEFFE) is typically stateless and acts as a proxy for other services, including authentication, authorization, and core services. The recent divorce of browser code being rendered by backend services was created by SPAs - Single Page Applications. In simpler architectures, a SPA and service could be as simple as:\n\n```mermaid\ngraph LR\n  SPA --> Backend\n```\n\nSPAs also became popular because the codebase could be built into static assets that could be services.\n\nHowever, as complexity arises with the above architecture and the SPA starts to rely on more services, a thin proxy is typically introduced, like NGINX:\n\n```mermaid\ngraph LR\n  SPA --> NGINX\n  NGINX --> Service1\n  NGINX --> Service2\n  NGINX --> Service3\n```\n\nWhile proxies like NGINX can continue to be useful, using it as a proxy ends up putting a lot of routing and additional API handling logic on the client. The client now ends up also having to support, understand, and embed how to interact with the data rom all the downstream services. To simplify and create consistent contracts with the UI, we can create a proxy service: a Backend for the Frontend:\n\n```mermaid\ngraph LR\n  SPA --> BEFFE\n  BEFFE --> Service1\n  BEFFE --> Service2\n  BEFFE --> Service3\n```\n\nThe Backend for the Frontend ends up serving two purposes:\n\n- It acts as a proxy for all UI requests\n- It manages and massages the APIs of downstream services to be a consistent API for the UI.\n\nWhen building a UI, only a single service that acts like an API Gateway is required to understand and encode within the application.\n\nThe Backend for the Frontend model also allows engineers working on services to separate logic for the UI with the logic from an internal service. An internal service can rely on the Backend for the Frontend to handle things like Authentication, Authorization, Caching, and Rate-limits. These mechanisms may also be implemented as independent services, but the Backend for the Frontend can make multiple API calls per any API request from the UI.\n\n## Moving away from SPAs\n\nA newer development in the UI space is server side render the application. If we are already introducing additional complexity by having a Backend for the Frontend, then why not combine the UI code with the backend service and generate hydrated and cacheable pages via the server instead of using a SPA?\n\n```mermaid\ngraph LR\n  UIServer[\"UI Server\"] --> Service1\n  UIServer --> Service2\n UIServer --> Service3\n```\n\nWith this design, we increase the complexity - the backend not has to be able to render the UI code. It must now render, hydrate, make requests to downstream APIs, and send those server-side rendered pages to the browser.\n\nBut this additional complexity enabled us to:\n\n- create truly authenticated and authorized routes – the code for those routes isn’t even streamed to the browser, unlike a SPA.\n- near instant page loads – reducing the bundle size by just sending the JavaScript needed to run the page makes this possible. And since API calls are happening within the same network, this saves the user from additional loading spinners after the initial bundle has been loaded.\n\nThe general trend of having a Backend for the Frontend comes with additional complexity. It stems from the want to enhance the experience of the user, and probably from a subconscious desire to return to a simpler time:\n\n```mermaid\ngraph TD\n  Server\n```\n","contentHTML":"<p>The Backend for the Frontend (BEFFE) is typically stateless and acts as a proxy for other services, including authentication, authorization, and core services. The recent divorce of browser code being rendered by backend services was created by SPAs - Single Page Applications. In simpler architectures, a SPA and service could be as simple as:</p>\n<div class=\"py-8 [&amp;_svg]:m-auto\"><div class=\"mermaid\" data-mermaid-src=\"graph LR\n  SPA --&gt; Backend\">graph LR\n  SPA --&gt; Backend</div></div>\n<p>SPAs also became popular because the codebase could be built into static assets that could be services.</p>\n<p>However, as complexity arises with the above architecture and the SPA starts to rely on more services, a thin proxy is typically introduced, like NGINX:</p>\n<div class=\"py-8 [&amp;_svg]:m-auto\"><div class=\"mermaid\" data-mermaid-src=\"graph LR\n  SPA --&gt; NGINX\n  NGINX --&gt; Service1\n  NGINX --&gt; Service2\n  NGINX --&gt; Service3\">graph LR\n  SPA --&gt; NGINX\n  NGINX --&gt; Service1\n  NGINX --&gt; Service2\n  NGINX --&gt; Service3</div></div>\n<p>While proxies like NGINX can continue to be useful, using it as a proxy ends up putting a lot of routing and additional API handling logic on the client. The client now ends up also having to support, understand, and embed how to interact with the data rom all the downstream services. To simplify and create consistent contracts with the UI, we can create a proxy service: a Backend for the Frontend:</p>\n<div class=\"py-8 [&amp;_svg]:m-auto\"><div class=\"mermaid\" data-mermaid-src=\"graph LR\n  SPA --&gt; BEFFE\n  BEFFE --&gt; Service1\n  BEFFE --&gt; Service2\n  BEFFE --&gt; Service3\">graph LR\n  SPA --&gt; BEFFE\n  BEFFE --&gt; Service1\n  BEFFE --&gt; Service2\n  BEFFE --&gt; Service3</div></div>\n<p>The Backend for the Frontend ends up serving two purposes:</p>\n<ul>\n<li>It acts as a proxy for all UI requests</li>\n<li>It manages and massages the APIs of downstream services to be a consistent API for the UI.</li>\n</ul>\n<p>When building a UI, only a single service that acts like an API Gateway is required to understand and encode within the application.</p>\n<p>The Backend for the Frontend model also allows engineers working on services to separate logic for the UI with the logic from an internal service. An internal service can rely on the Backend for the Frontend to handle things like Authentication, Authorization, Caching, and Rate-limits. These mechanisms may also be implemented as independent services, but the Backend for the Frontend can make multiple API calls per any API request from the UI.</p>\n<h2>Moving away from SPAs</h2>\n<p>A newer development in the UI space is server side render the application. If we are already introducing additional complexity by having a Backend for the Frontend, then why not combine the UI code with the backend service and generate hydrated and cacheable pages via the server instead of using a SPA?</p>\n<div class=\"py-8 [&amp;_svg]:m-auto\"><div class=\"mermaid\" data-mermaid-src=\"graph LR\n  UIServer[&quot;UI Server&quot;] --&gt; Service1\n  UIServer --&gt; Service2\n UIServer --&gt; Service3\">graph LR\n  UIServer[&quot;UI Server&quot;] --&gt; Service1\n  UIServer --&gt; Service2\n UIServer --&gt; Service3</div></div>\n<p>With this design, we increase the complexity - the backend not has to be able to render the UI code. It must now render, hydrate, make requests to downstream APIs, and send those server-side rendered pages to the browser.</p>\n<p>But this additional complexity enabled us to:</p>\n<ul>\n<li>create truly authenticated and authorized routes – the code for those routes isn’t even streamed to the browser, unlike a SPA.</li>\n<li>near instant page loads – reducing the bundle size by just sending the JavaScript needed to run the page makes this possible. And since API calls are happening within the same network, this saves the user from additional loading spinners after the initial bundle has been loaded.</li>\n</ul>\n<p>The general trend of having a Backend for the Frontend comes with additional complexity. It stems from the want to enhance the experience of the user, and probably from a subconscious desire to return to a simpler time:</p>\n<div class=\"py-8 [&amp;_svg]:m-auto\"><div class=\"mermaid\" data-mermaid-src=\"graph TD\n  Server\">graph TD\n  Server</div></div>","contentCode":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    ul: \"ul\",\n    li: \"li\",\n    h2: \"h2\"\n  }, props.components), {Mermaid} = _components;\n  if (!Mermaid) _missingMdxReference(\"Mermaid\", true);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"The Backend for the Frontend (BEFFE) is typically stateless and acts as a proxy for other services, including authentication, authorization, and core services. The recent divorce of browser code being rendered by backend services was created by SPAs - Single Page Applications. In simpler architectures, a SPA and service could be as simple as:\"\n    }), \"\\n\", _jsx(Mermaid, {\n      chart: \"graph LR\\n  SPA --> Backend\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"SPAs also became popular because the codebase could be built into static assets that could be services.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"However, as complexity arises with the above architecture and the SPA starts to rely on more services, a thin proxy is typically introduced, like NGINX:\"\n    }), \"\\n\", _jsx(Mermaid, {\n      chart: \"graph LR\\n  SPA --> NGINX\\n  NGINX --> Service1\\n  NGINX --> Service2\\n  NGINX --> Service3\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"While proxies like NGINX can continue to be useful, using it as a proxy ends up putting a lot of routing and additional API handling logic on the client. The client now ends up also having to support, understand, and embed how to interact with the data rom all the downstream services. To simplify and create consistent contracts with the UI, we can create a proxy service: a Backend for the Frontend:\"\n    }), \"\\n\", _jsx(Mermaid, {\n      chart: \"graph LR\\n  SPA --> BEFFE\\n  BEFFE --> Service1\\n  BEFFE --> Service2\\n  BEFFE --> Service3\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The Backend for the Frontend ends up serving two purposes:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"It acts as a proxy for all UI requests\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"It manages and massages the APIs of downstream services to be a consistent API for the UI.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"When building a UI, only a single service that acts like an API Gateway is required to understand and encode within the application.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The Backend for the Frontend model also allows engineers working on services to separate logic for the UI with the logic from an internal service. An internal service can rely on the Backend for the Frontend to handle things like Authentication, Authorization, Caching, and Rate-limits. These mechanisms may also be implemented as independent services, but the Backend for the Frontend can make multiple API calls per any API request from the UI.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Moving away from SPAs\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"A newer development in the UI space is server side render the application. If we are already introducing additional complexity by having a Backend for the Frontend, then why not combine the UI code with the backend service and generate hydrated and cacheable pages via the server instead of using a SPA?\"\n    }), \"\\n\", _jsx(Mermaid, {\n      chart: \"graph LR\\n  UIServer[\\\"UI Server\\\"] --> Service1\\n  UIServer --> Service2\\n UIServer --> Service3\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"With this design, we increase the complexity - the backend not has to be able to render the UI code. It must now render, hydrate, make requests to downstream APIs, and send those server-side rendered pages to the browser.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"But this additional complexity enabled us to:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"create truly authenticated and authorized routes – the code for those routes isn’t even streamed to the browser, unlike a SPA.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"near instant page loads – reducing the bundle size by just sending the JavaScript needed to run the page makes this possible. And since API calls are happening within the same network, this saves the user from additional loading spinners after the initial bundle has been loaded.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The general trend of having a Backend for the Frontend comes with additional complexity. It stems from the want to enhance the experience of the user, and probably from a subconscious desire to return to a simpler time:\"\n    }), \"\\n\", _jsx(Mermaid, {\n      chart: \"graph TD\\n  Server\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\nfunction _missingMdxReference(id, component) {\n  throw new Error(\"Expected \" + (component ? \"component\" : \"object\") + \" `\" + id + \"` to be defined: you likely forgot to import, pass, or provide it.\");\n}\n","excerptRaw":"\nThe Backend for the Frontend (BEFFE) is typically stateless and acts as a proxy for other services, including authentication, authorization, and core services. The recent divorce of browser code being rendered by backend services was created by SPAs - Single Page Applications. In simpler architectures, a SPA and service could be as simple as:","excerptHTML":"<p>The Backend for the Frontend (BEFFE) is typically stateless and acts as a proxy for other services, including authentication, authorization, and core services. The recent divorce of browser code being rendered by backend services was created by SPAs - Single Page Applications. In simpler architectures, a SPA and service could be as simple as:</p>","excerptCode":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {jsx: _jsx} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\"\n  }, props.components);\n  return _jsx(_components.p, {\n    children: \"The Backend for the Frontend (BEFFE) is typically stateless and acts as a proxy for other services, including authentication, authorization, and core services. The recent divorce of browser code being rendered by backend services was created by SPAs - Single Page Applications. In simpler architectures, a SPA and service could be as simple as:\"\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","tags":["backend","frontend","architecture"]}},"__N_SSG":true}