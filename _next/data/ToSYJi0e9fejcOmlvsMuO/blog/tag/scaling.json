{"pageProps":{"headTitle":"scaling posts - idmontie's Portfolio","headKeywords":"scaling","tag":"scaling","posts":[{"slug":"2023-12-08-the-breaking-point","date":"2023-12-08","title":"The Breaking Point - Understanding the performance of your systems","frontmatter":{"title":"The Breaking Point - Understanding the performance of your systems","tags":["architecture","scaling"]},"contentRaw":"\nEvery system has its limits. When designing and architecting systems, sometimes we as engineers like to assume things can scale infinitely. Maybe we can use larger machines, or maybe we can deploy more instances of a service. But even trying to scale systems isn't magically infinite. It's a smart move to understand the limitations of what we are building upfront so we can make better technology choices, and squeeze the performance out of our existing systems.\n\n![The Breaking Point](/media/2023-12-08-the-breaking-point/splash.png)\n\n## The Myth of Scaling\n\nSoftware engineering often has an optimistic mindset that scaling a service is easy: we just let the more instances of the service be deployed, and voilà, perfect scaling! But the reality is a bit more complex. Instances don't always fire up instantly, services can fail unexpectedly, and not every component can scale indefinitely.\n\nThat's why it's important to understand the limitations of our services. Knowing where the system might buckle under pressure allows us to devise strategies to prevent these breaking points from turning into disasters.\n\nFor example, we may have a public API service with set auto-scaling rules. When the service is hit with many requests, the service spawns additional instances to handle the workload. This ends up working well in most cases, but in the real-world, the service ends up getting hit with huge bursts of requests from downstream users, rather than a steady increase of traffic. When this happens, we scale the service instances up, but now the database gets hit with too many writes and fails over, since the database was originally designed as a read heavy use-case. Of course this is a contrived example, but similar edge-cases commonly appear in complex architectures.\n\nWithout understanding the full system and the constraints of each part, it can be difficult to successfully scale.\n\n## Understanding Server Limitations\n\nA good starting point for finding your system's breaking point is to see how it fares on a single machine. Profile the memory usage when under load, and use load testing to determine how many simultaneous requests the service can handle before performance degrades. To make sense of it all, set some benchmarks:\n\n1. Determine what it means for performance to degrade. In some systems, you may have SLAs for some percentage of requests to complete within a given response time. Other systems may have more leniency.\n2. Use readily available tools to bombard your service with requests and observe the results. For web API services, there are plenty of open source tools that can sustain issuing millions of requests a second and reporting results.\n3. Run the tests multiple times and throw out any outliers in the results. From the rest, understand the peak memory usage, peak disk usage, and peak CPU usage of the service.\n\nOnce you have the information from these types of load tests, it becomes easy to identify indicators that a service will begin to degrade. For some services it is CPU usage that will increase more than the other resources. For some services, memory can get exhausted first.\n\nThe load testing will also be a good indicator of how many requests a single instance of your service can handle before falling over.\n\n## Scaling Strategies\n\nWhen dealing with monolithic services or hefty microservices, you may find from load testing that just vertically scaling the service by adding more hardware is more cost efficient than replicating the instance.\n\nFor smaller service that use less memory and CPU footprint, it may make more sense to horizontally scale the service with many additional smaller servers being deployed.\n\nCloud services like AWS, Azure, and GCP help enable either decision. With AWS, there are many different instance types to get the right size for your specific service deployment. With the right size picked out, auto-scaling becomes a more budget-friendly option, as you're only spinning up instances that meet your specific resource requirements.\n\nCloud providers additionally have ways to detect when to auto-scale an instance based on resource usage, making load testing very informative to tune these values.\n\n## Conclusion\n\nScaling isn't a one-size-fits-all solution. It's about understanding the unique limitations and potential of your system. Through careful analysis, load testing, and a bit of strategic thinking, you can develop a scaling strategy that not only meets your performance needs but also keeps an eye on the cost of your system. Whether it's beefing up your hardware or multiplying smaller instances, the key lies in making informed, tailored decisions for your system's specific demands and quirks.\n","contentHTML":"<p>Every system has its limits. When designing and architecting systems, sometimes we as engineers like to assume things can scale infinitely. Maybe we can use larger machines, or maybe we can deploy more instances of a service. But even trying to scale systems isn&#x27;t magically infinite. It&#x27;s a smart move to understand the limitations of what we are building upfront so we can make better technology choices, and squeeze the performance out of our existing systems.</p>\n<p><img alt=\"The Breaking Point\" src=\"/media/2023-12-08-the-breaking-point/splash.png\" style=\"max-height:500px;margin:auto;text-align:center\"/></p>\n<h2>The Myth of Scaling</h2>\n<p>Software engineering often has an optimistic mindset that scaling a service is easy: we just let the more instances of the service be deployed, and voilà, perfect scaling! But the reality is a bit more complex. Instances don&#x27;t always fire up instantly, services can fail unexpectedly, and not every component can scale indefinitely.</p>\n<p>That&#x27;s why it&#x27;s important to understand the limitations of our services. Knowing where the system might buckle under pressure allows us to devise strategies to prevent these breaking points from turning into disasters.</p>\n<p>For example, we may have a public API service with set auto-scaling rules. When the service is hit with many requests, the service spawns additional instances to handle the workload. This ends up working well in most cases, but in the real-world, the service ends up getting hit with huge bursts of requests from downstream users, rather than a steady increase of traffic. When this happens, we scale the service instances up, but now the database gets hit with too many writes and fails over, since the database was originally designed as a read heavy use-case. Of course this is a contrived example, but similar edge-cases commonly appear in complex architectures.</p>\n<p>Without understanding the full system and the constraints of each part, it can be difficult to successfully scale.</p>\n<h2>Understanding Server Limitations</h2>\n<p>A good starting point for finding your system&#x27;s breaking point is to see how it fares on a single machine. Profile the memory usage when under load, and use load testing to determine how many simultaneous requests the service can handle before performance degrades. To make sense of it all, set some benchmarks:</p>\n<ol>\n<li>Determine what it means for performance to degrade. In some systems, you may have SLAs for some percentage of requests to complete within a given response time. Other systems may have more leniency.</li>\n<li>Use readily available tools to bombard your service with requests and observe the results. For web API services, there are plenty of open source tools that can sustain issuing millions of requests a second and reporting results.</li>\n<li>Run the tests multiple times and throw out any outliers in the results. From the rest, understand the peak memory usage, peak disk usage, and peak CPU usage of the service.</li>\n</ol>\n<p>Once you have the information from these types of load tests, it becomes easy to identify indicators that a service will begin to degrade. For some services it is CPU usage that will increase more than the other resources. For some services, memory can get exhausted first.</p>\n<p>The load testing will also be a good indicator of how many requests a single instance of your service can handle before falling over.</p>\n<h2>Scaling Strategies</h2>\n<p>When dealing with monolithic services or hefty microservices, you may find from load testing that just vertically scaling the service by adding more hardware is more cost efficient than replicating the instance.</p>\n<p>For smaller service that use less memory and CPU footprint, it may make more sense to horizontally scale the service with many additional smaller servers being deployed.</p>\n<p>Cloud services like AWS, Azure, and GCP help enable either decision. With AWS, there are many different instance types to get the right size for your specific service deployment. With the right size picked out, auto-scaling becomes a more budget-friendly option, as you&#x27;re only spinning up instances that meet your specific resource requirements.</p>\n<p>Cloud providers additionally have ways to detect when to auto-scale an instance based on resource usage, making load testing very informative to tune these values.</p>\n<h2>Conclusion</h2>\n<p>Scaling isn&#x27;t a one-size-fits-all solution. It&#x27;s about understanding the unique limitations and potential of your system. Through careful analysis, load testing, and a bit of strategic thinking, you can develop a scaling strategy that not only meets your performance needs but also keeps an eye on the cost of your system. Whether it&#x27;s beefing up your hardware or multiplying smaller instances, the key lies in making informed, tailored decisions for your system&#x27;s specific demands and quirks.</p>","contentCode":"\"use strict\";\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = {\n    h2: \"h2\",\n    img: \"img\",\n    li: \"li\",\n    ol: \"ol\",\n    p: \"p\",\n    ...props.components\n  };\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"Every system has its limits. When designing and architecting systems, sometimes we as engineers like to assume things can scale infinitely. Maybe we can use larger machines, or maybe we can deploy more instances of a service. But even trying to scale systems isn't magically infinite. It's a smart move to understand the limitations of what we are building upfront so we can make better technology choices, and squeeze the performance out of our existing systems.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/media/2023-12-08-the-breaking-point/splash.png\",\n        alt: \"The Breaking Point\"\n      })\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"The Myth of Scaling\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Software engineering often has an optimistic mindset that scaling a service is easy: we just let the more instances of the service be deployed, and voilà, perfect scaling! But the reality is a bit more complex. Instances don't always fire up instantly, services can fail unexpectedly, and not every component can scale indefinitely.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"That's why it's important to understand the limitations of our services. Knowing where the system might buckle under pressure allows us to devise strategies to prevent these breaking points from turning into disasters.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"For example, we may have a public API service with set auto-scaling rules. When the service is hit with many requests, the service spawns additional instances to handle the workload. This ends up working well in most cases, but in the real-world, the service ends up getting hit with huge bursts of requests from downstream users, rather than a steady increase of traffic. When this happens, we scale the service instances up, but now the database gets hit with too many writes and fails over, since the database was originally designed as a read heavy use-case. Of course this is a contrived example, but similar edge-cases commonly appear in complex architectures.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Without understanding the full system and the constraints of each part, it can be difficult to successfully scale.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Understanding Server Limitations\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"A good starting point for finding your system's breaking point is to see how it fares on a single machine. Profile the memory usage when under load, and use load testing to determine how many simultaneous requests the service can handle before performance degrades. To make sense of it all, set some benchmarks:\"\n    }), \"\\n\", _jsxs(_components.ol, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"Determine what it means for performance to degrade. In some systems, you may have SLAs for some percentage of requests to complete within a given response time. Other systems may have more leniency.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Use readily available tools to bombard your service with requests and observe the results. For web API services, there are plenty of open source tools that can sustain issuing millions of requests a second and reporting results.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Run the tests multiple times and throw out any outliers in the results. From the rest, understand the peak memory usage, peak disk usage, and peak CPU usage of the service.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Once you have the information from these types of load tests, it becomes easy to identify indicators that a service will begin to degrade. For some services it is CPU usage that will increase more than the other resources. For some services, memory can get exhausted first.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The load testing will also be a good indicator of how many requests a single instance of your service can handle before falling over.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Scaling Strategies\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"When dealing with monolithic services or hefty microservices, you may find from load testing that just vertically scaling the service by adding more hardware is more cost efficient than replicating the instance.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"For smaller service that use less memory and CPU footprint, it may make more sense to horizontally scale the service with many additional smaller servers being deployed.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Cloud services like AWS, Azure, and GCP help enable either decision. With AWS, there are many different instance types to get the right size for your specific service deployment. With the right size picked out, auto-scaling becomes a more budget-friendly option, as you're only spinning up instances that meet your specific resource requirements.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Cloud providers additionally have ways to detect when to auto-scale an instance based on resource usage, making load testing very informative to tune these values.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Conclusion\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Scaling isn't a one-size-fits-all solution. It's about understanding the unique limitations and potential of your system. Through careful analysis, load testing, and a bit of strategic thinking, you can develop a scaling strategy that not only meets your performance needs but also keeps an eye on the cost of your system. Whether it's beefing up your hardware or multiplying smaller instances, the key lies in making informed, tailored decisions for your system's specific demands and quirks.\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, {\n    ...props,\n    children: _jsx(_createMdxContent, {\n      ...props\n    })\n  }) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","excerptRaw":"\nEvery system has its limits. When designing and architecting systems, sometimes we as engineers like to assume things can scale infinitely. Maybe we can use larger machines, or maybe we can deploy more instances of a service. But even trying to scale systems isn't magically infinite. It's a smart move to understand the limitations of what we are building upfront so we can make better technology choices, and squeeze the performance out of our existing systems.","excerptHTML":"<p>Every system has its limits. When designing and architecting systems, sometimes we as engineers like to assume things can scale infinitely. Maybe we can use larger machines, or maybe we can deploy more instances of a service. But even trying to scale systems isn&#x27;t magically infinite. It&#x27;s a smart move to understand the limitations of what we are building upfront so we can make better technology choices, and squeeze the performance out of our existing systems.</p>","excerptCode":"\"use strict\";\nconst {jsx: _jsx} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = {\n    p: \"p\",\n    ...props.components\n  };\n  return _jsx(_components.p, {\n    children: \"Every system has its limits. When designing and architecting systems, sometimes we as engineers like to assume things can scale infinitely. Maybe we can use larger machines, or maybe we can deploy more instances of a service. But even trying to scale systems isn't magically infinite. It's a smart move to understand the limitations of what we are building upfront so we can make better technology choices, and squeeze the performance out of our existing systems.\"\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, {\n    ...props,\n    children: _jsx(_createMdxContent, {\n      ...props\n    })\n  }) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","tags":["architecture","scaling"]}]},"__N_SSG":true}