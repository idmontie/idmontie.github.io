{"pageProps":{"posts":[{"slug":"2023-07-03-llm-loops","date":"2023-07-03","title":"AI Feedback Systems","frontmatter":{"title":"AI Feedback Systems","tags":["ai","llm"]},"contentRaw":"\nWe are starting to see a rise of novel use-cases for AI in products and games using LLMs. Rather than the simple chatbot like experiences we have seen in the past using AI, we are starting to see feedback systems being added to these experiences, providing additional context to the LLM than just the past conversation.\n\nA typical game loop for this type of system would look like:\n\n```mermaid\ngraph LR\n  Rules --> LLM\n  InputStates[\"Input States\"] --> LLM\n  LLM --> OutputState\n  OutputState[\"Output State\"] --> GameEngine\n  GameEngine[\"Game Engine\"] --> InputStates\n```\n\nRules can be describes as written-word description, with an additional set of rules telling the LLM to reply using JSON output of a given schema. In this area, I have had success giving LLMs descriptions of output schemas in Typescript and asking for a JSON response that adheres to the type. Other methods of getting a consistent schema are more than likely possible here, as well as additional output methods.\n\nWhen the asynchronous task of creating and output state is complete, the Game Engine in this case can read, parse, and apply that new state to the world. Any additional interaction would then lead to the next set of input states that can be given to the LLM as a JSON blob.\n\nFor a more concrete example, we can imagine a game where we want our player to interact with a set of agents. The input states would be the state of each agent, the user’s interaction, and maybe some global environment data. The rules may be how each agent should behave, the rules of the game, and additional context. The LLM would take these inputs, and the output is instructed to be the next state of each agent. When the LLM returns this data, the Game Engine read it and applies it to the game’s representation of each agent, showing the player the impact of their actions.\n\nI’m looking forward to more novel use-cases for LLMs!\n","contentHTML":"<p>We are starting to see a rise of novel use-cases for AI in products and games using LLMs. Rather than the simple chatbot like experiences we have seen in the past using AI, we are starting to see feedback systems being added to these experiences, providing additional context to the LLM than just the past conversation.</p>\n<p>A typical game loop for this type of system would look like:</p>\n<mermaid chart=\"graph LR\n  Rules --&gt; LLM\n  InputStates[&quot;Input States&quot;] --&gt; LLM\n  LLM --&gt; OutputState\n  OutputState[&quot;Output State&quot;] --&gt; GameEngine\n  GameEngine[&quot;Game Engine&quot;] --&gt; InputStates\"></mermaid>\n<p>Rules can be describes as written-word description, with an additional set of rules telling the LLM to reply using JSON output of a given schema. In this area, I have had success giving LLMs descriptions of output schemas in Typescript and asking for a JSON response that adheres to the type. Other methods of getting a consistent schema are more than likely possible here, as well as additional output methods.</p>\n<p>When the asynchronous task of creating and output state is complete, the Game Engine in this case can read, parse, and apply that new state to the world. Any additional interaction would then lead to the next set of input states that can be given to the LLM as a JSON blob.</p>\n<p>For a more concrete example, we can imagine a game where we want our player to interact with a set of agents. The input states would be the state of each agent, the user’s interaction, and maybe some global environment data. The rules may be how each agent should behave, the rules of the game, and additional context. The LLM would take these inputs, and the output is instructed to be the next state of each agent. When the LLM returns this data, the Game Engine read it and applies it to the game’s representation of each agent, showing the player the impact of their actions.</p>\n<p>I’m looking forward to more novel use-cases for LLMs!</p>","contentCode":"\"use strict\";\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = {\n    mermaid: \"mermaid\",\n    p: \"p\",\n    ...props.components\n  };\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"We are starting to see a rise of novel use-cases for AI in products and games using LLMs. Rather than the simple chatbot like experiences we have seen in the past using AI, we are starting to see feedback systems being added to these experiences, providing additional context to the LLM than just the past conversation.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"A typical game loop for this type of system would look like:\"\n    }), \"\\n\", _jsx(_components.mermaid, {\n      chart: \"graph LR\\n  Rules --> LLM\\n  InputStates[\\\"Input States\\\"] --> LLM\\n  LLM --> OutputState\\n  OutputState[\\\"Output State\\\"] --> GameEngine\\n  GameEngine[\\\"Game Engine\\\"] --> InputStates\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Rules can be describes as written-word description, with an additional set of rules telling the LLM to reply using JSON output of a given schema. In this area, I have had success giving LLMs descriptions of output schemas in Typescript and asking for a JSON response that adheres to the type. Other methods of getting a consistent schema are more than likely possible here, as well as additional output methods.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"When the asynchronous task of creating and output state is complete, the Game Engine in this case can read, parse, and apply that new state to the world. Any additional interaction would then lead to the next set of input states that can be given to the LLM as a JSON blob.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"For a more concrete example, we can imagine a game where we want our player to interact with a set of agents. The input states would be the state of each agent, the user’s interaction, and maybe some global environment data. The rules may be how each agent should behave, the rules of the game, and additional context. The LLM would take these inputs, and the output is instructed to be the next state of each agent. When the LLM returns this data, the Game Engine read it and applies it to the game’s representation of each agent, showing the player the impact of their actions.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"I’m looking forward to more novel use-cases for LLMs!\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, {\n    ...props,\n    children: _jsx(_createMdxContent, {\n      ...props\n    })\n  }) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","excerptRaw":"\nWe are starting to see a rise of novel use-cases for AI in products and games using LLMs. Rather than the simple chatbot like experiences we have seen in the past using AI, we are starting to see feedback systems being added to these experiences, providing additional context to the LLM than just the past conversation.","excerptHTML":"<p>We are starting to see a rise of novel use-cases for AI in products and games using LLMs. Rather than the simple chatbot like experiences we have seen in the past using AI, we are starting to see feedback systems being added to these experiences, providing additional context to the LLM than just the past conversation.</p>","excerptCode":"\"use strict\";\nconst {jsx: _jsx} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = {\n    p: \"p\",\n    ...props.components\n  };\n  return _jsx(_components.p, {\n    children: \"We are starting to see a rise of novel use-cases for AI in products and games using LLMs. Rather than the simple chatbot like experiences we have seen in the past using AI, we are starting to see feedback systems being added to these experiences, providing additional context to the LLM than just the past conversation.\"\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, {\n    ...props,\n    children: _jsx(_createMdxContent, {\n      ...props\n    })\n  }) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","tags":["ai","llm"]},{"slug":"2023-07-01-fast-embedding-lookingup","date":"2023-07-01","title":"Fast Similar Embedding Lookup","frontmatter":{"title":"Fast Similar Embedding Lookup","tags":["nlp","machine learning"]},"contentRaw":"\nWhile working on the Clarity Hub NLP API, we had a common use-case where we would create embeddings from text, and use those embedding to determine cosine similarity with other embeddings. Doing this required loading all of the embeddings in-memory and then computing cosine similarity with the entire dataset. As the dataset grew, this operation would get incredibly slow.\n\nWe worked on a fast way to do these lookups using ranges that can be performed in any database. This approach was never implemented, but we worked on multiple proof-of-concepts to test out our ideas. The goal was to take an input text, compute an embedding, load the entire embedding datasets loaded into an AWS lambda, find the most similar set of vectors, and return the top N similar vectors in one use-case. To tackle that, we came up with the following idea.\n\nGiven a vector A, compute is similar to a unit vector U of the same dimension as A. So:\n\n```cpp\ndim(U) = dim(A)\n```\n\nAnd\n\n```cpp\nS_u = cos(θ) = A · U / ||A|| x ||U||\n```\n\nWhere S_u is the similarity with the unit vector. The unit vector just needs to be the same across all samples.\n\nFor each embedding, store the calculated S_u.\n\nIf we want to find similar vectors for a new vector B, then we compute is similarity to the unit vector.\n\nThen, we can query the database for vectors within an interval of `[S_u - ε, S_u + ε]` . This will give us a subset of the dataset that have similar similarities with the unit vector.\n\nWe can re-query increasing or decreasing ε until the top N results are found.\n\nTo further improve accuracy, we can also re-compute the similarity score using cosine similarity with the subset of vectors, which is still much faster then computing the similarity against the entire dataset.\n\nThis approach begins to break down as the cosine similarity to the unit vector chosen gets very large (`> 0.4`).  We end up with the possibility of matching against vectors that are of opposite directions – the least similar vectors to the original input vector.\n\nOne solution to workaround this could be to pre-compute the similarity of a vector against unit vectors for each dimension of the input vector. But this could be 512 or more cosine similarity calculations for modern embeddings just to precompute the data. Once all unit vector similarities are calculated and stored, the range query against the database would be made against the column for which the input vector’s similarity is closest to 0.\n\nThere are a lot of real solutions to this problem, but this was a fun exercise to think about and work on.\n\n## Further reading\n\nVector similarity search is becoming increasingly popular and integrated into databases. Here are some resources to learn more: [Vector Similarity Search](https://zilliz.com/blog/vector-similarity-search).\n","contentHTML":"<p>While working on the Clarity Hub NLP API, we had a common use-case where we would create embeddings from text, and use those embedding to determine cosine similarity with other embeddings. Doing this required loading all of the embeddings in-memory and then computing cosine similarity with the entire dataset. As the dataset grew, this operation would get incredibly slow.</p>\n<p>We worked on a fast way to do these lookups using ranges that can be performed in any database. This approach was never implemented, but we worked on multiple proof-of-concepts to test out our ideas. The goal was to take an input text, compute an embedding, load the entire embedding datasets loaded into an AWS lambda, find the most similar set of vectors, and return the top N similar vectors in one use-case. To tackle that, we came up with the following idea.</p>\n<p>Given a vector A, compute is similar to a unit vector U of the same dimension as A. So:</p>\n<div class=\"overflow-auto rounded bg-gray-200 p-4 font-mono text-sm dark:bg-gray-800 dark:text-gray-100\"><pre><code class=\"language-cpp\">dim(U) = dim(A)\n</code></pre></div>\n<p>And</p>\n<div class=\"overflow-auto rounded bg-gray-200 p-4 font-mono text-sm dark:bg-gray-800 dark:text-gray-100\"><pre><code class=\"language-cpp\">S_u = cos(θ) = A · U / ||A|| x ||U||\n</code></pre></div>\n<p>Where S_u is the similarity with the unit vector. The unit vector just needs to be the same across all samples.</p>\n<p>For each embedding, store the calculated S_u.</p>\n<p>If we want to find similar vectors for a new vector B, then we compute is similarity to the unit vector.</p>\n<p>Then, we can query the database for vectors within an interval of <code>[S_u - ε, S_u + ε]</code> . This will give us a subset of the dataset that have similar similarities with the unit vector.</p>\n<p>We can re-query increasing or decreasing ε until the top N results are found.</p>\n<p>To further improve accuracy, we can also re-compute the similarity score using cosine similarity with the subset of vectors, which is still much faster then computing the similarity against the entire dataset.</p>\n<p>This approach begins to break down as the cosine similarity to the unit vector chosen gets very large (<code>&gt; 0.4</code>).  We end up with the possibility of matching against vectors that are of opposite directions – the least similar vectors to the original input vector.</p>\n<p>One solution to workaround this could be to pre-compute the similarity of a vector against unit vectors for each dimension of the input vector. But this could be 512 or more cosine similarity calculations for modern embeddings just to precompute the data. Once all unit vector similarities are calculated and stored, the range query against the database would be made against the column for which the input vector’s similarity is closest to 0.</p>\n<p>There are a lot of real solutions to this problem, but this was a fun exercise to think about and work on.</p>\n<h2>Further reading</h2>\n<p>Vector similarity search is becoming increasingly popular and integrated into databases. Here are some resources to learn more: <a href=\"https://zilliz.com/blog/vector-similarity-search\">Vector Similarity Search</a>.</p>","contentCode":"\"use strict\";\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = {\n    a: \"a\",\n    code: \"code\",\n    h2: \"h2\",\n    p: \"p\",\n    pre: \"pre\",\n    ...props.components\n  };\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"While working on the Clarity Hub NLP API, we had a common use-case where we would create embeddings from text, and use those embedding to determine cosine similarity with other embeddings. Doing this required loading all of the embeddings in-memory and then computing cosine similarity with the entire dataset. As the dataset grew, this operation would get incredibly slow.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"We worked on a fast way to do these lookups using ranges that can be performed in any database. This approach was never implemented, but we worked on multiple proof-of-concepts to test out our ideas. The goal was to take an input text, compute an embedding, load the entire embedding datasets loaded into an AWS lambda, find the most similar set of vectors, and return the top N similar vectors in one use-case. To tackle that, we came up with the following idea.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Given a vector A, compute is similar to a unit vector U of the same dimension as A. So:\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-cpp\",\n        children: \"dim(U) = dim(A)\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"And\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-cpp\",\n        children: \"S_u = cos(θ) = A · U / ||A|| x ||U||\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Where S_u is the similarity with the unit vector. The unit vector just needs to be the same across all samples.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"For each embedding, store the calculated S_u.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"If we want to find similar vectors for a new vector B, then we compute is similarity to the unit vector.\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Then, we can query the database for vectors within an interval of \", _jsx(_components.code, {\n        children: \"[S_u - ε, S_u + ε]\"\n      }), \" . This will give us a subset of the dataset that have similar similarities with the unit vector.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"We can re-query increasing or decreasing ε until the top N results are found.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"To further improve accuracy, we can also re-compute the similarity score using cosine similarity with the subset of vectors, which is still much faster then computing the similarity against the entire dataset.\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"This approach begins to break down as the cosine similarity to the unit vector chosen gets very large (\", _jsx(_components.code, {\n        children: \"> 0.4\"\n      }), \").  We end up with the possibility of matching against vectors that are of opposite directions – the least similar vectors to the original input vector.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"One solution to workaround this could be to pre-compute the similarity of a vector against unit vectors for each dimension of the input vector. But this could be 512 or more cosine similarity calculations for modern embeddings just to precompute the data. Once all unit vector similarities are calculated and stored, the range query against the database would be made against the column for which the input vector’s similarity is closest to 0.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"There are a lot of real solutions to this problem, but this was a fun exercise to think about and work on.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Further reading\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Vector similarity search is becoming increasingly popular and integrated into databases. Here are some resources to learn more: \", _jsx(_components.a, {\n        href: \"https://zilliz.com/blog/vector-similarity-search\",\n        children: \"Vector Similarity Search\"\n      }), \".\"]\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, {\n    ...props,\n    children: _jsx(_createMdxContent, {\n      ...props\n    })\n  }) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","excerptRaw":"\nWhile working on the Clarity Hub NLP API, we had a common use-case where we would create embeddings from text, and use those embedding to determine cosine similarity with other embeddings. Doing this required loading all of the embeddings in-memory and then computing cosine similarity with the entire dataset. As the dataset grew, this operation would get incredibly slow.","excerptHTML":"<p>While working on the Clarity Hub NLP API, we had a common use-case where we would create embeddings from text, and use those embedding to determine cosine similarity with other embeddings. Doing this required loading all of the embeddings in-memory and then computing cosine similarity with the entire dataset. As the dataset grew, this operation would get incredibly slow.</p>","excerptCode":"\"use strict\";\nconst {jsx: _jsx} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = {\n    p: \"p\",\n    ...props.components\n  };\n  return _jsx(_components.p, {\n    children: \"While working on the Clarity Hub NLP API, we had a common use-case where we would create embeddings from text, and use those embedding to determine cosine similarity with other embeddings. Doing this required loading all of the embeddings in-memory and then computing cosine similarity with the entire dataset. As the dataset grew, this operation would get incredibly slow.\"\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, {\n    ...props,\n    children: _jsx(_createMdxContent, {\n      ...props\n    })\n  }) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","tags":["nlp","machine learning"]},{"slug":"2023-06-06-sora","date":"2023-06-06","title":"Sora - OpenAI Visual Studio Code Extension","frontmatter":{"title":"Sora - OpenAI Visual Studio Code Extension","tags":["openai","vscode"]},"contentRaw":"Github Copilot and other AI tools are hitting the scene. I decided to create my own Visual Studio Code extension, which is designed to use OpenAI’s APIs to bring some additional ChatGPT functionality into the code editor. The goal with Sora was to enable a developer to thoughtfully write a comment about the code they would like the AI to write, and then commit to it – rather than the real-time typeahead that Github Copilot provides.\n\nWith that goal in mind, Sora provides two ways to activate OpenAI: by typing `@OpenAI` (formally `@ChatGPT`) or clicking “Send to OpenAI” when hovering over a comment. The other improvement is that Sora will read any relative link references to files in your project. A great way to use this is to have OpenAI write code in the style that already exists in your project, for example:\n\n```tsx\n/**\n * Write tests for [my-file](./my-file.ts] using\n * [other-test](../something/other-file.test.ts) as an example\n */\n```\n\nUsing this extension lets users write specifications as comments, and have ChatGPT write the entire file for you.\n\n![Sora Preview](/media/2023-06-06-sora/sora-preview.gif)\n\nThis extension leverages the OpenAI API to send any referenced files and a starting prompt to the `gpt-3.5-turbo` chat completion endpoint. The prompt mainly sets the context for creating working code using a given language and reference files.\n\nOnce the response comes back, the extension parses it and appends it to the original file.\n\n## **Installation and Usage**\n\nYou can install the extension by going to [the VSCode Marketplace](https://marketplace.visualstudio.com/items?itemName=CapsuleCat.sora-by-capsule-cat), or searching for “Sora” in Visual Studio Code extensions.\n\nOnce installed, you will need to enter your own OpenAI API key. You can get your key by following [these instructions](https://help.openai.com/en/articles/4936850-where-do-i-find-my-secret-api-key). Then just enter `Sora: Set API Key` into the Visual Studio Code command prompt.\n\n![Sora Set API Key](/media/2023-06-06-sora/sora-set-api-key.png)\n\nYou can review the code [on Github](https://github.com/CapsuleCat/sora-by-capsule-cat).\n\n## **Conclusion**\n\nIt was fun building my first extension. Please feel free to reach out on our [Github](https://github.com/CapsuleCat/sora-by-capsule-cat) for feedback or questions.\n","contentHTML":"<p>Github Copilot and other AI tools are hitting the scene. I decided to create my own Visual Studio Code extension, which is designed to use OpenAI’s APIs to bring some additional ChatGPT functionality into the code editor. The goal with Sora was to enable a developer to thoughtfully write a comment about the code they would like the AI to write, and then commit to it – rather than the real-time typeahead that Github Copilot provides.</p>\n<p>With that goal in mind, Sora provides two ways to activate OpenAI: by typing <code>@OpenAI</code> (formally <code>@ChatGPT</code>) or clicking “Send to OpenAI” when hovering over a comment. The other improvement is that Sora will read any relative link references to files in your project. A great way to use this is to have OpenAI write code in the style that already exists in your project, for example:</p>\n<div class=\"overflow-auto rounded bg-gray-200 p-4 font-mono text-sm dark:bg-gray-800 dark:text-gray-100\"><pre><code class=\"language-tsx\">/**\n * Write tests for [my-file](./my-file.ts] using\n * [other-test](../something/other-file.test.ts) as an example\n */\n</code></pre></div>\n<p>Using this extension lets users write specifications as comments, and have ChatGPT write the entire file for you.</p>\n<p><img alt=\"Sora Preview\" src=\"/media/2023-06-06-sora/sora-preview.gif\" style=\"max-height:500px;margin:auto;text-align:center\"/></p>\n<p>This extension leverages the OpenAI API to send any referenced files and a starting prompt to the <code>gpt-3.5-turbo</code> chat completion endpoint. The prompt mainly sets the context for creating working code using a given language and reference files.</p>\n<p>Once the response comes back, the extension parses it and appends it to the original file.</p>\n<h2><strong>Installation and Usage</strong></h2>\n<p>You can install the extension by going to <a href=\"https://marketplace.visualstudio.com/items?itemName=CapsuleCat.sora-by-capsule-cat\">the VSCode Marketplace</a>, or searching for “Sora” in Visual Studio Code extensions.</p>\n<p>Once installed, you will need to enter your own OpenAI API key. You can get your key by following <a href=\"https://help.openai.com/en/articles/4936850-where-do-i-find-my-secret-api-key\">these instructions</a>. Then just enter <code>Sora: Set API Key</code> into the Visual Studio Code command prompt.</p>\n<p><img alt=\"Sora Set API Key\" src=\"/media/2023-06-06-sora/sora-set-api-key.png\" style=\"max-height:500px;margin:auto;text-align:center\"/></p>\n<p>You can review the code <a href=\"https://github.com/CapsuleCat/sora-by-capsule-cat\">on Github</a>.</p>\n<h2><strong>Conclusion</strong></h2>\n<p>It was fun building my first extension. Please feel free to reach out on our <a href=\"https://github.com/CapsuleCat/sora-by-capsule-cat\">Github</a> for feedback or questions.</p>","contentCode":"\"use strict\";\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = {\n    a: \"a\",\n    code: \"code\",\n    h2: \"h2\",\n    img: \"img\",\n    p: \"p\",\n    pre: \"pre\",\n    strong: \"strong\",\n    ...props.components\n  };\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"Github Copilot and other AI tools are hitting the scene. I decided to create my own Visual Studio Code extension, which is designed to use OpenAI’s APIs to bring some additional ChatGPT functionality into the code editor. The goal with Sora was to enable a developer to thoughtfully write a comment about the code they would like the AI to write, and then commit to it – rather than the real-time typeahead that Github Copilot provides.\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"With that goal in mind, Sora provides two ways to activate OpenAI: by typing \", _jsx(_components.code, {\n        children: \"@OpenAI\"\n      }), \" (formally \", _jsx(_components.code, {\n        children: \"@ChatGPT\"\n      }), \") or clicking “Send to OpenAI” when hovering over a comment. The other improvement is that Sora will read any relative link references to files in your project. A great way to use this is to have OpenAI write code in the style that already exists in your project, for example:\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"language-tsx\",\n        children: \"/**\\n * Write tests for [my-file](./my-file.ts] using\\n * [other-test](../something/other-file.test.ts) as an example\\n */\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Using this extension lets users write specifications as comments, and have ChatGPT write the entire file for you.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/media/2023-06-06-sora/sora-preview.gif\",\n        alt: \"Sora Preview\"\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"This extension leverages the OpenAI API to send any referenced files and a starting prompt to the \", _jsx(_components.code, {\n        children: \"gpt-3.5-turbo\"\n      }), \" chat completion endpoint. The prompt mainly sets the context for creating working code using a given language and reference files.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Once the response comes back, the extension parses it and appends it to the original file.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: _jsx(_components.strong, {\n        children: \"Installation and Usage\"\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"You can install the extension by going to \", _jsx(_components.a, {\n        href: \"https://marketplace.visualstudio.com/items?itemName=CapsuleCat.sora-by-capsule-cat\",\n        children: \"the VSCode Marketplace\"\n      }), \", or searching for “Sora” in Visual Studio Code extensions.\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Once installed, you will need to enter your own OpenAI API key. You can get your key by following \", _jsx(_components.a, {\n        href: \"https://help.openai.com/en/articles/4936850-where-do-i-find-my-secret-api-key\",\n        children: \"these instructions\"\n      }), \". Then just enter \", _jsx(_components.code, {\n        children: \"Sora: Set API Key\"\n      }), \" into the Visual Studio Code command prompt.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/media/2023-06-06-sora/sora-set-api-key.png\",\n        alt: \"Sora Set API Key\"\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"You can review the code \", _jsx(_components.a, {\n        href: \"https://github.com/CapsuleCat/sora-by-capsule-cat\",\n        children: \"on Github\"\n      }), \".\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      children: _jsx(_components.strong, {\n        children: \"Conclusion\"\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"It was fun building my first extension. Please feel free to reach out on our \", _jsx(_components.a, {\n        href: \"https://github.com/CapsuleCat/sora-by-capsule-cat\",\n        children: \"Github\"\n      }), \" for feedback or questions.\"]\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, {\n    ...props,\n    children: _jsx(_createMdxContent, {\n      ...props\n    })\n  }) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","excerptRaw":"Github Copilot and other AI tools are hitting the scene. I decided to create my own Visual Studio Code extension, which is designed to use OpenAI’s APIs to bring some additional ChatGPT functionality into the code editor. The goal with Sora was to enable a developer to thoughtfully write a comment about the code they would like the AI to write, and then commit to it – rather than the real-time typeahead that Github Copilot provides.\n","excerptHTML":"<p>Github Copilot and other AI tools are hitting the scene. I decided to create my own Visual Studio Code extension, which is designed to use OpenAI’s APIs to bring some additional ChatGPT functionality into the code editor. The goal with Sora was to enable a developer to thoughtfully write a comment about the code they would like the AI to write, and then commit to it – rather than the real-time typeahead that Github Copilot provides.</p>","excerptCode":"\"use strict\";\nconst {jsx: _jsx} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = {\n    p: \"p\",\n    ...props.components\n  };\n  return _jsx(_components.p, {\n    children: \"Github Copilot and other AI tools are hitting the scene. I decided to create my own Visual Studio Code extension, which is designed to use OpenAI’s APIs to bring some additional ChatGPT functionality into the code editor. The goal with Sora was to enable a developer to thoughtfully write a comment about the code they would like the AI to write, and then commit to it – rather than the real-time typeahead that Github Copilot provides.\"\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, {\n    ...props,\n    children: _jsx(_createMdxContent, {\n      ...props\n    })\n  }) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","tags":["openai","vscode"]},{"slug":"2023-05-06-gptp","date":"2023-05-06","title":"Revisiting GPTP - the Starcraft modding toolkit","frontmatter":{"title":"Revisiting GPTP - the Starcraft modding toolkit","tags":["starcraft","gptp","cplusplus"]},"contentRaw":"\nOne of the first PC games I played was Starcraft and the expansion Starcraft: Broodwar. We didn’t have a PC, so I had to play it on a friend’s computer, but I remember being immersed in the real-time strategy gameplay.\n\nLater, I would get a PC and bought my own copy of Starcraft. This was years later, but the community for the game was still impressive. Joining one of the older Starcraft forums, I discovered “mods” for the game. The most popular ones were just graphical changes, but that concept of changing a game to display your own graphics was so interesting to me. I’m not sure how I stumbled on it, but I found [http://www.staredit.net/](http://www.staredit.net/) (yes, it’s still active!) and I learned that you could do so much more than just change the graphics of Starcraft with mods. There were people in the community working on hooking into existing Starcraft code to modify the gameplay and graphics using C++.\n\n## Diving deep\n\nIn order to create these mods, we needed to know the hex address of different functions that Starcraft would call during them game. And to do anything meaningful, we also needed to know the structure of units and sprites and where those were stored as well. A lot of this base work required using tools like OllyDbg to analyze the assembly of the Starcraft executable, and ArtMoney for analyzing the memory of the game while it was executing.\n\nI used OllyDbg to find what functions Starcraft would call during execution – from common functions like the game-loop, to highly specialized functions like checking supply limits. ArtMoney let us determine how structures like units were laid out – where each unit’s health was stored, how much damage each weapon would do, and more.\n\nMost of this information was shared in forums, chat rooms, and a few disparate sites for looking up hex addresses. Of course, no write up about modding Starcraft is complete without mentioning all the work from ShadowFlare (check out [ShadowFlare’s realm](https://sfsrealm.hopto.org/) for all the work they did), the great people at Staredit.Net, and plenty of others who built amazing tools to work with Starcraft files. All these sites were amazing resources for figuring out file specs and working with the Starcraft engine.\n\nHowever, each time I wanted to write a new mod or experiment with an idea, I’d have to look up all of this information across all of these sites.\n\n## Bringing it together\n\nThe goal with GPTP (General Plugin Template Project), was to take all of this work that the amazing modding community had done and bring it together into a C++ Visual Studio project that could be copied for a new mod.\n\nMy initial idea was simple: take all of the code for injecting new function hooks into Starcraft and wrap it in some very friendly functions. When a modder would come in to create a new project, they would have three functions exposed to them to work with: gameStart, gameLoop, gameEnd (I don’t remember the exact names I gave them at the time).\n\nI released the initial GPTP back in 2008-2009. This included the project setup for compiling and producing a QDP file that could be loaded into Starcraft. Additionally, it contained known structures and hex addresses that developers could use to build their mod. The Intellisense autocomplete feature really helped developers leverage these structures.\n\n## The power of open source\n\nI made the original version of GPTP back in high school, but I didn’t have time to continue to work on it when I went to university. I returned to the community a decade later, and found that not only was the community still alive and working on mods, but that they were using GPTP. At this point, GPTP was unrecognizable from my original work; the goal was the same, but the quality of the code was greatly improved and the number of hooks, known structures, and addresses was much more impressive.\n\nYou can view the project on Github: [general-plugin-template-project](https://github.com/SCMapsAndMods/general-plugin-template-project).\n\nI’d like to thank open source for this sort of development. The amount of work that has been continually added onto this project couldn’t have been done without all the contributions from the community.\n\n## Does it still work?\n\nI recently went back to Starcraft modding as a fun little project. I wanted to hop in and see how easy or difficult it would be to create a mod in 2020.\n\nOne of the harder things to do in 2020 is find all the tools and initial setup for modding Starcraft. Some of the modding sites are down for good, and with them the knowledge they contained.\n\nBut, I pulled together the tools (thanks PyMS), pulled together tutorials and other instructions into a Notion document, and created my first Starcraft mod!\n\nGathering all the tools was the hard part. After that, it was actually very straightforward creating a new GPTP template and mod. All of the known structures and hooks are pretty self-explanatory and I was able to even add my own hooks once I found the hex addresses using OllyDbg.\n\n## Looking forward\n\nStarcraft modding is like an ancient art at this point. There isn’t a large audience for it, so in general, any new mod will be played by maybe 10 people. But to me, it’s a combination of nostalgia, hard work, tinkering, discovery, and creation that makes it so much fun. That feeling of finding a new function to hook onto, writing C++ code that injects itself into it, and then running the code in the Starcraft engine and seeing it work is a special kind of rewarding experience.\n\n## Thanks\n\nI’d like to give thanks to everyone who has ever contributed to the modding community for Starcraft. There are so many names that I couldn’t possibly name them all, but please check of [http://staredit.net](http://staredit.net) if any of this interests you.\n","contentHTML":"<p>One of the first PC games I played was Starcraft and the expansion Starcraft: Broodwar. We didn’t have a PC, so I had to play it on a friend’s computer, but I remember being immersed in the real-time strategy gameplay.</p>\n<p>Later, I would get a PC and bought my own copy of Starcraft. This was years later, but the community for the game was still impressive. Joining one of the older Starcraft forums, I discovered “mods” for the game. The most popular ones were just graphical changes, but that concept of changing a game to display your own graphics was so interesting to me. I’m not sure how I stumbled on it, but I found <a href=\"http://www.staredit.net/\">http://www.staredit.net/</a> (yes, it’s still active!) and I learned that you could do so much more than just change the graphics of Starcraft with mods. There were people in the community working on hooking into existing Starcraft code to modify the gameplay and graphics using C++.</p>\n<h2>Diving deep</h2>\n<p>In order to create these mods, we needed to know the hex address of different functions that Starcraft would call during them game. And to do anything meaningful, we also needed to know the structure of units and sprites and where those were stored as well. A lot of this base work required using tools like OllyDbg to analyze the assembly of the Starcraft executable, and ArtMoney for analyzing the memory of the game while it was executing.</p>\n<p>I used OllyDbg to find what functions Starcraft would call during execution – from common functions like the game-loop, to highly specialized functions like checking supply limits. ArtMoney let us determine how structures like units were laid out – where each unit’s health was stored, how much damage each weapon would do, and more.</p>\n<p>Most of this information was shared in forums, chat rooms, and a few disparate sites for looking up hex addresses. Of course, no write up about modding Starcraft is complete without mentioning all the work from ShadowFlare (check out <a href=\"https://sfsrealm.hopto.org/\">ShadowFlare’s realm</a> for all the work they did), the great people at Staredit.Net, and plenty of others who built amazing tools to work with Starcraft files. All these sites were amazing resources for figuring out file specs and working with the Starcraft engine.</p>\n<p>However, each time I wanted to write a new mod or experiment with an idea, I’d have to look up all of this information across all of these sites.</p>\n<h2>Bringing it together</h2>\n<p>The goal with GPTP (General Plugin Template Project), was to take all of this work that the amazing modding community had done and bring it together into a C++ Visual Studio project that could be copied for a new mod.</p>\n<p>My initial idea was simple: take all of the code for injecting new function hooks into Starcraft and wrap it in some very friendly functions. When a modder would come in to create a new project, they would have three functions exposed to them to work with: gameStart, gameLoop, gameEnd (I don’t remember the exact names I gave them at the time).</p>\n<p>I released the initial GPTP back in 2008-2009. This included the project setup for compiling and producing a QDP file that could be loaded into Starcraft. Additionally, it contained known structures and hex addresses that developers could use to build their mod. The Intellisense autocomplete feature really helped developers leverage these structures.</p>\n<h2>The power of open source</h2>\n<p>I made the original version of GPTP back in high school, but I didn’t have time to continue to work on it when I went to university. I returned to the community a decade later, and found that not only was the community still alive and working on mods, but that they were using GPTP. At this point, GPTP was unrecognizable from my original work; the goal was the same, but the quality of the code was greatly improved and the number of hooks, known structures, and addresses was much more impressive.</p>\n<p>You can view the project on Github: <a href=\"https://github.com/SCMapsAndMods/general-plugin-template-project\">general-plugin-template-project</a>.</p>\n<p>I’d like to thank open source for this sort of development. The amount of work that has been continually added onto this project couldn’t have been done without all the contributions from the community.</p>\n<h2>Does it still work?</h2>\n<p>I recently went back to Starcraft modding as a fun little project. I wanted to hop in and see how easy or difficult it would be to create a mod in 2020.</p>\n<p>One of the harder things to do in 2020 is find all the tools and initial setup for modding Starcraft. Some of the modding sites are down for good, and with them the knowledge they contained.</p>\n<p>But, I pulled together the tools (thanks PyMS), pulled together tutorials and other instructions into a Notion document, and created my first Starcraft mod!</p>\n<p>Gathering all the tools was the hard part. After that, it was actually very straightforward creating a new GPTP template and mod. All of the known structures and hooks are pretty self-explanatory and I was able to even add my own hooks once I found the hex addresses using OllyDbg.</p>\n<h2>Looking forward</h2>\n<p>Starcraft modding is like an ancient art at this point. There isn’t a large audience for it, so in general, any new mod will be played by maybe 10 people. But to me, it’s a combination of nostalgia, hard work, tinkering, discovery, and creation that makes it so much fun. That feeling of finding a new function to hook onto, writing C++ code that injects itself into it, and then running the code in the Starcraft engine and seeing it work is a special kind of rewarding experience.</p>\n<h2>Thanks</h2>\n<p>I’d like to give thanks to everyone who has ever contributed to the modding community for Starcraft. There are so many names that I couldn’t possibly name them all, but please check of <a href=\"http://staredit.net\">http://staredit.net</a> if any of this interests you.</p>","contentCode":"\"use strict\";\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = {\n    a: \"a\",\n    h2: \"h2\",\n    p: \"p\",\n    ...props.components\n  };\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"One of the first PC games I played was Starcraft and the expansion Starcraft: Broodwar. We didn’t have a PC, so I had to play it on a friend’s computer, but I remember being immersed in the real-time strategy gameplay.\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Later, I would get a PC and bought my own copy of Starcraft. This was years later, but the community for the game was still impressive. Joining one of the older Starcraft forums, I discovered “mods” for the game. The most popular ones were just graphical changes, but that concept of changing a game to display your own graphics was so interesting to me. I’m not sure how I stumbled on it, but I found \", _jsx(_components.a, {\n        href: \"http://www.staredit.net/\",\n        children: \"http://www.staredit.net/\"\n      }), \" (yes, it’s still active!) and I learned that you could do so much more than just change the graphics of Starcraft with mods. There were people in the community working on hooking into existing Starcraft code to modify the gameplay and graphics using C++.\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Diving deep\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"In order to create these mods, we needed to know the hex address of different functions that Starcraft would call during them game. And to do anything meaningful, we also needed to know the structure of units and sprites and where those were stored as well. A lot of this base work required using tools like OllyDbg to analyze the assembly of the Starcraft executable, and ArtMoney for analyzing the memory of the game while it was executing.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"I used OllyDbg to find what functions Starcraft would call during execution – from common functions like the game-loop, to highly specialized functions like checking supply limits. ArtMoney let us determine how structures like units were laid out – where each unit’s health was stored, how much damage each weapon would do, and more.\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Most of this information was shared in forums, chat rooms, and a few disparate sites for looking up hex addresses. Of course, no write up about modding Starcraft is complete without mentioning all the work from ShadowFlare (check out \", _jsx(_components.a, {\n        href: \"https://sfsrealm.hopto.org/\",\n        children: \"ShadowFlare’s realm\"\n      }), \" for all the work they did), the great people at Staredit.Net, and plenty of others who built amazing tools to work with Starcraft files. All these sites were amazing resources for figuring out file specs and working with the Starcraft engine.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"However, each time I wanted to write a new mod or experiment with an idea, I’d have to look up all of this information across all of these sites.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Bringing it together\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The goal with GPTP (General Plugin Template Project), was to take all of this work that the amazing modding community had done and bring it together into a C++ Visual Studio project that could be copied for a new mod.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"My initial idea was simple: take all of the code for injecting new function hooks into Starcraft and wrap it in some very friendly functions. When a modder would come in to create a new project, they would have three functions exposed to them to work with: gameStart, gameLoop, gameEnd (I don’t remember the exact names I gave them at the time).\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"I released the initial GPTP back in 2008-2009. This included the project setup for compiling and producing a QDP file that could be loaded into Starcraft. Additionally, it contained known structures and hex addresses that developers could use to build their mod. The Intellisense autocomplete feature really helped developers leverage these structures.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"The power of open source\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"I made the original version of GPTP back in high school, but I didn’t have time to continue to work on it when I went to university. I returned to the community a decade later, and found that not only was the community still alive and working on mods, but that they were using GPTP. At this point, GPTP was unrecognizable from my original work; the goal was the same, but the quality of the code was greatly improved and the number of hooks, known structures, and addresses was much more impressive.\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"You can view the project on Github: \", _jsx(_components.a, {\n        href: \"https://github.com/SCMapsAndMods/general-plugin-template-project\",\n        children: \"general-plugin-template-project\"\n      }), \".\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"I’d like to thank open source for this sort of development. The amount of work that has been continually added onto this project couldn’t have been done without all the contributions from the community.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Does it still work?\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"I recently went back to Starcraft modding as a fun little project. I wanted to hop in and see how easy or difficult it would be to create a mod in 2020.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"One of the harder things to do in 2020 is find all the tools and initial setup for modding Starcraft. Some of the modding sites are down for good, and with them the knowledge they contained.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"But, I pulled together the tools (thanks PyMS), pulled together tutorials and other instructions into a Notion document, and created my first Starcraft mod!\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Gathering all the tools was the hard part. After that, it was actually very straightforward creating a new GPTP template and mod. All of the known structures and hooks are pretty self-explanatory and I was able to even add my own hooks once I found the hex addresses using OllyDbg.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Looking forward\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Starcraft modding is like an ancient art at this point. There isn’t a large audience for it, so in general, any new mod will be played by maybe 10 people. But to me, it’s a combination of nostalgia, hard work, tinkering, discovery, and creation that makes it so much fun. That feeling of finding a new function to hook onto, writing C++ code that injects itself into it, and then running the code in the Starcraft engine and seeing it work is a special kind of rewarding experience.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Thanks\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"I’d like to give thanks to everyone who has ever contributed to the modding community for Starcraft. There are so many names that I couldn’t possibly name them all, but please check of \", _jsx(_components.a, {\n        href: \"http://staredit.net\",\n        children: \"http://staredit.net\"\n      }), \" if any of this interests you.\"]\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, {\n    ...props,\n    children: _jsx(_createMdxContent, {\n      ...props\n    })\n  }) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","excerptRaw":"\nOne of the first PC games I played was Starcraft and the expansion Starcraft: Broodwar. We didn’t have a PC, so I had to play it on a friend’s computer, but I remember being immersed in the real-time strategy gameplay.","excerptHTML":"<p>One of the first PC games I played was Starcraft and the expansion Starcraft: Broodwar. We didn’t have a PC, so I had to play it on a friend’s computer, but I remember being immersed in the real-time strategy gameplay.</p>","excerptCode":"\"use strict\";\nconst {jsx: _jsx} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = {\n    p: \"p\",\n    ...props.components\n  };\n  return _jsx(_components.p, {\n    children: \"One of the first PC games I played was Starcraft and the expansion Starcraft: Broodwar. We didn’t have a PC, so I had to play it on a friend’s computer, but I remember being immersed in the real-time strategy gameplay.\"\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, {\n    ...props,\n    children: _jsx(_createMdxContent, {\n      ...props\n    })\n  }) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","tags":["starcraft","gptp","cplusplus"]},{"slug":"2023-04-02-dark-emblem-rewrite","date":"2023-04-02","title":"Dark Emblem Rewrite","frontmatter":{"title":"Dark Emblem Rewrite","tags":["dark emblem","wax","nft","blockchain"]},"contentRaw":"\nDark Emblem is an NFT project that was sparked by the idea of combining cards games with Crypto Kitties. You can buy packs, open them to collect random cards, and then use those cards to battle raids with others.\n\nTo support all of the functionality, I created a WAX contract, written in C++, that I deployed to the WAX Blockchain. This contract handled the creation and storage of packs and cards, along with defining an in-game currency DREM.\n\nThere was also a server that would sync with the changes on the Blockchain and provide some additional metadata on top of that. While Blockchains like ETH have stable public APIs that can be used to query data from the Blockchain through 3rd party vendors, WAX does not have the same maturity there. This meant that we would need to proxy requests that would normally be made to a WAX API mode through our servers instead.\n\nI also created a UI application to interact with the server and the Blockchain. The infrastructure ends up looking like:\n\n```mermaid\ngraph LR\n  WAX --> Dfuse\n Dfuse --> Listener(Dark Emblem Listener Service)\n  Listener --> Redis\n  Redis --> API(Dark Emblem API)\n  API --> Dapp\n  WAXEndpoint(WAX Endpoint) --> Dapp\n```\n\nOne of the first steps to interacting with the dapp is to sign in. The authentication flow is similar to an oauth sign in where we let the user sign in with a third party authenticator and then issue a challenge for the user to verify they are who they say they are. At this point we issue a JWT for further API requests.\n\n```mermaid\nsequenceDiagram\n  Dapp->>+3rdPartyAuthenticator: Request sign in\n  3rdPartyAuthenticator->>+Dapp: Wallet name\n  Dapp->>+DarkEmblemAPI: Request nonce\n  DarkEmblemAPI->>+Dapp: Send nonce\n  Dapp->>+3rdPartyAuthenticator: Sign nonce\n  3rdPartyAuthenticator->>+Dapp: Signed nonce\n  Dapp->>+DarkEmblemAPI: Send signed nonce\n  DarkEmblemAPI->>+Dapp: JWT\n```\n\nOnce a user is logged in, they can fully interact with the Dark Emblem WAX contract via the UI. They can purchase packs, open them to get cards, and then use those cards in the Dark Emblem universe. The UI lets users combine Hero cards together to “Ascend” them into a new card. Or they can burn 3 Equipment cards to “Transmogrify” them into a single, better equipment card.\n\nUsers can also participate in Raids, where they stake their cards to defeat an enemy monster – gaining $DREM and XP when they beat the monster.\n\nDapp Schemas and Atomic Assets\n\n```mermaid\nclassDiagram\n  class Card {\n    name: string\n  img: string\n  traits: number[]\n  matronid: number\n  sireid: number\n  rank: number\n  packid: string\n  cardtype: string\n  website: string\n  twitter: string\n  mintedat: number\n  rarity: string\n  cooldown: number\n  xp: number\n  }\n  class Raid {\n  asset_id: number\n  owner: name\n  staked_at: number\n  raid_id: number\n  }\n  class Staked {\n  asset_id: number\n  owner: name\n  staked_at: number\n  raid_id: number\n  }\n\n```\n\nThe Dark Emblem project involves a lot of moving parts, and I’d love to dive deeper into each technical aspect in future blog posts.\n\n## Additional reading\n\n* [Dark Emblem Blog Announcement](https://www.darkemblem.com/blog/post/2023-04-02-new-site-released)\n* [Capsule Cat Announcement](https://capsulecat.com/blog/04-02-2023-dark-emblem-update/)\n","contentHTML":"<p>Dark Emblem is an NFT project that was sparked by the idea of combining cards games with Crypto Kitties. You can buy packs, open them to collect random cards, and then use those cards to battle raids with others.</p>\n<p>To support all of the functionality, I created a WAX contract, written in C++, that I deployed to the WAX Blockchain. This contract handled the creation and storage of packs and cards, along with defining an in-game currency DREM.</p>\n<p>There was also a server that would sync with the changes on the Blockchain and provide some additional metadata on top of that. While Blockchains like ETH have stable public APIs that can be used to query data from the Blockchain through 3rd party vendors, WAX does not have the same maturity there. This meant that we would need to proxy requests that would normally be made to a WAX API mode through our servers instead.</p>\n<p>I also created a UI application to interact with the server and the Blockchain. The infrastructure ends up looking like:</p>\n<mermaid chart=\"graph LR\n  WAX --&gt; Dfuse\n Dfuse --&gt; Listener(Dark Emblem Listener Service)\n  Listener --&gt; Redis\n  Redis --&gt; API(Dark Emblem API)\n  API --&gt; Dapp\n  WAXEndpoint(WAX Endpoint) --&gt; Dapp\"></mermaid>\n<p>One of the first steps to interacting with the dapp is to sign in. The authentication flow is similar to an oauth sign in where we let the user sign in with a third party authenticator and then issue a challenge for the user to verify they are who they say they are. At this point we issue a JWT for further API requests.</p>\n<mermaid chart=\"sequenceDiagram\n  Dapp-&gt;&gt;+3rdPartyAuthenticator: Request sign in\n  3rdPartyAuthenticator-&gt;&gt;+Dapp: Wallet name\n  Dapp-&gt;&gt;+DarkEmblemAPI: Request nonce\n  DarkEmblemAPI-&gt;&gt;+Dapp: Send nonce\n  Dapp-&gt;&gt;+3rdPartyAuthenticator: Sign nonce\n  3rdPartyAuthenticator-&gt;&gt;+Dapp: Signed nonce\n  Dapp-&gt;&gt;+DarkEmblemAPI: Send signed nonce\n  DarkEmblemAPI-&gt;&gt;+Dapp: JWT\"></mermaid>\n<p>Once a user is logged in, they can fully interact with the Dark Emblem WAX contract via the UI. They can purchase packs, open them to get cards, and then use those cards in the Dark Emblem universe. The UI lets users combine Hero cards together to “Ascend” them into a new card. Or they can burn 3 Equipment cards to “Transmogrify” them into a single, better equipment card.</p>\n<p>Users can also participate in Raids, where they stake their cards to defeat an enemy monster – gaining $DREM and XP when they beat the monster.</p>\n<p>Dapp Schemas and Atomic Assets</p>\n<mermaid chart=\"classDiagram\n  class Card {\n    name: string\n  img: string\n  traits: number[]\n  matronid: number\n  sireid: number\n  rank: number\n  packid: string\n  cardtype: string\n  website: string\n  twitter: string\n  mintedat: number\n  rarity: string\n  cooldown: number\n  xp: number\n  }\n  class Raid {\n  asset_id: number\n  owner: name\n  staked_at: number\n  raid_id: number\n  }\n  class Staked {\n  asset_id: number\n  owner: name\n  staked_at: number\n  raid_id: number\n  }\n\"></mermaid>\n<p>The Dark Emblem project involves a lot of moving parts, and I’d love to dive deeper into each technical aspect in future blog posts.</p>\n<h2>Additional reading</h2>\n<ul>\n<li><a href=\"https://www.darkemblem.com/blog/post/2023-04-02-new-site-released\">Dark Emblem Blog Announcement</a></li>\n<li><a href=\"https://capsulecat.com/blog/04-02-2023-dark-emblem-update/\">Capsule Cat Announcement</a></li>\n</ul>","contentCode":"\"use strict\";\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = {\n    a: \"a\",\n    h2: \"h2\",\n    li: \"li\",\n    mermaid: \"mermaid\",\n    p: \"p\",\n    ul: \"ul\",\n    ...props.components\n  };\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"Dark Emblem is an NFT project that was sparked by the idea of combining cards games with Crypto Kitties. You can buy packs, open them to collect random cards, and then use those cards to battle raids with others.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"To support all of the functionality, I created a WAX contract, written in C++, that I deployed to the WAX Blockchain. This contract handled the creation and storage of packs and cards, along with defining an in-game currency DREM.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"There was also a server that would sync with the changes on the Blockchain and provide some additional metadata on top of that. While Blockchains like ETH have stable public APIs that can be used to query data from the Blockchain through 3rd party vendors, WAX does not have the same maturity there. This meant that we would need to proxy requests that would normally be made to a WAX API mode through our servers instead.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"I also created a UI application to interact with the server and the Blockchain. The infrastructure ends up looking like:\"\n    }), \"\\n\", _jsx(_components.mermaid, {\n      chart: \"graph LR\\n  WAX --> Dfuse\\n Dfuse --> Listener(Dark Emblem Listener Service)\\n  Listener --> Redis\\n  Redis --> API(Dark Emblem API)\\n  API --> Dapp\\n  WAXEndpoint(WAX Endpoint) --> Dapp\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"One of the first steps to interacting with the dapp is to sign in. The authentication flow is similar to an oauth sign in where we let the user sign in with a third party authenticator and then issue a challenge for the user to verify they are who they say they are. At this point we issue a JWT for further API requests.\"\n    }), \"\\n\", _jsx(_components.mermaid, {\n      chart: \"sequenceDiagram\\n  Dapp->>+3rdPartyAuthenticator: Request sign in\\n  3rdPartyAuthenticator->>+Dapp: Wallet name\\n  Dapp->>+DarkEmblemAPI: Request nonce\\n  DarkEmblemAPI->>+Dapp: Send nonce\\n  Dapp->>+3rdPartyAuthenticator: Sign nonce\\n  3rdPartyAuthenticator->>+Dapp: Signed nonce\\n  Dapp->>+DarkEmblemAPI: Send signed nonce\\n  DarkEmblemAPI->>+Dapp: JWT\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Once a user is logged in, they can fully interact with the Dark Emblem WAX contract via the UI. They can purchase packs, open them to get cards, and then use those cards in the Dark Emblem universe. The UI lets users combine Hero cards together to “Ascend” them into a new card. Or they can burn 3 Equipment cards to “Transmogrify” them into a single, better equipment card.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Users can also participate in Raids, where they stake their cards to defeat an enemy monster – gaining $DREM and XP when they beat the monster.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Dapp Schemas and Atomic Assets\"\n    }), \"\\n\", _jsx(_components.mermaid, {\n      chart: \"classDiagram\\n  class Card {\\n    name: string\\n  img: string\\n  traits: number[]\\n  matronid: number\\n  sireid: number\\n  rank: number\\n  packid: string\\n  cardtype: string\\n  website: string\\n  twitter: string\\n  mintedat: number\\n  rarity: string\\n  cooldown: number\\n  xp: number\\n  }\\n  class Raid {\\n  asset_id: number\\n  owner: name\\n  staked_at: number\\n  raid_id: number\\n  }\\n  class Staked {\\n  asset_id: number\\n  owner: name\\n  staked_at: number\\n  raid_id: number\\n  }\\n\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The Dark Emblem project involves a lot of moving parts, and I’d love to dive deeper into each technical aspect in future blog posts.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Additional reading\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: _jsx(_components.a, {\n          href: \"https://www.darkemblem.com/blog/post/2023-04-02-new-site-released\",\n          children: \"Dark Emblem Blog Announcement\"\n        })\n      }), \"\\n\", _jsx(_components.li, {\n        children: _jsx(_components.a, {\n          href: \"https://capsulecat.com/blog/04-02-2023-dark-emblem-update/\",\n          children: \"Capsule Cat Announcement\"\n        })\n      }), \"\\n\"]\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, {\n    ...props,\n    children: _jsx(_createMdxContent, {\n      ...props\n    })\n  }) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","excerptRaw":"\nDark Emblem is an NFT project that was sparked by the idea of combining cards games with Crypto Kitties. You can buy packs, open them to collect random cards, and then use those cards to battle raids with others.","excerptHTML":"<p>Dark Emblem is an NFT project that was sparked by the idea of combining cards games with Crypto Kitties. You can buy packs, open them to collect random cards, and then use those cards to battle raids with others.</p>","excerptCode":"\"use strict\";\nconst {jsx: _jsx} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = {\n    p: \"p\",\n    ...props.components\n  };\n  return _jsx(_components.p, {\n    children: \"Dark Emblem is an NFT project that was sparked by the idea of combining cards games with Crypto Kitties. You can buy packs, open them to collect random cards, and then use those cards to battle raids with others.\"\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, {\n    ...props,\n    children: _jsx(_createMdxContent, {\n      ...props\n    })\n  }) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","tags":["dark emblem","wax","nft","blockchain"]},{"slug":"2023-01-24-chat-gpt-doesnt-understand","date":"2023-01-24","title":"ChatGPT Doesn't Understand","frontmatter":{"title":"ChatGPT Doesn't Understand","tags":["openai","chatgpt"]},"contentRaw":"\nLooks like everyone is trying to discover ChatGPT’s limitations. In one article I read, the author asked whether [ChatGPT can play 20 questions](https://medium.com/@theworldaccordingtocgpt/chatgpt-can-play-20-questions-7911405f7aff). The human in this scenario tried to play 20 questions with ChatGPT, and found that they needed to guide the AI to ask questions. From my perspective, it looked more like a failure to engineer the initial prompt correctly.\n\nI attempted to do 20 questions with ChatGPT using a different prompt:\n\n*Me: I would like to play a game. It's called twenty questions. You will ask me one question at a time and I will answer it. The goal is for you guess the thing I am thinking about.*\n\n*ChatGPT: Sure, I'd be happy to play \"Twenty Questions\" with you. Please go ahead and think of something, and I will start asking my first question.*\n\n*Me: I am ready*\n\n*ChatGPT: Great, here is my first question: Is the thing you are thinking of an object?*\n\n*Me: Yes*\n\n*ChatGPT: Is the object something that you can hold in your hand?*\n\n*Me: Yes*\n\n*ChatGPT: Is the object something that you would typically find in a kitchen?*\n\n*Me: No*\n\n*ChatGPT: Is the object something that you would use for transportation?*\n\n*Me: No*\n\n*ChatGPT: Is the object something that you would use for entertainment or leisure?*\n\n*Me: Yes*\n\n*ChatGPT: Is the object a musical instrument?*\n\n*Me: No*\n\n*ChatGPT: Is the object a type of game or puzzle?*\n\n*Me: No*\n\n*ChatGPT: Is the object a piece of technology such as a phone or computer?*\n\n*Me: No*\n\n*ChatGPT: Is the object a book or magazine?*\n\n*Me: Yes*\n\n*ChatGPT: Is the object a novel or a collection of short stories?*\n\n*Me: Yes*\n\n*ChatGPT: Is the object a specific book that is widely known and read?*\n\n*Me: Yes*\n\n*ChatGPT: Is the object \"To Kill a Mockingbird\" by Harper Lee?*\n\n*Me: No*\n\n*ChatGPT: I'm sorry, I'm running out of ideas. Can you give me a hint or tell me the answer?*\n\nSeems like we hit a wall here with twenty questions, but ChatGPT is very close, so I decided to give a hint.\n\n*Me: You are on the right track. It is by Walt Whitman*\n\n*ChatGPT: Is the object \"Leaves of Grass\" by Walt Whitman?*\n\n*Me: Yes! Congratulations!*\n\n*ChatGPT: Thank you! I'm glad I was able to guess the object you were thinking of. It was fun playing \"Twenty Questions\" with you. Let me know if you want to play again sometime.*\n\n---\n\nI’ve been keeping an eye on how other people are using ChatGPT, and [prompt engineering](https://en.wikipedia.org/wiki/Prompt_engineering) is one of the more fascinating aspects around how people are communicating with the AI. Prompt engineering plays a key role in communicating with models like ChatGPT. Carefully crafting prompts can help maximize the quality of the reponse. However, it is important to keep in mind that, like any AI model, ChatGPT has its own set of limitations.\n\nSo I was curious when I read that “[ChatGPT can’t write sentences that end in a specific letter](https://www.reddit.com/r/ChatGPT/comments/101e454/chatgpt_cant_write_sentences_that_end_with_a/)”. Is this a real limitation of ChatGPT, or maybe this was just a poor prompt.\n\nSo let’s start with a very simple question to ChatGPT:\n\n*Me: Can you write a sentence that ends with the letter \"a\"?*\n\n*ChatGPT: The cat sat on the mat.*\n\n*Me: That sentence does not end in the letter \"a\". Can you write a sentence that ends in the letter \"a\"?*\n\n*ChatGPT: I can't think of anything right now, but the word \"America\" ends with letter \"a\".*\n\n*Me: Great, write a sentence that ends in that word. That will then satisfy the original question.*\n\n*ChatGPT: The land of the free, and the home of the brave, America.*\n\nOkay, that does technically end in the letter a, but the sentence seems very strange.\n\n*Me: That isn't a very coherent sentence. Can you write a sentence that ends in the letter \"a\"?*\n\n*ChatGPT: I am sorry for the confusion, here is a sentence that ends with the letter \"a\" - \"He saw the drama unfold on the stage, it was quite the spectacle, a.”*\n\nLooks like we are going off the rails here. Let’s try a different approach:\n\n*Me: Write a sentence that ends in a word that ends with the letter \"a\".*\n\n*ChatGPT: The sun was setting over the horizon, painting the sky with shades of orange and red, a spectacle to behold and end the day with.*\n\nI’m going to ask ChatGPT to tell me a prompt to feed back to itself. I’ve found this useful for resetting context when having long conversations. Or if ChatGPT gets into weird states like the above response.\n\n*Me: Can you tell me how to write a prompt for you that would actually get you to write a sentence that ends in the letter \"a\".*\n\n*ChatGPT: Sure, to get me to write a sentence that ends in the letter \"a\" you could provide a prompt like: \"Write a sentence that ends with a word that ends with the letter 'a'\" or \"Can you write a sentence with a word ending in 'a' at the end of the sentence?\" . This makes it clear that the sentence should end in a word ending with the letter 'a'.*\n\n*Me: Can you write a sentence with a word ending in 'a' at the end of the sentence?*\n\n*ChatGPT: The sky was clear and blue, not a cloud in sight, perfect for a day out at the parka.*\n\n---\n\nWhile ChatGPT is great at forming sentences and even code, it seems like it has no understanding and hasn’t been trained on letters, syllables, or the concept of sentences. I've seen others get ChatGPT to create poems, but it has no concept of why words would rhyme. It can give great answers to questions, but doesn’t understand how to count how many words are in its response.\n\nOne Redditor in the link above mentions that ChatGPT fails at ASCII art. I’ve tried getting the AI to print ASCII art before, and I’ll leave you with part of a conversation I’ve had with it. This appears to be the best ASCII art I could get it to print, even after having a long conversation where I provided different ASCII art examples:\n\n*Me: Can you make ASCII art? Show me a picture of a cat.*\n\n*ChatGPT: Sure! Here is a picture of a cat using ASCII art:*\n\n```\n /\\_/\\\n( o.o )\n >^<\n\n```\n\n*I hope you like it! Is there anything else I can help with?*\n\n*Me: Can you make it more detailed*\n\n*ChatGPT: Certainly! Here is a more detailed version of an ASCII art cat:*\n\n```\n /\\_/\\\n( o.o )\n  >^<\n  | |\n  || ||\n  \\\\ //\n  // \\\\\n  \\\\_//\n  /   \\\n /     \\\n\n```\n\nWith new language models coming out, it's important to recognize what the limitations\nof the AI are. Poor responses can be a result of poorly written prompts, but there\nare times, like with letters in words with ChatGPT, where the AI just doesn't have\nthe capability to understand the question.\n","contentHTML":"<p>Looks like everyone is trying to discover ChatGPT’s limitations. In one article I read, the author asked whether <a href=\"https://medium.com/@theworldaccordingtocgpt/chatgpt-can-play-20-questions-7911405f7aff\">ChatGPT can play 20 questions</a>. The human in this scenario tried to play 20 questions with ChatGPT, and found that they needed to guide the AI to ask questions. From my perspective, it looked more like a failure to engineer the initial prompt correctly.</p>\n<p>I attempted to do 20 questions with ChatGPT using a different prompt:</p>\n<p><em>Me: I would like to play a game. It&#x27;s called twenty questions. You will ask me one question at a time and I will answer it. The goal is for you guess the thing I am thinking about.</em></p>\n<p><em>ChatGPT: Sure, I&#x27;d be happy to play &quot;Twenty Questions&quot; with you. Please go ahead and think of something, and I will start asking my first question.</em></p>\n<p><em>Me: I am ready</em></p>\n<p><em>ChatGPT: Great, here is my first question: Is the thing you are thinking of an object?</em></p>\n<p><em>Me: Yes</em></p>\n<p><em>ChatGPT: Is the object something that you can hold in your hand?</em></p>\n<p><em>Me: Yes</em></p>\n<p><em>ChatGPT: Is the object something that you would typically find in a kitchen?</em></p>\n<p><em>Me: No</em></p>\n<p><em>ChatGPT: Is the object something that you would use for transportation?</em></p>\n<p><em>Me: No</em></p>\n<p><em>ChatGPT: Is the object something that you would use for entertainment or leisure?</em></p>\n<p><em>Me: Yes</em></p>\n<p><em>ChatGPT: Is the object a musical instrument?</em></p>\n<p><em>Me: No</em></p>\n<p><em>ChatGPT: Is the object a type of game or puzzle?</em></p>\n<p><em>Me: No</em></p>\n<p><em>ChatGPT: Is the object a piece of technology such as a phone or computer?</em></p>\n<p><em>Me: No</em></p>\n<p><em>ChatGPT: Is the object a book or magazine?</em></p>\n<p><em>Me: Yes</em></p>\n<p><em>ChatGPT: Is the object a novel or a collection of short stories?</em></p>\n<p><em>Me: Yes</em></p>\n<p><em>ChatGPT: Is the object a specific book that is widely known and read?</em></p>\n<p><em>Me: Yes</em></p>\n<p><em>ChatGPT: Is the object &quot;To Kill a Mockingbird&quot; by Harper Lee?</em></p>\n<p><em>Me: No</em></p>\n<p><em>ChatGPT: I&#x27;m sorry, I&#x27;m running out of ideas. Can you give me a hint or tell me the answer?</em></p>\n<p>Seems like we hit a wall here with twenty questions, but ChatGPT is very close, so I decided to give a hint.</p>\n<p><em>Me: You are on the right track. It is by Walt Whitman</em></p>\n<p><em>ChatGPT: Is the object &quot;Leaves of Grass&quot; by Walt Whitman?</em></p>\n<p><em>Me: Yes! Congratulations!</em></p>\n<p><em>ChatGPT: Thank you! I&#x27;m glad I was able to guess the object you were thinking of. It was fun playing &quot;Twenty Questions&quot; with you. Let me know if you want to play again sometime.</em></p>\n<hr/>\n<p>I’ve been keeping an eye on how other people are using ChatGPT, and <a href=\"https://en.wikipedia.org/wiki/Prompt_engineering\">prompt engineering</a> is one of the more fascinating aspects around how people are communicating with the AI. Prompt engineering plays a key role in communicating with models like ChatGPT. Carefully crafting prompts can help maximize the quality of the reponse. However, it is important to keep in mind that, like any AI model, ChatGPT has its own set of limitations.</p>\n<p>So I was curious when I read that “<a href=\"https://www.reddit.com/r/ChatGPT/comments/101e454/chatgpt_cant_write_sentences_that_end_with_a/\">ChatGPT can’t write sentences that end in a specific letter</a>”. Is this a real limitation of ChatGPT, or maybe this was just a poor prompt.</p>\n<p>So let’s start with a very simple question to ChatGPT:</p>\n<p><em>Me: Can you write a sentence that ends with the letter &quot;a&quot;?</em></p>\n<p><em>ChatGPT: The cat sat on the mat.</em></p>\n<p><em>Me: That sentence does not end in the letter &quot;a&quot;. Can you write a sentence that ends in the letter &quot;a&quot;?</em></p>\n<p><em>ChatGPT: I can&#x27;t think of anything right now, but the word &quot;America&quot; ends with letter &quot;a&quot;.</em></p>\n<p><em>Me: Great, write a sentence that ends in that word. That will then satisfy the original question.</em></p>\n<p><em>ChatGPT: The land of the free, and the home of the brave, America.</em></p>\n<p>Okay, that does technically end in the letter a, but the sentence seems very strange.</p>\n<p><em>Me: That isn&#x27;t a very coherent sentence. Can you write a sentence that ends in the letter &quot;a&quot;?</em></p>\n<p><em>ChatGPT: I am sorry for the confusion, here is a sentence that ends with the letter &quot;a&quot; - &quot;He saw the drama unfold on the stage, it was quite the spectacle, a.”</em></p>\n<p>Looks like we are going off the rails here. Let’s try a different approach:</p>\n<p><em>Me: Write a sentence that ends in a word that ends with the letter &quot;a&quot;.</em></p>\n<p><em>ChatGPT: The sun was setting over the horizon, painting the sky with shades of orange and red, a spectacle to behold and end the day with.</em></p>\n<p>I’m going to ask ChatGPT to tell me a prompt to feed back to itself. I’ve found this useful for resetting context when having long conversations. Or if ChatGPT gets into weird states like the above response.</p>\n<p><em>Me: Can you tell me how to write a prompt for you that would actually get you to write a sentence that ends in the letter &quot;a&quot;.</em></p>\n<p><em>ChatGPT: Sure, to get me to write a sentence that ends in the letter &quot;a&quot; you could provide a prompt like: &quot;Write a sentence that ends with a word that ends with the letter &#x27;a&#x27;&quot; or &quot;Can you write a sentence with a word ending in &#x27;a&#x27; at the end of the sentence?&quot; . This makes it clear that the sentence should end in a word ending with the letter &#x27;a&#x27;.</em></p>\n<p><em>Me: Can you write a sentence with a word ending in &#x27;a&#x27; at the end of the sentence?</em></p>\n<p><em>ChatGPT: The sky was clear and blue, not a cloud in sight, perfect for a day out at the parka.</em></p>\n<hr/>\n<p>While ChatGPT is great at forming sentences and even code, it seems like it has no understanding and hasn’t been trained on letters, syllables, or the concept of sentences. I&#x27;ve seen others get ChatGPT to create poems, but it has no concept of why words would rhyme. It can give great answers to questions, but doesn’t understand how to count how many words are in its response.</p>\n<p>One Redditor in the link above mentions that ChatGPT fails at ASCII art. I’ve tried getting the AI to print ASCII art before, and I’ll leave you with part of a conversation I’ve had with it. This appears to be the best ASCII art I could get it to print, even after having a long conversation where I provided different ASCII art examples:</p>\n<p><em>Me: Can you make ASCII art? Show me a picture of a cat.</em></p>\n<p><em>ChatGPT: Sure! Here is a picture of a cat using ASCII art:</em></p>\n<div class=\"overflow-auto rounded bg-gray-200 p-4 font-mono text-sm dark:bg-gray-800 dark:text-gray-100\"><pre><code> /\\_/\\\n( o.o )\n &gt;^&lt;\n\n</code></pre></div>\n<p><em>I hope you like it! Is there anything else I can help with?</em></p>\n<p><em>Me: Can you make it more detailed</em></p>\n<p><em>ChatGPT: Certainly! Here is a more detailed version of an ASCII art cat:</em></p>\n<div class=\"overflow-auto rounded bg-gray-200 p-4 font-mono text-sm dark:bg-gray-800 dark:text-gray-100\"><pre><code> /\\_/\\\n( o.o )\n  &gt;^&lt;\n  | |\n  || ||\n  \\\\ //\n  // \\\\\n  \\\\_//\n  /   \\\n /     \\\n\n</code></pre></div>\n<p>With new language models coming out, it&#x27;s important to recognize what the limitations\nof the AI are. Poor responses can be a result of poorly written prompts, but there\nare times, like with letters in words with ChatGPT, where the AI just doesn&#x27;t have\nthe capability to understand the question.</p>","contentCode":"\"use strict\";\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = {\n    a: \"a\",\n    code: \"code\",\n    em: \"em\",\n    hr: \"hr\",\n    p: \"p\",\n    pre: \"pre\",\n    ...props.components\n  };\n  return _jsxs(_Fragment, {\n    children: [_jsxs(_components.p, {\n      children: [\"Looks like everyone is trying to discover ChatGPT’s limitations. In one article I read, the author asked whether \", _jsx(_components.a, {\n        href: \"https://medium.com/@theworldaccordingtocgpt/chatgpt-can-play-20-questions-7911405f7aff\",\n        children: \"ChatGPT can play 20 questions\"\n      }), \". The human in this scenario tried to play 20 questions with ChatGPT, and found that they needed to guide the AI to ask questions. From my perspective, it looked more like a failure to engineer the initial prompt correctly.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"I attempted to do 20 questions with ChatGPT using a different prompt:\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"Me: I would like to play a game. It's called twenty questions. You will ask me one question at a time and I will answer it. The goal is for you guess the thing I am thinking about.\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"ChatGPT: Sure, I'd be happy to play \\\"Twenty Questions\\\" with you. Please go ahead and think of something, and I will start asking my first question.\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"Me: I am ready\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"ChatGPT: Great, here is my first question: Is the thing you are thinking of an object?\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"Me: Yes\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"ChatGPT: Is the object something that you can hold in your hand?\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"Me: Yes\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"ChatGPT: Is the object something that you would typically find in a kitchen?\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"Me: No\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"ChatGPT: Is the object something that you would use for transportation?\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"Me: No\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"ChatGPT: Is the object something that you would use for entertainment or leisure?\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"Me: Yes\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"ChatGPT: Is the object a musical instrument?\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"Me: No\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"ChatGPT: Is the object a type of game or puzzle?\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"Me: No\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"ChatGPT: Is the object a piece of technology such as a phone or computer?\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"Me: No\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"ChatGPT: Is the object a book or magazine?\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"Me: Yes\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"ChatGPT: Is the object a novel or a collection of short stories?\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"Me: Yes\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"ChatGPT: Is the object a specific book that is widely known and read?\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"Me: Yes\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"ChatGPT: Is the object \\\"To Kill a Mockingbird\\\" by Harper Lee?\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"Me: No\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"ChatGPT: I'm sorry, I'm running out of ideas. Can you give me a hint or tell me the answer?\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Seems like we hit a wall here with twenty questions, but ChatGPT is very close, so I decided to give a hint.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"Me: You are on the right track. It is by Walt Whitman\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"ChatGPT: Is the object \\\"Leaves of Grass\\\" by Walt Whitman?\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"Me: Yes! Congratulations!\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"ChatGPT: Thank you! I'm glad I was able to guess the object you were thinking of. It was fun playing \\\"Twenty Questions\\\" with you. Let me know if you want to play again sometime.\"\n      })\n    }), \"\\n\", _jsx(_components.hr, {}), \"\\n\", _jsxs(_components.p, {\n      children: [\"I’ve been keeping an eye on how other people are using ChatGPT, and \", _jsx(_components.a, {\n        href: \"https://en.wikipedia.org/wiki/Prompt_engineering\",\n        children: \"prompt engineering\"\n      }), \" is one of the more fascinating aspects around how people are communicating with the AI. Prompt engineering plays a key role in communicating with models like ChatGPT. Carefully crafting prompts can help maximize the quality of the reponse. However, it is important to keep in mind that, like any AI model, ChatGPT has its own set of limitations.\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"So I was curious when I read that “\", _jsx(_components.a, {\n        href: \"https://www.reddit.com/r/ChatGPT/comments/101e454/chatgpt_cant_write_sentences_that_end_with_a/\",\n        children: \"ChatGPT can’t write sentences that end in a specific letter\"\n      }), \"”. Is this a real limitation of ChatGPT, or maybe this was just a poor prompt.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"So let’s start with a very simple question to ChatGPT:\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"Me: Can you write a sentence that ends with the letter \\\"a\\\"?\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"ChatGPT: The cat sat on the mat.\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"Me: That sentence does not end in the letter \\\"a\\\". Can you write a sentence that ends in the letter \\\"a\\\"?\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"ChatGPT: I can't think of anything right now, but the word \\\"America\\\" ends with letter \\\"a\\\".\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"Me: Great, write a sentence that ends in that word. That will then satisfy the original question.\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"ChatGPT: The land of the free, and the home of the brave, America.\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Okay, that does technically end in the letter a, but the sentence seems very strange.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"Me: That isn't a very coherent sentence. Can you write a sentence that ends in the letter \\\"a\\\"?\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"ChatGPT: I am sorry for the confusion, here is a sentence that ends with the letter \\\"a\\\" - \\\"He saw the drama unfold on the stage, it was quite the spectacle, a.”\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Looks like we are going off the rails here. Let’s try a different approach:\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"Me: Write a sentence that ends in a word that ends with the letter \\\"a\\\".\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"ChatGPT: The sun was setting over the horizon, painting the sky with shades of orange and red, a spectacle to behold and end the day with.\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"I’m going to ask ChatGPT to tell me a prompt to feed back to itself. I’ve found this useful for resetting context when having long conversations. Or if ChatGPT gets into weird states like the above response.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"Me: Can you tell me how to write a prompt for you that would actually get you to write a sentence that ends in the letter \\\"a\\\".\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"ChatGPT: Sure, to get me to write a sentence that ends in the letter \\\"a\\\" you could provide a prompt like: \\\"Write a sentence that ends with a word that ends with the letter 'a'\\\" or \\\"Can you write a sentence with a word ending in 'a' at the end of the sentence?\\\" . This makes it clear that the sentence should end in a word ending with the letter 'a'.\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"Me: Can you write a sentence with a word ending in 'a' at the end of the sentence?\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"ChatGPT: The sky was clear and blue, not a cloud in sight, perfect for a day out at the parka.\"\n      })\n    }), \"\\n\", _jsx(_components.hr, {}), \"\\n\", _jsx(_components.p, {\n      children: \"While ChatGPT is great at forming sentences and even code, it seems like it has no understanding and hasn’t been trained on letters, syllables, or the concept of sentences. I've seen others get ChatGPT to create poems, but it has no concept of why words would rhyme. It can give great answers to questions, but doesn’t understand how to count how many words are in its response.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"One Redditor in the link above mentions that ChatGPT fails at ASCII art. I’ve tried getting the AI to print ASCII art before, and I’ll leave you with part of a conversation I’ve had with it. This appears to be the best ASCII art I could get it to print, even after having a long conversation where I provided different ASCII art examples:\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"Me: Can you make ASCII art? Show me a picture of a cat.\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"ChatGPT: Sure! Here is a picture of a cat using ASCII art:\"\n      })\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        children: \" /\\\\_/\\\\\\n( o.o )\\n >^<\\n\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"I hope you like it! Is there anything else I can help with?\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"Me: Can you make it more detailed\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.em, {\n        children: \"ChatGPT: Certainly! Here is a more detailed version of an ASCII art cat:\"\n      })\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        children: \" /\\\\_/\\\\\\n( o.o )\\n  >^<\\n  | |\\n  || ||\\n  \\\\\\\\ //\\n  // \\\\\\\\\\n  \\\\\\\\_//\\n  /   \\\\\\n /     \\\\\\n\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"With new language models coming out, it's important to recognize what the limitations\\nof the AI are. Poor responses can be a result of poorly written prompts, but there\\nare times, like with letters in words with ChatGPT, where the AI just doesn't have\\nthe capability to understand the question.\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, {\n    ...props,\n    children: _jsx(_createMdxContent, {\n      ...props\n    })\n  }) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","excerptRaw":"\nLooks like everyone is trying to discover ChatGPT’s limitations. In one article I read, the author asked whether [ChatGPT can play 20 questions](https://medium.com/@theworldaccordingtocgpt/chatgpt-can-play-20-questions-7911405f7aff). The human in this scenario tried to play 20 questions with ChatGPT, and found that they needed to guide the AI to ask questions. From my perspective, it looked more like a failure to engineer the initial prompt correctly.","excerptHTML":"<p>Looks like everyone is trying to discover ChatGPT’s limitations. In one article I read, the author asked whether <a href=\"https://medium.com/@theworldaccordingtocgpt/chatgpt-can-play-20-questions-7911405f7aff\">ChatGPT can play 20 questions</a>. The human in this scenario tried to play 20 questions with ChatGPT, and found that they needed to guide the AI to ask questions. From my perspective, it looked more like a failure to engineer the initial prompt correctly.</p>","excerptCode":"\"use strict\";\nconst {jsx: _jsx, jsxs: _jsxs} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = {\n    a: \"a\",\n    p: \"p\",\n    ...props.components\n  };\n  return _jsxs(_components.p, {\n    children: [\"Looks like everyone is trying to discover ChatGPT’s limitations. In one article I read, the author asked whether \", _jsx(_components.a, {\n      href: \"https://medium.com/@theworldaccordingtocgpt/chatgpt-can-play-20-questions-7911405f7aff\",\n      children: \"ChatGPT can play 20 questions\"\n    }), \". The human in this scenario tried to play 20 questions with ChatGPT, and found that they needed to guide the AI to ask questions. From my perspective, it looked more like a failure to engineer the initial prompt correctly.\"]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, {\n    ...props,\n    children: _jsx(_createMdxContent, {\n      ...props\n    })\n  }) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","tags":["openai","chatgpt"]},{"slug":"2023-01-15-hierarchy-of-webapp-needs","date":"2023-01-15","title":"The Hierarchy of Webapp Needs","frontmatter":{"title":"The Hierarchy of Webapp Needs","tags":["webapp","checklist"]},"contentRaw":"\nI was thinking about all the little projects I work on and how they grow over time. The applications end up hitting some milestones and end up needing similar functionality that compliments the core features. A small project ends up getting complex enough that it requires some unit tests. I’ll go to deploy the project and now I need some deployment scripts and analytics to ensure the application is running correctly. The technology may change between each project, but web applications always seem to have the same steps that need to be taken to strengthen the application as it is scaled up.\n\nAt a large company, adding a new set of functionality always has a suite of concerns to think through before implementation: how will we deploy this feature, how do we validate that users are using the feature like we expected them to, how do we monitor for bugs and errors? This is on top of the basic functionality of actually writing and testing that new feature.\n\nWhat if we thought about this like Maslow’s Hierarchy of Needs, but in the context of a web application.\n\nMaslow's Hierarchy of Needs is a psychological model of human motivation proposed by Abraham Maslow in 1943. The model describes a hierarchy of human needs, beginning with basic physiological needs such as food and shelter and progressing upwards to higher-level needs such as self-actualization. Maslow argued that as humans satisfy their basic needs, they can move on to satisfy their higher-level needs.\n\nApplying Maslow's Hierarchy of Needs to web applications, we can identify the different levels of needs that need to be met in order to make a web application successful.\n\nBasic functionality:\n\n- A single build/run script\n- Basic functionality (e.g. CRUD operations for a web app)\n- User interface (e.g. layout, navigation, responsive design)\n- Integration with external services (e.g. databases, APIs)\n\nSafety and security:\n\n- Linting (e.g. ESLint, Prettier)\n- Unit tests (e.g. Jest, Mocha)\n- Basic error handling and reporting (e.g. logging, alerting)\n- Input validation (e.g. form validation)\n- Security best practices (e.g. encryption, password hashing, session management)\n\nLove and belonging:\n\n- Basic analytics (e.g. page views, user engagement)\n- User authentication and authorization (e.g. sign-up, login, role-based access control)\n- User feedback (e.g. contact form, survey)\n- Social media integration (e.g. sharing, commenting)\n\nEsteem:\n\n- Advanced analytics (e.g. user behavior tracking, A/B testing)\n- Performance monitoring (e.g. load testing, monitoring of server resources)\n- User experience optimization (e.g. user testing, usability analysis)\n\nSelf-actualization:\n\n- End-to-end testing (e.g. Selenium, Cypress)\n- Accessibility and internationalization (e.g. support for screen readers, translation)\n- Scalability (e.g. load balancing, caching)\n- Continuous integration and delivery (e.g. Jenkins, Travis CI)\n- Deployment (e.g. Docker, Kubernetes)\n- Automated testing (e.g. unit test, integration test)\n\nOf course this list isn’t exhaustive, but I’ve been thinking about it as more of a checklist to build upon when working on small side-projects that end up getting significant attention and development time.\n","contentHTML":"<p>I was thinking about all the little projects I work on and how they grow over time. The applications end up hitting some milestones and end up needing similar functionality that compliments the core features. A small project ends up getting complex enough that it requires some unit tests. I’ll go to deploy the project and now I need some deployment scripts and analytics to ensure the application is running correctly. The technology may change between each project, but web applications always seem to have the same steps that need to be taken to strengthen the application as it is scaled up.</p>\n<p>At a large company, adding a new set of functionality always has a suite of concerns to think through before implementation: how will we deploy this feature, how do we validate that users are using the feature like we expected them to, how do we monitor for bugs and errors? This is on top of the basic functionality of actually writing and testing that new feature.</p>\n<p>What if we thought about this like Maslow’s Hierarchy of Needs, but in the context of a web application.</p>\n<p>Maslow&#x27;s Hierarchy of Needs is a psychological model of human motivation proposed by Abraham Maslow in 1943. The model describes a hierarchy of human needs, beginning with basic physiological needs such as food and shelter and progressing upwards to higher-level needs such as self-actualization. Maslow argued that as humans satisfy their basic needs, they can move on to satisfy their higher-level needs.</p>\n<p>Applying Maslow&#x27;s Hierarchy of Needs to web applications, we can identify the different levels of needs that need to be met in order to make a web application successful.</p>\n<p>Basic functionality:</p>\n<ul>\n<li>A single build/run script</li>\n<li>Basic functionality (e.g. CRUD operations for a web app)</li>\n<li>User interface (e.g. layout, navigation, responsive design)</li>\n<li>Integration with external services (e.g. databases, APIs)</li>\n</ul>\n<p>Safety and security:</p>\n<ul>\n<li>Linting (e.g. ESLint, Prettier)</li>\n<li>Unit tests (e.g. Jest, Mocha)</li>\n<li>Basic error handling and reporting (e.g. logging, alerting)</li>\n<li>Input validation (e.g. form validation)</li>\n<li>Security best practices (e.g. encryption, password hashing, session management)</li>\n</ul>\n<p>Love and belonging:</p>\n<ul>\n<li>Basic analytics (e.g. page views, user engagement)</li>\n<li>User authentication and authorization (e.g. sign-up, login, role-based access control)</li>\n<li>User feedback (e.g. contact form, survey)</li>\n<li>Social media integration (e.g. sharing, commenting)</li>\n</ul>\n<p>Esteem:</p>\n<ul>\n<li>Advanced analytics (e.g. user behavior tracking, A/B testing)</li>\n<li>Performance monitoring (e.g. load testing, monitoring of server resources)</li>\n<li>User experience optimization (e.g. user testing, usability analysis)</li>\n</ul>\n<p>Self-actualization:</p>\n<ul>\n<li>End-to-end testing (e.g. Selenium, Cypress)</li>\n<li>Accessibility and internationalization (e.g. support for screen readers, translation)</li>\n<li>Scalability (e.g. load balancing, caching)</li>\n<li>Continuous integration and delivery (e.g. Jenkins, Travis CI)</li>\n<li>Deployment (e.g. Docker, Kubernetes)</li>\n<li>Automated testing (e.g. unit test, integration test)</li>\n</ul>\n<p>Of course this list isn’t exhaustive, but I’ve been thinking about it as more of a checklist to build upon when working on small side-projects that end up getting significant attention and development time.</p>","contentCode":"\"use strict\";\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = {\n    li: \"li\",\n    p: \"p\",\n    ul: \"ul\",\n    ...props.components\n  };\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"I was thinking about all the little projects I work on and how they grow over time. The applications end up hitting some milestones and end up needing similar functionality that compliments the core features. A small project ends up getting complex enough that it requires some unit tests. I’ll go to deploy the project and now I need some deployment scripts and analytics to ensure the application is running correctly. The technology may change between each project, but web applications always seem to have the same steps that need to be taken to strengthen the application as it is scaled up.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"At a large company, adding a new set of functionality always has a suite of concerns to think through before implementation: how will we deploy this feature, how do we validate that users are using the feature like we expected them to, how do we monitor for bugs and errors? This is on top of the basic functionality of actually writing and testing that new feature.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"What if we thought about this like Maslow’s Hierarchy of Needs, but in the context of a web application.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Maslow's Hierarchy of Needs is a psychological model of human motivation proposed by Abraham Maslow in 1943. The model describes a hierarchy of human needs, beginning with basic physiological needs such as food and shelter and progressing upwards to higher-level needs such as self-actualization. Maslow argued that as humans satisfy their basic needs, they can move on to satisfy their higher-level needs.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Applying Maslow's Hierarchy of Needs to web applications, we can identify the different levels of needs that need to be met in order to make a web application successful.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Basic functionality:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"A single build/run script\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Basic functionality (e.g. CRUD operations for a web app)\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"User interface (e.g. layout, navigation, responsive design)\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Integration with external services (e.g. databases, APIs)\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Safety and security:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"Linting (e.g. ESLint, Prettier)\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Unit tests (e.g. Jest, Mocha)\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Basic error handling and reporting (e.g. logging, alerting)\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Input validation (e.g. form validation)\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Security best practices (e.g. encryption, password hashing, session management)\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Love and belonging:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"Basic analytics (e.g. page views, user engagement)\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"User authentication and authorization (e.g. sign-up, login, role-based access control)\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"User feedback (e.g. contact form, survey)\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Social media integration (e.g. sharing, commenting)\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Esteem:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"Advanced analytics (e.g. user behavior tracking, A/B testing)\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Performance monitoring (e.g. load testing, monitoring of server resources)\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"User experience optimization (e.g. user testing, usability analysis)\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Self-actualization:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"End-to-end testing (e.g. Selenium, Cypress)\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Accessibility and internationalization (e.g. support for screen readers, translation)\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Scalability (e.g. load balancing, caching)\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Continuous integration and delivery (e.g. Jenkins, Travis CI)\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Deployment (e.g. Docker, Kubernetes)\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Automated testing (e.g. unit test, integration test)\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Of course this list isn’t exhaustive, but I’ve been thinking about it as more of a checklist to build upon when working on small side-projects that end up getting significant attention and development time.\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, {\n    ...props,\n    children: _jsx(_createMdxContent, {\n      ...props\n    })\n  }) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","excerptRaw":"\nI was thinking about all the little projects I work on and how they grow over time. The applications end up hitting some milestones and end up needing similar functionality that compliments the core features. A small project ends up getting complex enough that it requires some unit tests. I’ll go to deploy the project and now I need some deployment scripts and analytics to ensure the application is running correctly. The technology may change between each project, but web applications always seem to have the same steps that need to be taken to strengthen the application as it is scaled up.","excerptHTML":"<p>I was thinking about all the little projects I work on and how they grow over time. The applications end up hitting some milestones and end up needing similar functionality that compliments the core features. A small project ends up getting complex enough that it requires some unit tests. I’ll go to deploy the project and now I need some deployment scripts and analytics to ensure the application is running correctly. The technology may change between each project, but web applications always seem to have the same steps that need to be taken to strengthen the application as it is scaled up.</p>","excerptCode":"\"use strict\";\nconst {jsx: _jsx} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = {\n    p: \"p\",\n    ...props.components\n  };\n  return _jsx(_components.p, {\n    children: \"I was thinking about all the little projects I work on and how they grow over time. The applications end up hitting some milestones and end up needing similar functionality that compliments the core features. A small project ends up getting complex enough that it requires some unit tests. I’ll go to deploy the project and now I need some deployment scripts and analytics to ensure the application is running correctly. The technology may change between each project, but web applications always seem to have the same steps that need to be taken to strengthen the application as it is scaled up.\"\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, {\n    ...props,\n    children: _jsx(_createMdxContent, {\n      ...props\n    })\n  }) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","tags":["webapp","checklist"]},{"slug":"2023-01-07-clarity-hub-infer","date":"2023-01-07","title":"Clarity Hub Infer API","frontmatter":{"title":"Clarity Hub Infer API","tags":["nlp","machine learning"]},"contentRaw":"\n![Screen Shot 2023-01-07 at 3.57.30 PM.png](/media/2023-01-07-clarity-hub-infer/Screen_Shot_2023-01-07_at_3.57.30_PM.png)\n\nWhile working on Clarity Hub, we created a Clarity Hub Infer API along with a developer portal that would let anyone create infer models.\n\nThe Clarity Hub Infer API provides a fast and intuitive way to create, manage, and deploy NLP models based on labelling utterances.\n\nAt the most basic level, the Infer API would let users send utterances via an API and get toxicity analysis, sentiment scores, and simple NLP data like nouns and topics from the utterances.\n\nThe power of the Infer API is that consumers can supply a set of pre-labelled utterances to the API, and the API will create a model from this, even if there are only a few utterances used for training. Then the consumer can send a new utterance get a label using that model.\n\nThe NLP APIs at Clarity Hub were a set of APIs:\n\n```mermaid\ngraph RL\n  NLP(Clarity Hub NLP API) --> API(Clarity Hub Infer API) --> Consumer\n```\n\nThe Consumer would user the Infer API which provided APIs for training and labeling datasets and getting toxicity and sentiment analyses. the Clarity Hub NLP API contained trained Tensorflow datasets for creating embeddings via the Universal Sentence Encoder (USE).\n\nAn **embedding** a vector that represents an utterance - a sentence, sentence fragment, or paragraph of text.\n\nTraining would involve a consumer sending a payload of utterances with labels to the Infer API, which would call the NLP API internally to create embeddings. We then clustered these embeddings to and re-labelled the clusters using the given labels. If no label was found for an utterance cluster, we attempted to pull a topic out of the utterances to re-label it.\n\nThe clusters with labels were then stored into S3.\n\n```mermaid\ngraph TD\n  Train -->|Utterances with labels| USE\n  USE -->|Embeddings with labels| Clustering\n  Clustering -->|Embedding Clusters| Labeller\n  Labeller -->|Clusters with Labels| S3\n```\n\nTo classify a new utterance, we created an embedding from it, loaded the existing dataset in, then ran cosine similarity to find the most probabilistic matches:\n\n```mermaid\ngraph TD\n  Classify --> |Utterance| USE\n  USE --> |Embedding| Classifier(Classifier)\n  Classifier --> |Embedding + Clusters from S3| Similarity(Cosine Similarity)\n  Similarity --> |Labels with Probability| Response\n```\n\n### What it looked like\n\n![Screen Shot 2023-01-07 at 3.57.56 PM.png](/media/2023-01-07-clarity-hub-infer/Screen_Shot_2023-01-07_at_3.57.56_PM.png)\n\n![Screen Shot 2023-01-07 at 3.58.08 PM.png](/media/2023-01-07-clarity-hub-infer/Screen_Shot_2023-01-07_at_3.58.08_PM.png)\n\n![Screen Shot 2023-01-07 at 3.58.28 PM.png](/media/2023-01-07-clarity-hub-infer/Screen_Shot_2023-01-07_at_3.58.28_PM.png)\n\n### Conclusion\n\nWith ChatGPT and other NLP models coming out lately, this seems fairly basic, but the following processes are still very useful to understand:\n\n- Convert language to a representation that is easier to work with, like a vector.\n- Clustering vectors is a great way to find representative vectors, reducing the size of the number of vectors you need to work with.\n- Cosine Similarity can be used to find how similar vectors are. If a vector is labelled with metadata, it also tells you how similar the metadata between the vectors are as well.\n\nYou can see [my project page](/projects/2020-05-18-clarity-hub-infer) for more details and links to the Github repos.\n","contentHTML":"<p><img alt=\"Screen Shot 2023-01-07 at 3.57.30 PM.png\" src=\"/media/2023-01-07-clarity-hub-infer/Screen_Shot_2023-01-07_at_3.57.30_PM.png\" style=\"max-height:500px;margin:auto;text-align:center\"/></p>\n<p>While working on Clarity Hub, we created a Clarity Hub Infer API along with a developer portal that would let anyone create infer models.</p>\n<p>The Clarity Hub Infer API provides a fast and intuitive way to create, manage, and deploy NLP models based on labelling utterances.</p>\n<p>At the most basic level, the Infer API would let users send utterances via an API and get toxicity analysis, sentiment scores, and simple NLP data like nouns and topics from the utterances.</p>\n<p>The power of the Infer API is that consumers can supply a set of pre-labelled utterances to the API, and the API will create a model from this, even if there are only a few utterances used for training. Then the consumer can send a new utterance get a label using that model.</p>\n<p>The NLP APIs at Clarity Hub were a set of APIs:</p>\n<mermaid chart=\"graph RL\n  NLP(Clarity Hub NLP API) --&gt; API(Clarity Hub Infer API) --&gt; Consumer\"></mermaid>\n<p>The Consumer would user the Infer API which provided APIs for training and labeling datasets and getting toxicity and sentiment analyses. the Clarity Hub NLP API contained trained Tensorflow datasets for creating embeddings via the Universal Sentence Encoder (USE).</p>\n<p>An <strong>embedding</strong> a vector that represents an utterance - a sentence, sentence fragment, or paragraph of text.</p>\n<p>Training would involve a consumer sending a payload of utterances with labels to the Infer API, which would call the NLP API internally to create embeddings. We then clustered these embeddings to and re-labelled the clusters using the given labels. If no label was found for an utterance cluster, we attempted to pull a topic out of the utterances to re-label it.</p>\n<p>The clusters with labels were then stored into S3.</p>\n<mermaid chart=\"graph TD\n  Train --&gt;|Utterances with labels| USE\n  USE --&gt;|Embeddings with labels| Clustering\n  Clustering --&gt;|Embedding Clusters| Labeller\n  Labeller --&gt;|Clusters with Labels| S3\"></mermaid>\n<p>To classify a new utterance, we created an embedding from it, loaded the existing dataset in, then ran cosine similarity to find the most probabilistic matches:</p>\n<mermaid chart=\"graph TD\n  Classify --&gt; |Utterance| USE\n  USE --&gt; |Embedding| Classifier(Classifier)\n  Classifier --&gt; |Embedding + Clusters from S3| Similarity(Cosine Similarity)\n  Similarity --&gt; |Labels with Probability| Response\"></mermaid>\n<h3>What it looked like</h3>\n<p><img alt=\"Screen Shot 2023-01-07 at 3.57.56 PM.png\" src=\"/media/2023-01-07-clarity-hub-infer/Screen_Shot_2023-01-07_at_3.57.56_PM.png\" style=\"max-height:500px;margin:auto;text-align:center\"/></p>\n<p><img alt=\"Screen Shot 2023-01-07 at 3.58.08 PM.png\" src=\"/media/2023-01-07-clarity-hub-infer/Screen_Shot_2023-01-07_at_3.58.08_PM.png\" style=\"max-height:500px;margin:auto;text-align:center\"/></p>\n<p><img alt=\"Screen Shot 2023-01-07 at 3.58.28 PM.png\" src=\"/media/2023-01-07-clarity-hub-infer/Screen_Shot_2023-01-07_at_3.58.28_PM.png\" style=\"max-height:500px;margin:auto;text-align:center\"/></p>\n<h3>Conclusion</h3>\n<p>With ChatGPT and other NLP models coming out lately, this seems fairly basic, but the following processes are still very useful to understand:</p>\n<ul>\n<li>Convert language to a representation that is easier to work with, like a vector.</li>\n<li>Clustering vectors is a great way to find representative vectors, reducing the size of the number of vectors you need to work with.</li>\n<li>Cosine Similarity can be used to find how similar vectors are. If a vector is labelled with metadata, it also tells you how similar the metadata between the vectors are as well.</li>\n</ul>\n<p>You can see <a href=\"/projects/2020-05-18-clarity-hub-infer\">my project page</a> for more details and links to the Github repos.</p>","contentCode":"\"use strict\";\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = {\n    a: \"a\",\n    h3: \"h3\",\n    img: \"img\",\n    li: \"li\",\n    mermaid: \"mermaid\",\n    p: \"p\",\n    strong: \"strong\",\n    ul: \"ul\",\n    ...props.components\n  };\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/media/2023-01-07-clarity-hub-infer/Screen_Shot_2023-01-07_at_3.57.30_PM.png\",\n        alt: \"Screen Shot 2023-01-07 at 3.57.30 PM.png\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"While working on Clarity Hub, we created a Clarity Hub Infer API along with a developer portal that would let anyone create infer models.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The Clarity Hub Infer API provides a fast and intuitive way to create, manage, and deploy NLP models based on labelling utterances.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"At the most basic level, the Infer API would let users send utterances via an API and get toxicity analysis, sentiment scores, and simple NLP data like nouns and topics from the utterances.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The power of the Infer API is that consumers can supply a set of pre-labelled utterances to the API, and the API will create a model from this, even if there are only a few utterances used for training. Then the consumer can send a new utterance get a label using that model.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The NLP APIs at Clarity Hub were a set of APIs:\"\n    }), \"\\n\", _jsx(_components.mermaid, {\n      chart: \"graph RL\\n  NLP(Clarity Hub NLP API) --> API(Clarity Hub Infer API) --> Consumer\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The Consumer would user the Infer API which provided APIs for training and labeling datasets and getting toxicity and sentiment analyses. the Clarity Hub NLP API contained trained Tensorflow datasets for creating embeddings via the Universal Sentence Encoder (USE).\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"An \", _jsx(_components.strong, {\n        children: \"embedding\"\n      }), \" a vector that represents an utterance - a sentence, sentence fragment, or paragraph of text.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Training would involve a consumer sending a payload of utterances with labels to the Infer API, which would call the NLP API internally to create embeddings. We then clustered these embeddings to and re-labelled the clusters using the given labels. If no label was found for an utterance cluster, we attempted to pull a topic out of the utterances to re-label it.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The clusters with labels were then stored into S3.\"\n    }), \"\\n\", _jsx(_components.mermaid, {\n      chart: \"graph TD\\n  Train -->|Utterances with labels| USE\\n  USE -->|Embeddings with labels| Clustering\\n  Clustering -->|Embedding Clusters| Labeller\\n  Labeller -->|Clusters with Labels| S3\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"To classify a new utterance, we created an embedding from it, loaded the existing dataset in, then ran cosine similarity to find the most probabilistic matches:\"\n    }), \"\\n\", _jsx(_components.mermaid, {\n      chart: \"graph TD\\n  Classify --> |Utterance| USE\\n  USE --> |Embedding| Classifier(Classifier)\\n  Classifier --> |Embedding + Clusters from S3| Similarity(Cosine Similarity)\\n  Similarity --> |Labels with Probability| Response\"\n    }), \"\\n\", _jsx(_components.h3, {\n      children: \"What it looked like\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/media/2023-01-07-clarity-hub-infer/Screen_Shot_2023-01-07_at_3.57.56_PM.png\",\n        alt: \"Screen Shot 2023-01-07 at 3.57.56 PM.png\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/media/2023-01-07-clarity-hub-infer/Screen_Shot_2023-01-07_at_3.58.08_PM.png\",\n        alt: \"Screen Shot 2023-01-07 at 3.58.08 PM.png\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/media/2023-01-07-clarity-hub-infer/Screen_Shot_2023-01-07_at_3.58.28_PM.png\",\n        alt: \"Screen Shot 2023-01-07 at 3.58.28 PM.png\"\n      })\n    }), \"\\n\", _jsx(_components.h3, {\n      children: \"Conclusion\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"With ChatGPT and other NLP models coming out lately, this seems fairly basic, but the following processes are still very useful to understand:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"Convert language to a representation that is easier to work with, like a vector.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Clustering vectors is a great way to find representative vectors, reducing the size of the number of vectors you need to work with.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Cosine Similarity can be used to find how similar vectors are. If a vector is labelled with metadata, it also tells you how similar the metadata between the vectors are as well.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"You can see \", _jsx(_components.a, {\n        href: \"/projects/2020-05-18-clarity-hub-infer\",\n        children: \"my project page\"\n      }), \" for more details and links to the Github repos.\"]\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, {\n    ...props,\n    children: _jsx(_createMdxContent, {\n      ...props\n    })\n  }) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","excerptRaw":"\n![Screen Shot 2023-01-07 at 3.57.30 PM.png](/media/2023-01-07-clarity-hub-infer/Screen_Shot_2023-01-07_at_3.57.30_PM.png)","excerptHTML":"<p><img alt=\"Screen Shot 2023-01-07 at 3.57.30 PM.png\" src=\"/media/2023-01-07-clarity-hub-infer/Screen_Shot_2023-01-07_at_3.57.30_PM.png\" style=\"max-height:500px;margin:auto;text-align:center\"/></p>","excerptCode":"\"use strict\";\nconst {jsx: _jsx} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = {\n    img: \"img\",\n    p: \"p\",\n    ...props.components\n  };\n  return _jsx(_components.p, {\n    children: _jsx(_components.img, {\n      src: \"/media/2023-01-07-clarity-hub-infer/Screen_Shot_2023-01-07_at_3.57.30_PM.png\",\n      alt: \"Screen Shot 2023-01-07 at 3.57.30 PM.png\"\n    })\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, {\n    ...props,\n    children: _jsx(_createMdxContent, {\n      ...props\n    })\n  }) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","tags":["nlp","machine learning"]},{"slug":"2023-01-01-nx-nextjs-starter","date":"2023-01-01","title":"NX NextJS Starter","frontmatter":{"title":"NX NextJS Starter","tags":["nextjs","nx","typescript"]},"contentRaw":"\nTo kickstart the year, I created a repo that contains a simple starter kit for using NextJS with NX. You can see the repo here:\n\n[Github](https://github.com/idmontie/nx-nextjs-starter)\n\nThis personal Github page is built using this starter kit. I wanted to create a starter kit that was simple and easy to use and also has a lot of eslint and Typescript configuration setup. I've also been working on revamping the website for Dark Emblem - my NFT side-project. The starter kit is based on the linting rules and Typescript set up that I've been using for that project.\n\n## Nx\n\nI've traditionally used Lerna for my monorepo projects, but now that Nx has taken over maintenance of Lerna, I decided to give Nx a try directly.\n\nNx has been enjoyable to use. Managing many different React projects with internal libraries has been very easy to set up, use and deploy.\n\n## NextJS\n\nMy last few projects have been Single Page Apps (SPAs) or statically generated sites using Gatsby or Docusaurus. All three of those tools are great, but I wanted to try out NextJS for a few reasons:\n\n* In my Dark Emblem project, I was having difficulty getting share links to Discord and Twitter to work properly. This was mainly caused by those sites not running JavaScript, so page links would just render the default SPA title and description. I knew that NextJS had a solution for this, so I wanted to try it out.\n* I wanted more control of my documentation and blog websites, so I needed to be able to use custom server-side code.\n\n## Starter Kit\n\nOverall the starter kit is a pretty simple example. It just containers preconfigured Nx, husky, lint-staged, eslint, prettier, Typescript, and NextJS. It does not contain any UI components or anything like that. It's just a simple starter kit to get you up and running with a monorepo NextJS project.\n\nFeel free to check it out at [Github](https://github.com/idmontie/nx-nextjs-starter).","contentHTML":"<p>To kickstart the year, I created a repo that contains a simple starter kit for using NextJS with NX. You can see the repo here:</p>\n<p><a href=\"https://github.com/idmontie/nx-nextjs-starter\">Github</a></p>\n<p>This personal Github page is built using this starter kit. I wanted to create a starter kit that was simple and easy to use and also has a lot of eslint and Typescript configuration setup. I&#x27;ve also been working on revamping the website for Dark Emblem - my NFT side-project. The starter kit is based on the linting rules and Typescript set up that I&#x27;ve been using for that project.</p>\n<h2>Nx</h2>\n<p>I&#x27;ve traditionally used Lerna for my monorepo projects, but now that Nx has taken over maintenance of Lerna, I decided to give Nx a try directly.</p>\n<p>Nx has been enjoyable to use. Managing many different React projects with internal libraries has been very easy to set up, use and deploy.</p>\n<h2>NextJS</h2>\n<p>My last few projects have been Single Page Apps (SPAs) or statically generated sites using Gatsby or Docusaurus. All three of those tools are great, but I wanted to try out NextJS for a few reasons:</p>\n<ul>\n<li>In my Dark Emblem project, I was having difficulty getting share links to Discord and Twitter to work properly. This was mainly caused by those sites not running JavaScript, so page links would just render the default SPA title and description. I knew that NextJS had a solution for this, so I wanted to try it out.</li>\n<li>I wanted more control of my documentation and blog websites, so I needed to be able to use custom server-side code.</li>\n</ul>\n<h2>Starter Kit</h2>\n<p>Overall the starter kit is a pretty simple example. It just containers preconfigured Nx, husky, lint-staged, eslint, prettier, Typescript, and NextJS. It does not contain any UI components or anything like that. It&#x27;s just a simple starter kit to get you up and running with a monorepo NextJS project.</p>\n<p>Feel free to check it out at <a href=\"https://github.com/idmontie/nx-nextjs-starter\">Github</a>.</p>","contentCode":"\"use strict\";\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = {\n    a: \"a\",\n    h2: \"h2\",\n    li: \"li\",\n    p: \"p\",\n    ul: \"ul\",\n    ...props.components\n  };\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"To kickstart the year, I created a repo that contains a simple starter kit for using NextJS with NX. You can see the repo here:\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.a, {\n        href: \"https://github.com/idmontie/nx-nextjs-starter\",\n        children: \"Github\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"This personal Github page is built using this starter kit. I wanted to create a starter kit that was simple and easy to use and also has a lot of eslint and Typescript configuration setup. I've also been working on revamping the website for Dark Emblem - my NFT side-project. The starter kit is based on the linting rules and Typescript set up that I've been using for that project.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Nx\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"I've traditionally used Lerna for my monorepo projects, but now that Nx has taken over maintenance of Lerna, I decided to give Nx a try directly.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Nx has been enjoyable to use. Managing many different React projects with internal libraries has been very easy to set up, use and deploy.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"NextJS\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"My last few projects have been Single Page Apps (SPAs) or statically generated sites using Gatsby or Docusaurus. All three of those tools are great, but I wanted to try out NextJS for a few reasons:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"In my Dark Emblem project, I was having difficulty getting share links to Discord and Twitter to work properly. This was mainly caused by those sites not running JavaScript, so page links would just render the default SPA title and description. I knew that NextJS had a solution for this, so I wanted to try it out.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"I wanted more control of my documentation and blog websites, so I needed to be able to use custom server-side code.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Starter Kit\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Overall the starter kit is a pretty simple example. It just containers preconfigured Nx, husky, lint-staged, eslint, prettier, Typescript, and NextJS. It does not contain any UI components or anything like that. It's just a simple starter kit to get you up and running with a monorepo NextJS project.\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Feel free to check it out at \", _jsx(_components.a, {\n        href: \"https://github.com/idmontie/nx-nextjs-starter\",\n        children: \"Github\"\n      }), \".\"]\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, {\n    ...props,\n    children: _jsx(_createMdxContent, {\n      ...props\n    })\n  }) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","excerptRaw":"\nTo kickstart the year, I created a repo that contains a simple starter kit for using NextJS with NX. You can see the repo here:","excerptHTML":"<p>To kickstart the year, I created a repo that contains a simple starter kit for using NextJS with NX. You can see the repo here:</p>","excerptCode":"\"use strict\";\nconst {jsx: _jsx} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = {\n    p: \"p\",\n    ...props.components\n  };\n  return _jsx(_components.p, {\n    children: \"To kickstart the year, I created a repo that contains a simple starter kit for using NextJS with NX. You can see the repo here:\"\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, {\n    ...props,\n    children: _jsx(_createMdxContent, {\n      ...props\n    })\n  }) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","tags":["nextjs","nx","typescript"]},{"slug":"2015-01-05-mobile","date":"2015-01-05","title":"Mobile","frontmatter":{"title":"Mobile","tags":["mobile"]},"contentRaw":"\nThis short post was originally on a blog I started for creating web applications. This post goes into mobile considerations for web applications.\n\n## Tools to Sync Mobile and Web Development\n\nThese tools not only help with Mobile development, but they also help with cross-browser development as well (see Developer Tools):\n\n* [Ghostlab](http://vanamco.com/ghostlab/) - Synchronized browser testing for web and mobile.  Test files and URLs against multiple devices by syncing clicks, scrolls, keystrokes across the devices. $49 for a license.\n* [Edge Inspect CC](https://creative.adobe.com/products/inspect) - Synchronized Browsing & Refreshing. Test files and URLs agaisnt multiple devices at once.  Part of the Adobe Creative Cloud.\n","contentHTML":"<p>This short post was originally on a blog I started for creating web applications. This post goes into mobile considerations for web applications.</p>\n<h2>Tools to Sync Mobile and Web Development</h2>\n<p>These tools not only help with Mobile development, but they also help with cross-browser development as well (see Developer Tools):</p>\n<ul>\n<li><a href=\"http://vanamco.com/ghostlab/\">Ghostlab</a> - Synchronized browser testing for web and mobile.  Test files and URLs against multiple devices by syncing clicks, scrolls, keystrokes across the devices. $49 for a license.</li>\n<li><a href=\"https://creative.adobe.com/products/inspect\">Edge Inspect CC</a> - Synchronized Browsing &amp; Refreshing. Test files and URLs agaisnt multiple devices at once.  Part of the Adobe Creative Cloud.</li>\n</ul>","contentCode":"\"use strict\";\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = {\n    a: \"a\",\n    h2: \"h2\",\n    li: \"li\",\n    p: \"p\",\n    ul: \"ul\",\n    ...props.components\n  };\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"This short post was originally on a blog I started for creating web applications. This post goes into mobile considerations for web applications.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Tools to Sync Mobile and Web Development\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"These tools not only help with Mobile development, but they also help with cross-browser development as well (see Developer Tools):\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.a, {\n          href: \"http://vanamco.com/ghostlab/\",\n          children: \"Ghostlab\"\n        }), \" - Synchronized browser testing for web and mobile.  Test files and URLs against multiple devices by syncing clicks, scrolls, keystrokes across the devices. $49 for a license.\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.a, {\n          href: \"https://creative.adobe.com/products/inspect\",\n          children: \"Edge Inspect CC\"\n        }), \" - Synchronized Browsing & Refreshing. Test files and URLs agaisnt multiple devices at once.  Part of the Adobe Creative Cloud.\"]\n      }), \"\\n\"]\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, {\n    ...props,\n    children: _jsx(_createMdxContent, {\n      ...props\n    })\n  }) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","excerptRaw":"\nThis short post was originally on a blog I started for creating web applications. This post goes into mobile considerations for web applications.","excerptHTML":"<p>This short post was originally on a blog I started for creating web applications. This post goes into mobile considerations for web applications.</p>","excerptCode":"\"use strict\";\nconst {jsx: _jsx} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = {\n    p: \"p\",\n    ...props.components\n  };\n  return _jsx(_components.p, {\n    children: \"This short post was originally on a blog I started for creating web applications. This post goes into mobile considerations for web applications.\"\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = props.components || ({});\n  return MDXLayout ? _jsx(MDXLayout, {\n    ...props,\n    children: _jsx(_createMdxContent, {\n      ...props\n    })\n  }) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","tags":["mobile"]}],"hasNextPage":true,"hasPreviousPage":true},"__N_SSG":true}